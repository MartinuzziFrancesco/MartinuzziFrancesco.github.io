[{"categories":[],"content":"Summary My research is focused on analyzing the dynamics of extreme events and their consequences on the environment. For this task I am using a particular Machine Learning family of models, called Reservoir Computing. More broadly, my interests vary from applications of Machine Learning, to Dynamical Systems and Self Organization. I like every thing computational, and odds are if something has to do with programming I am going to enjoy it. Pubblications ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models [arXiv], Martinuzzi, F., Rackauckas, C., AbdelRehim, A., Mahecha, M. D. and Mora, K. arXiv preprint (2022) Composable and Reusable Neural Surrogates to Predict System Response of Causal Model Components [pdf] Anantharaman, R., AbdelRehim, A., Martinuzzi, F., Yalburgi, S., Fischer, K., Hertz, G., de Vos, P., Laughman, C., Ma, Y., Shah, V., Edelman, A. and Rackauckas, C. AAAI 2022 Workshop on AI for Design and Manufacturing (ADAM), (2022) Composing Modeling and Simulation with Machine Learning in Julia [arXiv] Rackauckas, C., Anantharaman, R., Edelman, A., Gowda, S., Gwozdz, M., Jain, A., Laughman, C., Ma, Y., Martinuzzi, F., Pal, A., Rajput, U., Saba, E. and Shah, V. B. 14th Modelica Conference 2021, (2021) Talks Chaotic time series predictions with ReservoirComputing.jl. [Video] JuliaCon 2021, (2021) Notable Software ReservoirComputing.jl: Julia implementation of a vast range of different Reservoir Computing algorithms. CellularAutomata.jl: lightweight Julia implementation of Cellular Automata, both one dimensional and two dimensional. Conferences and Summer Schools JuliaCon 2021: attend as speaker Arpa-e Summit 2021: attended as awardee with Julia Computing JuliaCon 2020: attended RegML 2020: summer school: admitted and attended ","date":"30310-103-10","objectID":"/research/:0:0","tags":[],"title":"Research","uri":"/research/"},{"categories":null,"content":"Introduction My proposal for the 2020 Google Summer of Code with the Julia Language was based on the implementation of a library for the family of models know as Reservoir Computing. After working for a month on the issue #34 of NeuralPDE.jl I created the initial draft of the ReservoirComputing.jl package, consisting at the time of only the implementation of Echo State Networks (ESNs). Having prior knowledge on the model I started digging a little more on the literature and found a lot of interesting variations of ESNs, which led me to base my proposal entirely on the addition of these models into the existing library. The work done in the three months period was based on weekly goals, which allowed me to keep a steady and consistent pace throughout the project. Most of the implementations done at the end were the one present in the proposal, although there were a couple of little variations from the initial idea. Work done during GSoC Week 1: implemented different Linear Model solvers for ESN, tests for the code and checked the results against the literature - blog post. Week 2: implemented Support Vector Echo State Machines (SVESMs), tests for the code and checked the results against the literature - blog post. Week 3: implemented Echo State Gaussian Processes (ESGPs), tests for the code and checked the results against the literature - blog post. Week 4: implemented Singular Value Decomposition-based reservoir, tests for the code and checked the results against the literature - blog post. Week 5: comparison of the various models implemented in the task of predicting chaotic systems both short and long term - blog post. Week 6: implemented minimum complexity ESN methods, tests for the code and checked the results against the literature - blog post. Week 7: implemented Elementary Cellular Automata-based reservoir computers (RECA), tests for the code and checked the results against the literature - blog post. Week 8: implemented two dimensional cellular automata-based reservoir computers, tests for the code and checked the results against the literature - blog post. Week 9: implemented Cycle Reservoirs with Regular Jumps, tests for the code and checked the results against the literature - blog post. Week 10: implemented Reservoir Memory Machines (RMMs), tests for the code and checked the results against the literature - blog post. Week 11: implemented Gated Recurring Unit-based reservoir, tests for the code and checked the results against the literature - blog post. Week 12: added the documentation - link to documentation. The code and the library can be found on GitHub, under the SciML organization: ReservoirComputing.jl. Future directions I hope to be able to continue to maintain and improve this package in the future, and there are already some ideas of the challenges that could be tackled. Since I started my journey with Julia, my knowledge of the language has increased and looking back there is room for major improvements in the code, both for optimization and usability. Merging all the train and predict function into a single one will surely reduce confusion, and adding more examples to the documentation is already something that I am working on. Acknowledgements First of all I would like to thanks my mentors Chris, David and Ranjan for following me in this project and for giving me incredible autonomy in these months; a special thanks to Chris that has taken the time to help me from the first commits and all throughout the application process. Another big thanks goes to the Julia community, an incredible group of people that are working towards a more open and welcoming scientific ecosystem, and are always ready to help and guide newcomers as myself; I don’t think I have ever felt so welcomed by people I have yet to meet face to face before. Finally thanks to the Google Open Source program for providing both the opportunity to have this experience and the funding to help people partecipate to it. ","date":"25258-825-08","objectID":"/posts/12_gsoc_week/:0:0","tags":null,"title":"Google Summer of Code 2020 Final Report","uri":"/posts/12_gsoc_week/"},{"categories":null,"content":"Following an architecture found on [1] this week we decided to implement a reservoir model based on the Gated Recurring Unit (GRU) structure, first described in [2]. This architecture is an evolution of the standard Recurrent Neural Network (RNN) update equations and works in a similar way to Long Short Term Memory (LSTM) with a forget gate but with fewer parameters; the LSTM usually outperfofms the GRU in most task but it could be interesting to see the behavior of this unit in the Echo State Network (ESN) model. In the first part of this post we will briefly explain the theory behind the model and after we will show an example to see the performance of this architecture. Gated Recurring Unit As described in [2] the update equations in the GRU hidden unit are described as follows: The reset gate is computed by $$\\textbf{r}_t = \\sigma (\\textbf{W}_r \\textbf{x}_t + \\textbf{U}_r \\textbf{h}_{t-1} + \\textbf{b}_r)$$ where \\( \\sigma \\) is the sigmoid function. \\( \\textbf{x}_t \\) is the input at time \\( t \\) and \\( \\textbf{h}_{t-1} \\) is the previous hidden state. In the ESN case it will be the provious state vector. In a similar way, the update gate is computed by $$\\textbf{z}_t = \\sigma (\\textbf{W}_z \\textbf{x}_t + \\textbf{U}_z \\textbf{h}_{t-1} + \\textbf{b}_z)$$ The candidate activation vector is given by $$\\tilde{\\textbf{h}}_t = f(\\textbf{W}_h \\textbf{x}_t + \\textbf{U}_h (\\textbf{r}_t \\circ \\textbf{h}_{t-1}) + \\textbf{b}_h)$$ where \\( \\circ \\) represents the Hadamard product. In the ESN case \\( \\textbf{U}_h = \\textbf{W}, \\textbf{W}_h = \\textbf{W}_in \\) where \\( \\textbf{W} \\) is the reservoir matrix and \\( \\textbf{W}_in \\) is the input layer matrix. In the original implementation the activation function \\( f \\) is taken to be the hyperbolic tangent. The final states vector is given by $$\\textbf{h}_t = (1-\\textbf{z}_t) \\circ \\textbf{h}_{t-1} + \\textbf{z}_t \\circ \\tilde{\\textbf{h}}_t$$ Alternative forms are known but for the first implementation we decided to focus more our attention on the standard model. The \\( \\textbf{W}, \\textbf{U} \\) layers are fixed and constructed using the irrational number input layer generator (see [3] or week 9), with a different start for the change of sign but in the future we would like to give more possibilities for the construction of these layers. Implementation in ReservoirComputing The overall implementation is not the hardest part, band following the instructions of the original paper we were able to implement a gru base function that updates the states vector at every time step. Building on that function we implemented two public function, the constructor GRUESN and the predictor GRUESNpredict. The first one takes as input the same inputs as the ESN constructor with the addition of the gates_weight optional value, set to 0.9 as default. The GRUESNpredict function takes as input the same values as the ESNpredict function and return the prediction made by the GRUESN. Example Since this model isn not found in literature, only as comparison in [1] but for different tasks than time series prediction, we chose to use yet again the Henon map to test the capabilities of this model in the reproduction of a choatic system. This particular model was chosen since is less complex than the Lorenz system and it requires little parameter tuning in order to obtain decent results. Let us start by insalling and importing the usual packages using Pkg Pkg.add(\"ReservoirComputing\") Pkg.add(\"DynamicalSystems\") Pkg.add(\"Plots\") using ReservoirComputing using DynamicalSystems using Plots The construction of the Henon map is straight forward. Again the data points are shifted by -0.5 and scaled by 2: ds = Systems.henon() traj = trajectory(ds, 7000) data = Matrix(traj)' data = (data .-0.5) .* 2 shift = 200 train_len = 2000 predict_len = 3000 train = data[:, shift:shift+train_len-1] test = data[:, shift+train_len:shift+train_len+predict_len-1] For this example we will use the irrational sign input matrix in order to be consi","date":"16168-816-08","objectID":"/posts/11_gsoc_week/:0:0","tags":null,"title":"GSoC week 11: Gated Recurring Unit-based reservoir","uri":"/posts/11_gsoc_week/"},{"categories":null,"content":"Documentation [1] Paaßen, Benjamin, and Alexander Schulz. “Reservoir memory machines.” arXiv preprint arXiv:2003.04793 (2020). [2] Cho, Kyunghyun, et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014). [2] Rodan, Ali, and Peter Tiňo. “Simple deterministically constructed cycle reservoirs with regular jumps.” Neural computation 24.7 (2012): 1822-1852. ","date":"16168-816-08","objectID":"/posts/11_gsoc_week/:1:0","tags":null,"title":"GSoC week 11: Gated Recurring Unit-based reservoir","uri":"/posts/11_gsoc_week/"},{"categories":null,"content":"For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year [1]. This was one of the hardest, and longest, implementations to date and I believe there is still some work to be done. In this post we will briefly touch on the theory behind the model, and after an example of their usage will be presented. Theoretical Background Born as an alternative to the Neural Turing Machine [2] the RMM is an extension of the Echo State Network model, with the addition of an actual memory \\( \\textbf{M}_t \\in \\mathbb{R}^{K \\times n} \\), a write head and a read head. The dynamics of the RMM are the following: In the first step the previous memory state is copied \\( \\textbf{M}_t = \\textbf{M}_{t-1} \\), with the initial memory step being initialized to zero. The write head is then controlled by the value \\( c_t^w = \\textbf{u}^w \\textbf{x}^t + \\textbf{v}^r \\textbf{h}^t \\) where \\( \\textbf{u}^w, \\textbf{v}^r \\) are learnable parameters and \\( \\textbf{x}^t, \\textbf{h}^t \\) are the input vector and state vector at time t respectively. If \\( c_t^w \u003e 0 \\) then the input is written to memory, \\( \\textbf{m}_{t, k} = \\textbf{x}^t \\) and \\( k_t = k_{t-1}+1 \\). \\( k \\) is resetted to 1 if it exceeds the memory size \\( K \\). In the other case the memory and \\( k \\) are left as they are. Each time step the read head is controlled in a similar way using the vector \\( \\textbf{c}_t^r = \\textbf{U}^r \\textbf{x}^t + \\textbf{V}^r \\textbf{h}^t \\) where \\( \\textbf{U}^r, \\textbf{V}^r \\) are learnable parameters. If \\( c^r_{t, 2} = max{c^r_{t, 1}, c^r_{t, 2}, c^r_{t, 3}} \\) \\( l_t = l_{t-1}+1 \\), otherwise \\( l_t = 1 \\). After that the memory read at time \\( t \\) is set as the \\( l\\)th row of \\( \\textbf{M}_t \\), \\( \\textbf{r}^t = \\textbf{m}_{t, l_t} \\) The output of the system is determined by \\( \\textbf{y_t} = \\textbf{V} \\textbf{h}^t + \\textbf{R} \\textbf{r}^t \\) where \\( \\textbf{V}, \\textbf{R} \\) are learnable parameters. Setting \\( \\textbf{R} = 0 \\) we can see that the result is the standard ESN. For a more detailed explanation of the procedure and of the training process please refer to the original paper. Implementation in ReservoirComputing.jl Following both the paper and the code provided (original in Python, click here) we were able to implement a RMM mutable struct and a RMMdirect_predict function able to train and do predictions with the RMM model. The default constructor for RMM takes as input W the reservoir matrix in_data the training data out_data the desired output W_in the input layer matrix memory_size the size \\( K \\) of the memory activation optional activation function for the reservoir states, with default tanh alpha optional leaking rate, with default 1.0 nla_type optional non linear algorithm, eith default NLADefault() extended_states optional boolean for the extended states option, with default false The constructor trains the RMM, so ance it is initialized there is only need for a predict function. The RMMdirect_predict takes as input rmmm an initialized RMM input the input data and gives as output the prediction based on the input data given. The prediction process is relatively different from the implementation used in ReservoirComputing.jl, so we will not be able to do a proper comparison with the other models we implemented. In the future we do want to uniform the RMM with the other architectures present in the library, but it seems like a moth worth of work, so for the moment we are happy with the basic implementations obtained. Examples For example we will use the next step prediction for the Henon map, used also in last week test. The map is defined as $$x_{x+1} = 1 - ax_n^2 + y_n$$ $$ y_{n+1} = bx_n $$ Let us start by installing and importing all the needed packages: using Pkg Pkg.add(\"ReservoirComputing\") Pkg.add(\"DynamicalSystems\") Pkg.add(\"LinearAlgebra\") using ReservoirComputing using DynamicalSystems using LinearAlgebra Now we can generate the ","date":"13138-813-08","objectID":"/posts/10_gsoc_week/:0:0","tags":null,"title":"GSoC week 10: Reservoir Memory Machines","uri":"/posts/10_gsoc_week/"},{"categories":null,"content":"Documentation [1] Paaßen, Benjamin, and Alexander Schulz. “Reservoir memory machines.” arXiv preprint arXiv:2003.04793 (2020). [2] Graves, Alex, Greg Wayne, and Ivo Danihelka. “Neural turing machines.” arXiv preprint arXiv:1410.5401 (2014). [3] Rodan, Ali, and Peter Tiňo. “Simple deterministically constructed cycle reservoirs with regular jumps.” Neural computation 24.7 (2012): 1822-1852. [4] Rodan, Ali, and Peter Tino. “Minimum complexity echo state network.” IEEE transactions on neural networks 22.1 (2010): 131-144. ","date":"13138-813-08","objectID":"/posts/10_gsoc_week/:1:0","tags":null,"title":"GSoC week 10: Reservoir Memory Machines","uri":"/posts/10_gsoc_week/"},{"categories":null,"content":"This week body of work is less then the usual amount, since most of my time was spent watching the incredible talks given at JuliaCon 2020. This was my first time attending and I just wanted to spend a few lines congratulating all the speakers for the amazing work they are doing with Julia, and most importantly I wanted to thank the organizers for the fantastic job they did: it really felt like an actual physical conference and the sense of community was truly awesome to experience. In the middle of all the talks I was still able to read a couple of papers and write some code, and this week work is a companion to the work done in week 6: expanding the research done in their previous article [1], they constructed a different type of cycle reservoir with random jumps and a different way to create an input layer [2]. In this post we will discuss the theory expressed in the paper and, after explaining the implementation in ReservoirComputing.jl, we will show how this construction performs on the tasks we takled in week 6. Cycle Reservoirs with Jumps and irrational sign input layer The costruction of Cycle Reservoirs with Jumps (CRJ) builds over the idea of the Simple Cycle Reservoir (SCR): contrary to the stadard construction of an Echo State Network (ESN) standard reservoir the two algorithms proposed are completely deterministic and really simple in nature. In the CRJ model the reservoir nodes are connected in a unidirectional cycle, as they are in the SCR model, with bidirectional shortcuts (called jumps). The value for the cycle connections are the same \\( r_c \u003e 0 \\), and all the jumps also share the same values \\( r_j \u003e 0 \\). The construction of the CRJ reservoir can be described in the following way: The lower subdiagonal of the reservoir \\( \\textbf{W} \\) is equal to the chosen \\( r_c \\) The upper right corner of reservoir \\( \\textbf{W} \\) is equal to the chosen \\( r_c \\) With a chosen jump size \\( 1 \u003c l \u003c [N/2] \\) if \\( (N \\text{mod}l) = 0 \\) then there are \\( [N/l] \\) jumps, the first being from unit 1 to unit \\( 1+l \\), the last from unit \\( N+1-l \\) to unit 1. If \\( (N \\text{mod}l) \\ne 0 \\) then there are \\( [N/l] \\) jumps, the last jump ending in unit \\( N+1-(N\\text{mod}l) \\). All the jumps have the same chosen value \\( r_j \\) Along with the construction of the CRJ model the paper [2] proposes a fully connected input layer with the same absolute value of the connection weight. The sign of the input weights is determined using the decimal expansion of an irrational number, \\( \\pi \\) being the choice of the authors. The first \\( N \\) digits \\( d_1, d_2,…,d_N \\) are taken and if \\( 0 \\le d_n \\le 4 \\) then the nth input will have sign - (minus), else if \\( 5 \\le d_n \\le 9 \\) it will have a + (plus) sign. ","date":"2028-82-08","objectID":"/posts/09_gsoc_week/:0:0","tags":null,"title":"GSoC week 9: Cycle Reservoirs with Regular Jumps","uri":"/posts/09_gsoc_week/"},{"categories":null,"content":"Implementation in ReservoirComputing A new function called CRJ() has been added to the reservoirs construction; this function takes as input res_size the size of the reservior cyrcle_weight the value of the weights \\( r_c \\) jump_weight the value of the weights \\( r_j \\) jump_size the number of jumps \\( l \\) and gives as output a reservoir matrix. In addition a function for the construction of the input layer has also been added. Denominated irrational_sign_input() it takes as input res_size the size of the reservior in_size the size of the input vector weight the absolute value of the connection weight irrational an optionl input, with default \\( \\pi \\), used for the determination of the sign for the connection weights Example To remain in line with the work done in the 6th week, and in order to be able to do a meaningful comparison, we are going to use the Henon map for our tests. The Henon map is defined as $$x_{x+1} = 1 - ax_n^2 + y_n$$ $$ y_{n+1} = bx_n $$ To obtaine the data for out tests we are going to use DynamicalSystems.jl. Before starting the work let’s download and inport all useful packages using Pkg Pkg.add(\"ReservoirComputing\") Pkg.add(\"Plots\") Pkg.add(\"DynamicalSystems\") Pkg.add(\"LinearAlgebra\") Pkg.add(\"Random\") using ReservoirComputing using Plots using DynamicalSystems using LinearAlgebra using Random Now we can generate the Henon map, and we will shift the data points by -0.5 and scale them by 2 to reproduce the data we had last time. The initial transient will be washed out and we will create two datasets called train and test: ds = Systems.henon() traj = trajectory(ds, 7000) data = Matrix(traj)' data = (data .-0.5) .* 2 shift = 200 train_len = 2000 predict_len = 3000 train = data[:, shift:shift+train_len-1] test = data[:, shift+train_len:shift+train_len+predict_len-1] Having the needed data we can proceed to the prediction tasks. ","date":"2028-82-08","objectID":"/posts/09_gsoc_week/:1:0","tags":null,"title":"GSoC week 9: Cycle Reservoirs with Regular Jumps","uri":"/posts/09_gsoc_week/"},{"categories":null,"content":"One step ahead prediction For sake of comparison we are going to use the same values as last time for the construction of the ESN: approx_res_size = 100 radius = 0.3 sparsity = 0.5 sigma = 1.0 beta = 1*10^(-1) extended_states = true input_weight = 0.95 cyrcle_weight = 0.95 jump_weight = 0.2 jumps = 5 Since this task was not used in the paper [2] the new parameters jump_weight and jumps are obtained using a manual grid search and as such are probably not as optimized as the other values. We can proceed to the construction of the ESN with the CRJ reservoir and irrational-determined input layer: @time W = CRJ(approx_res_size, cyrcle_weight, jump_weight, jumps) W_in = irrational_sign_input(approx_res_size, size(train, 1), input_weight) esn_crj = ESN(W, train, W_in, extended_states = extended_states) 0.000053 seconds (6 allocations: 78.359 KiB) Following the procedure we used lst time, in order to test the accuracy of the prediction we are going to use the Normalized Mean Square Error (NMSE), defined as $$NMSE = \\frac{\u003c||\\hat{y}(t)-y(t)||^2\u003e}{\u003c||y(t)-\u003cy(t)\u003e||^2\u003e}$$ where \\( \\hat{y}(t) \\) is the readout output \\( y(t) \\) is the target output \\( \u003c\\cdot\u003e \\) indicates the empirical mean \\( ||\\cdot|| \\) is the Euclidean norm. A simple NMSE function can be created following: function NMSE(target, output) num = 0.0 den = 0.0 sums = [] for i=1:size(target, 1) append!(sums, sum(target[i,:])) end for i=1:size(target, 2) num += norm(output[:,i]-target[:,i])^2.0 den += norm(target[:,i]-sums./size(target, 2))^2.0 end nmse = (num/size(target, 2))/(den/size(target, 2)) return nmse end Testing the one step ahead predicting capabilities of this new implementation we obtain: wout = ESNtrain(esn_crj, beta) output = ESNpredict_h_steps(esn_crj, predict_len, 1, test, wout) println(NMSE(test, output)) 0.0010032069150514866 This result outperforms all the architectures tested in week 6, getting a little closer to the standard ESN implementation result. Even though this task is not present in the paper the better results shows that the implementation is valid nevertheless. ","date":"2028-82-08","objectID":"/posts/09_gsoc_week/:2:0","tags":null,"title":"GSoC week 9: Cycle Reservoirs with Regular Jumps","uri":"/posts/09_gsoc_week/"},{"categories":null,"content":"Attractor reconstruction Following the work done in week 6 we want to explore the capabilities of this construction in the reconstruction of the chaotic attractor of the Henon map. Using the already built ESN we will predict the system for predict_len steps and at the end we will plot the results to see if they are in line with the one obtained with the other architectures. To refresh our memory we will start by plotting the actual Henon map: scatter(test[1,:], test[2,:], label=\"actual\") Let’s see if the CRJ-based ESN is capable of reproducing the climate of this attractor: wout = ESNtrain(esn_crj, beta) output = ESNpredict(esn_crj, predict_len, wout) scatter(output[1,:], output[2, :], lable = \"ESN-CRJ\") The result is actually more clear cut then the results obtained in the 6th week. This architecture seems to be able to represent the attractor in a more precise manner. Both the tests we have done have resulted in a better performance with respect to the other deterministic constructions for reservoirs and input layer. A more statistical accurate exploration is of course needed but both our results and the results found in the paper show the capabilities of this new implementation of a deterministic reservoir. As always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me! ","date":"2028-82-08","objectID":"/posts/09_gsoc_week/:3:0","tags":null,"title":"GSoC week 9: Cycle Reservoirs with Regular Jumps","uri":"/posts/09_gsoc_week/"},{"categories":null,"content":"Documentation [1] Rodan, Ali, and Peter Tino. “Minimum complexity echo state network.” IEEE transactions on neural networks 22.1 (2010): 131-144. [2] Rodan, Ali, and Peter Tiňo. “Simple deterministically constructed cycle reservoirs with regular jumps.” Neural computation 24.7 (2012): 1822-1852. ","date":"2028-82-08","objectID":"/posts/09_gsoc_week/:4:0","tags":null,"title":"GSoC week 9: Cycle Reservoirs with Regular Jumps","uri":"/posts/09_gsoc_week/"},{"categories":null,"content":"Continuing the work started last week we are going to further explore the capabilities of Reservoir Computing using Cellular Automata (CA) as the reservoir. As always a little theorical introduction is given and then we will illustrate the use of the model implemented in ReservoirComputing.jl. Reservoir Computing with Two Dimensional Cellular Automata ","date":"26267-726-07","objectID":"/posts/08_gsoc_week/:0:0","tags":null,"title":"GSoC week 8: Reservoir Computing with Cellular Automata Part 2","uri":"/posts/08_gsoc_week/"},{"categories":null,"content":"Two Dimensional Cellular Automata (Conway’s Game of Life) In the previous week we used Elementary CA (ECA) to train our model, and this time we want to see if we are able to obtain similar results using a two dimensional CA. As proposed in [1] we are going to use Conway’s Game of Life [2] (GoL), so a little introduction to this model is essential to proceed. Conway’s Game of Life (GoL) is an example of two-dimensional CA with a Moore neighborhood with range $r=1$ [3], defined as: $$ N^{M}_{(x_0, y_0)} = {(x, y):|x-x_0| \\le r, |y-y_0| \\le r } $$ where $(x_0, y_0)$ is the given cell. In the standard GoL format each cell in the grid can be in either of two states: dead or alive (identified respectively with 0 and 1). The transition rules are determined as follows: Any alive cell with fewer than two alive cells in its neighborhood will transition to a dead state in the next generation Any alive cell with two or three alive cells in its neighborhood will remain alive in the next generation Any alive cell with more than three alive neighbors will transition to a dead state in the next generation Any dead cell with three alive neighbors will transition to an alive state in the next generation This CA shows class 4 behavior, neither completely random nor completely repetitive. It is also capable of universal computation and it’s Turing complete [4]. We can obtain a GIF of the system using the package ReservoirComputing and Plots in a couple of lines of code: first let’s import the packages using ReservoirComputing using Plots We can now define the variables for the GoL CA, namely dimensions and generations, and defining the GoL object at the same time: size = 100 generations = 250 @time gol = GameOfLife(rand(Bool, size, size), generations); 0.091884 seconds (8 allocations: 2.394 MiB) and now we can plot the obtiained GoL system: @gif for i=1:generations heatmap(gol.all_runs[:, :, i], color=cgrad([:white,:black]), legend = :none, axis=false) plot!(size=(500,500)) end As we can see, starting from a random position, we obtained an evolving GoL system. ","date":"26267-726-07","objectID":"/posts/08_gsoc_week/:1:0","tags":null,"title":"GSoC week 8: Reservoir Computing with Cellular Automata Part 2","uri":"/posts/08_gsoc_week/"},{"categories":null,"content":"Game of Life reservoir Architecture Since the data used for testing in the literature is also binary in nature, in order to feed it to the reservoir, the method proposed in [1] was based on randomly projecting the input data into the reservoir, whose size should follow that of the input data. This means that for an input of dimension $L_{in}=4$ the size of the reservoir would have been $m=2 \\times 2$. This procedure was repeated a number $R$ of times, effectively creating $R$ different reservoirs. These reservoirs were then connected and the information was allowed to flow between them, in order to obtain an higher dimensional reservoir. This architecture has showed the capability to correctly solve the 5 bit and 20 bit memory task. In the implementation in ReservoirComputing.jl we want to propose an expansion of the encoding method, also capable of solving the 5 bit memory task. Following intuitions given by more recent papers in the field of ReCA, in particular [5] and [6], we decided to input the data to the reservoir using $T$ random projections into an higher dimension matrix. This way the initial state has room to expand and memory of the precedent state is conserved. The procedure is similar to that described by [1], and is illustrated in the figure. Let $\\text{X}_1$ be the first input vector. This will be randomly mapped onto a matrix of zeros $T$ times using a fixed mapping scheme $[\\text{P}_1, \\text{P}_2, …, \\text{P}_{\\text{R}}]$ in order to form the initial configuration $\\text{A}_0^{(1)}$ for the GoL. The transition function $Z$, the rules of GoL, is then applied for $I$ generations: $$\\text{A}_{1}^{(1)}=\\text{Z}(\\text{A}_0^{(1)})$$ $$ \\text{A}_{2}^{(1)} = \\text{Z}(\\text{A}_{1}^{(1)}) $$ $$ \\vdots $$ $$ \\text{A}_{\\text{I}}^{(1)} = \\text{Z}(\\text{A}_{\\text{I}-1}^{(1)}) $$ This constitutes the evolution of the CA given the input $\\text{X}_1$. In order to create the state vector we need to vectorize and concatenate the matrices we obtained. Identifying with $\\text{A}_{0, 1}^{(1)}$ the first column of $\\text{A}_0^{(1)}$, let $c$ be the total number of columns of $\\text{A}_0^{(1)}$, then the vectorization of $\\text{A}_0^{(1)}$ will be $$\\text{v}\\text{A}_0^{(1)} = [\\text{A}_{0, 1}^{(1)}, \\text{A}_{0, 2}^{(1)}, …, \\text{A}_{0, c}^{(1)}]$$ This procedure is done for every timestep $I$, and at the end the vector state $\\textbf{x}^{(1)}$ will be $$\\textbf{x}^{(1)} = [\\text{v}\\text{A}_0^{(1)}, \\text{v}\\text{A}_1^{(1)}, …, \\text{v}\\text{A}_{I}^{(1)}]$$ An illustration of this process can be seen in figure. To feed the second input vector $\\text{X}_2$ we use the same mapping created in the first step. Instead of using an initial empty matrix this time we will project the input over the matrix representing the last evolution of the prior step, $\\text{A}_{\\text{I}}^{(1)}$. The matrix thus obtained is evolved as described above, to obtain the state vectors for the second input vector. This procedure is repeated for every input vector. The training is carried out using Ridge Regression. Example For example we will try to reproduce the 5 bit memory task, described last week. If you want to follow along and experiment with the model, the data can be found here: the 5bitinput.txt is the input data and the 5bitoutput is the desired output. To read the data we can use the following using DelimitedFiles input = readdlm(\"./5bitinput.txt\", ',', Int) output = readdlm(\"./5bitoutput.txt\", ',', Int) Now that we have the data we can train the model and see if it is capable of solving the 5 bit memory task with a distractor period of 200. using ReservoirComputing reca = RECA_TwoDim(input, 30, 10, 110) W_out = ESNtrain(reca, 0.001; train_data = convert(AbstractArray{Float64}, output)) reca_output = RECATDdirect_predict_discrete(reca, W_out, input) reca_output == output true It seems that for architecture used in this example the task is easily solvable. A more deep investigation can be made iterating over different values of reservoir size, permu","date":"26267-726-07","objectID":"/posts/08_gsoc_week/:2:0","tags":null,"title":"GSoC week 8: Reservoir Computing with Cellular Automata Part 2","uri":"/posts/08_gsoc_week/"},{"categories":null,"content":"Documentation [1] Yilmaz, Ozgur. “Reservoir computing using cellular automata.” arXiv preprint arXiv:1410.0162 (2014). [2] Gardner, Martin. “Mathematical games: The fantastic combinations of John Conway’s new solitaire game “life”.” Scientific American 223.4 (1970): 120-123. [3] Weisstein, Eric W. “Moore Neighborhood.” From MathWorld–A Wolfram Web Resource. https://mathworld.wolfram.com/MooreNeighborhood.html [4] Wolfram, Stephen. A new kind of science. Vol. 5. Champaign, IL: Wolfram media, 2002. [5] Margem, Mrwan, and Osman S. Gedik. “Feed-forward versus recurrent architecture and local versus cellular automata distributed representation in reservoir computing for sequence memory learning.” Artificial Intelligence Review (2020): 1-30. [6] Nichele, Stefano, and Andreas Molund. “Deep reservoir computing using cellular automata.” arXiv preprint arXiv:1703.02806 (2017). ","date":"26267-726-07","objectID":"/posts/08_gsoc_week/:3:0","tags":null,"title":"GSoC week 8: Reservoir Computing with Cellular Automata Part 2","uri":"/posts/08_gsoc_week/"},{"categories":null,"content":"In the past few years a new framework based on the concept of Reservoir Computing has been proposed: the Cellular Automata based Reservoir Computer (ReCA) [1]. The advantage it proposes over standard implementations is given by the binary state of the reservoir and the fact that it doesn’t require much parameter tuning to obtain state of the art results. Since the initial conception of the use of ECA for reservoir computing numerous improvement have taken place. A recurrent design, together with the ReCA denomination, has been proposed in [2], and new methods for states encoding are studied in [3]. Also the use of two reservoir is studied in [4], as well as the implementation of two different rules, staked both horizontally [5] and vertically [6]. Lastly an exploration of complex rules is done in [7]. In this post we will illustrate the implementation in ReservoirComputing.jl of the general model, based on the architecture illustrated in [4] which build over the original implementation, improving the results. As always we will give an initial theoretical introduction, and then some examples of applications will be shown. Reservoir Computing with Elementary Cellular Automata ","date":"19197-719-07","objectID":"/posts/07_gsoc_week/:0:0","tags":null,"title":"GSoC week 7: Reservoir Computing with Cellular Automata Part 1","uri":"/posts/07_gsoc_week/"},{"categories":null,"content":"Elementary Cellular Automata Initially introduced by Van Neumann as self-reproducing machines [8] Cellular Automata (CA) is a dynamical computational model based on a regular grid, of arbitrary dimensions, composed by cells. These cells can be in a different number of states and are updated according to a specific rule \\( f \\) which takes as an input the cell itself and its neighborhood and gives as output the state of the cell in the next generation. All the cells are updated simultaneously making the CA a discrete system with respect to time. The rule space is determined by the number of states and the number of possible neighbors. Let \\( K \\) be the number of states and \\( S \\) the the number of neighbors including the cell itself, then the possible number of neighborhood sates is given by \\( K^S \\). Since each element is transitioning to one of \\( K \\) states itself the transition function space is \\( K^{K^S} \\) [9]. Elementary cellular automata (ECA) are defined by a one dimensional grid of cells that are in one of two states, usually represented by 0 and 1. Each cell \\( x \\) updates its state \\( x_i^t \\) depending on the states of its two neighbors \\( x_{i-1}^t \\) and \\( x_{i+1}^t \\) according to the transition function \\( f:{0,1}^3 \\rightarrow {0,1} \\). There are \\( 2^8=256 \\) elementary rules [10] that can be identified by numbers ranging from 0 to 255 taking the output table of each function as binary encoding of a digital number [11]. An example of rule 30 can be observed below. Thanks to symmetries this rules con be grouped into 88 classes with equivalent characteristics [12]. Another distinction can be made, grouping the ECAs according to the general behavior they display. The first step in this direction was done by Wolfram [13], that identified four classes with the following description: Class 1: CA states evolve to a homogeneous behavior Class 2: CA states evolve periodically Class 3: CA states evolve with no defined pattern Class 4: can show all evolution patterns in an unpredictable manner A more refined analysis by Li and Packard divided the Class 2 into two different sub-classes, distinguishing between fixed point and periodic CA. Class 3 rules are defined as globally chaotic and class 4 are considered difficult to include in specific categories. ","date":"19197-719-07","objectID":"/posts/07_gsoc_week/:1:0","tags":null,"title":"GSoC week 7: Reservoir Computing with Cellular Automata Part 1","uri":"/posts/07_gsoc_week/"},{"categories":null,"content":"ReCA Architecture In the first stage the input needs to be mapped into the CA system. In the literature the ReCA approach has only been tested with binary test sets, so the chosen procedure for the input data is to translate directly the input onto the first state of the CA. In the original design [1] this was done by a random permutation of the elements of the input vector in a vector of the same dimension, $\\text{L}_{\\text{in}}$. The reservoir was then composed of \\( \\text{R} \\) different ECA systems, each of which had a different random mapping as encoder. The evolution was done using the combination of the \\( \\text{R} \\) reservoirs, so that the information could flow between one and the other. This approach yielded better results than letting them evolve separately. The starting vector for the ECA system is then the combination of the \\( \\text{R} \\) mappings of the starting input vector, making it of dimensions $\\text{R} \\cdot \\text{L}_{\\text{in}}$. An improvement over the here discussed method, proposed in [4], is to map the input into a different sized vector $\\text{L}_{\\text{d}}$, with $\\text{L}_{\\text{d}} \u003e \\text{L}_{\\text{in}}$, padded with zeros. The higher dimension of the input vector allows the CA system to evolve with more freedom. Using a number of recombinations \\( \\text{R} \\) the input vector to the CA system will be of dimensions $\\text{R} \\cdot \\text{L}_{\\text{d}}$. At the boundaries of the CA are used periodic boundary conditions (PBC), so that the last cell is neighbor with the first one. Let $\\text{X}_1$ be the first input vector. This will be randomly mapped onto a vector of zeros \\( \\text{R} \\) times using a fixed mapping scheme $[\\text{P}_1, \\text{P}_2, …, \\text{P}_{\\text{R}}]$ and concatenated to form the initial configuration $\\text{A}_0$ for the CA: $$\\text{A}_0^{(1)} = [\\text{X}_{1}^{\\text{P}_{1}}, \\text{X}_{1}^{\\text{P}_{2}}, …, \\text{X}_{1}^{\\text{P}_{\\text{R}}}]$$ The transition function Z is then applied for I generations: $$\\text{A}_{1}^{(1)} = \\text{Z}(\\text{A}_0^{(1)})$$ $$\\text{A}_{2}^{(1)} = \\text{Z}(\\text{A}_{1}^{(1)})$$ $$\\vdots$$ $$\\text{A}_{\\text{I}}^{(1)} = \\text{Z}(\\text{A}_{\\text{I}-1}^{(1)})$$ This constitutes the evolution of the CA given the input $\\text{X}_1$. In the standard ReCA approach the state vector is the concatenation of all the steps $\\text{A}_{1}^{(1)}$ through $\\text{A}_{\\text{I}}^{(1)}$ to form $\\text{A}^{(1)} = [\\text{A}_{1}^{(1)}, \\text{A}_{2}^{(1)}, …, \\text{A}_{\\text{I}}^{(1)}]$. The final states matrix, of dimensions $\\text{R} \\cdot \\text{L}_{\\text{d}} \\times \\text{T}$, is obtained stacking the state vectors column wise, in order to obtain: $\\textbf{X}=[\\text{A}^{(1) \\text{T}}, \\text{A}^{(2) \\text{T}}, …, \\text{A}^{(\\text{T}) \\text{T}}]$. For the training technically every method we have implemented could be used, but in this first trial we just used the Ridge Regression. In the original paper the use of the pseudo-inverse was opted. Implementation in ReservoirComputing.jl Following the procedure described above we implemented in ReservoirComputing.jl a RECA_discrete object and a RECAdirect_predict_discrete function. The goal was to reproduce the results found in the literature, so the discrete approach was the only way to ensure that our implementation is correct. One of the goals is to expand this architecture to be also able to predict continuous values, such as timeseries. In this week an effort in this direction was made, but further exploration is needed. The RECA_discrete constructor takes as input train_data the data needed for the ReCA training rule the ECA rule for the reservoir generations the number of generations the ECA will expand in expansion_size the \\( L_d \\) parameter permutations the number of additional ECA for the reservoir training nla_type the non linear algorithm for the reservoir states. Default is NLADefalut() The training is done using the already implemented ESNtrain, that will probably need a name change in the future since now it ","date":"19197-719-07","objectID":"/posts/07_gsoc_week/:2:0","tags":null,"title":"GSoC week 7: Reservoir Computing with Cellular Automata Part 1","uri":"/posts/07_gsoc_week/"},{"categories":null,"content":"Documentation [1] Yilmaz, Ozgur. “Reservoir computing using cellular automata.” arXiv preprint arXiv:1410.0162 (2014). [2] Margem, Mrwan, and Ozgür Yilmaz. “An experimental study on cellular automata reservoir in pathological sequence learning tasks.” (2017). [3] Margem, Mrwan, and Osman S. Gedik. “Feed-forward versus recurrent architecture and local versus cellular automata distributed representation in reservoir computing for sequence memory learning.” Artificial Intelligence Review (2020): 1-30. [4] Nichele, Stefano, and Andreas Molund. “Deep reservoir computing using cellular automata.” arXiv preprint arXiv:1703.02806 (2017). [5] Nichele, Stefano, and Magnus S. Gundersen. “Reservoir computing using non-uniform binary cellular automata.” arXiv preprint arXiv:1702.03812 (2017). [6] McDonald, Nathan. “Reservoir computing \u0026 extreme learning machines using pairs of cellular automata rules.” 2017 International Joint Conference on Neural Networks (IJCNN). IEEE, 2017. [7] Babson, Neil, and Christof Teuscher. “Reservoir Computing with Complex Cellular Automata.” Complex Systems 28.4 (2019). [8] Neumann, János, and Arthur W. Burks. Theory of self-reproducing automata. Vol. 1102024. Urbana: University of Illinois press, 1966. [9] Bia_ynicki-Birula, Iwo, and Iwo Bialynicki-Birula. Modeling Reality: How computers mirror life. Vol. 1. Oxford University Press on Demand, 2004. [10] Wolfram, Stephen. A new kind of science. Vol. 5. Champaign, IL: Wolfram media, 2002. [11] Adamatzky, Andrew, and Genaro J. Martinez. “On generative morphological diversity of elementary cellular automata.” Kybernetes (2010). [12] Wuensche, Andrew, Mike Lesser, and Michael J. Lesser. Global Dynamics of Cellular Automata: An Atlas of Basin of Attraction Fields of One-Dimensional Cellular Automata. Vol. 1. Andrew Wuensche, 1992. [13] Wolfram, Stephen. “Universality and complexity in cellular automata.” Physica D: Nonlinear Phenomena 10.1-2 (1984): 1-35. [14] Hochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” Neural computation 9.8 (1997): 1735-1780. ","date":"19197-719-07","objectID":"/posts/07_gsoc_week/:3:0","tags":null,"title":"GSoC week 7: Reservoir Computing with Cellular Automata Part 1","uri":"/posts/07_gsoc_week/"},{"categories":null,"content":"Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning. And even when the optimal parameters are found, the prediction is run-dependent and can show different results with different generations of the reservoir. Is this the only way possible to contruct an Echo State Network (ESN)? Is there a deterministic way to build a ESN? These are the question posed in [1], and the following post is an illustration of the implementation in ReservoirComputing.jl of their construction of a deterministic input layer and three reservoirs. As always we will quickly lay out the theory, then an example will be given. Minimum complexity reservoir and input layer The usual construction of a reservoir implies the creation of a random sparse matrix, with given sparsity and dimension, and following rescaling of the values in order to have set the spectral radius to be under a determined value, usually one, in order to ensure the Echo State Property (ESP) [2]. As already stated in the work done in the 4th week, this construction, although efficient, could have some downsides. The particular problem we want to solve with the current implementation is the one given by the randomness of the process: both the reservoir and the input layer construction are initially generated as random and later rescaled. The paper we are following for a possible solution [1] introduces three different constructions for a deterministic reservoir: Delay Line Reservoir (DLR): is composed of units organized in a line. The elements of the lower subdiagonal of the reservoir matrix have non-zero values, and all are the same. DLR with backward connections (DLRB): based on the DLR each reservoir unit is also connected to the preceding neuron. This is obtained setting as non-zero the elements of both the upper and lower subdiagonal, with two different values. Simple Cycle Reservoir (SCR): is composed by units organized in a cycle. The non-zero elements of the reservoir are the lower subdiagonal and the upper right corner, all set to the same weight. In addition to these reservoirs, also a contruction for the input layer is given: all input connections have the same absolute weight and the sign of each value is determined randomly by a draw from a Bernoulli distribution of mean 1/2. In the paper is stated that any other imposition of sign over the input weight deteriorates the results, so a little randomness is manteined even in this construction, but of course is still far from the original implementation. ","date":"12127-712-07","objectID":"/posts/06_gsoc_week/:0:0","tags":null,"title":"GSoC week 6: minimum complexity echo state network","uri":"/posts/06_gsoc_week/"},{"categories":null,"content":"Implementation in ReservoirComputing The implementation of the construction of reservoir and input layer as described in the paper is straightforward: following the instructions we created three different functions for the reservoir named DLR(), DLRB() and SCR() that take as input res_size the size of the reservoir weight the value for the weights fb_weight the value for the feedback weights, only needed for the DLRB() function. The result of each function is a reservoir matrix with the requested construction. In addition we also added a min_complex_input function, taking as input res_size the size of the reservoir in_size the size of the input array weight the value of the weights and giving as output the minimum complexity input layer. Example For this example we are goind to use the Henon map, defined as $$x_{x+1} = 1 - ax_n^2 + y_n$$ $$ y_{n+1} = bx_n $$ The attractor depends on the two values \\( a, b \\) and shows chaotic behaviour for the classical values of \\( a=1.4 \\) and \\( b=0.3 \\). To obtain a dataset for the Henon map this time we will use the DynamicalSystems package. Before starting the work we will need to download all the necessary utilies and import them: using Pkg Pkg.add(\"ReservoirComputing\") Pkg.add(\"Plots\") Pkg.add(\"DynamicalSystems\") Pkg.add(\"Statistics\") Pkg.add(\"LinearAlgebra\") Pkg.add(\"Random\") using ReservoirComputing using Plots using DynamicalSystems using Statistics using LinearAlgebra using Random Now we can generate the Henon map, which will be shifted by -0.5 and scaled by 2, in order to have consistency with the paper. At the same time we are going to wash out any initial transient and construct the training, train, and testing, test, datasets, following the values given by the paper: ds = Systems.henon() traj = trajectory(ds, 7000) data = Matrix(traj)' data = (data .-0.5) .* 2 shift = 200 train_len = 2000 predict_len = 3000 train = data[:, shift:shift+train_len-1] test = data[:, shift+train_len:shift+train_len+predict_len-1] ","date":"12127-712-07","objectID":"/posts/06_gsoc_week/:1:0","tags":null,"title":"GSoC week 6: minimum complexity echo state network","uri":"/posts/06_gsoc_week/"},{"categories":null,"content":"One step ahead prediction Now we can set the parameters for the construction of the ESN, for which we followed closely the ones given in the paper, outside for the ridge regression value. Note that since some values are corresponding to our default (activation function, alpha and non linear algorithm) we will omit them for clarity. approx_res_size = 100 radius = 0.3 sparsity = 0.5 sigma = 1.0 beta = 1*10^(-1) extended_states = true input_weight = 0.95 r= 0.95 b = 0.05 We can now build both the standard ESN and three other ESNs based on the novel reservoir implementation. We are going to need the four of them for a comparison of the results: Random.seed!(17) #fixed seed for reproducibility @time W = init_reservoir_givensp(approx_res_size, radius, sparsity) W_in = init_dense_input_layer(approx_res_size, size(train, 1), sigma) esn = ESN(W, train, W_in, extended_states = extended_states) Winmc = min_complex_input(size(train, 1), approx_res_size, input_weight) @time Wscr = SCR(approx_res_size, r) esnscr = ESN(Wscr, train, Winmc, extended_states = extended_states) @time Wdlrb = DLRB(approx_res_size, r, b) esndlrb = ESN(Wdlrb, train, Winmc, extended_states = extended_states) @time Wdlr = DLR(approx_res_size, r) esndlr = ESN(Wdlr, train, Winmc, extended_states = extended_states) 0.012062 seconds (33 allocations: 359.922 KiB) 0.000020 seconds (6 allocations: 78.359 KiB) 0.000019 seconds (6 allocations: 78.359 KiB) 0.000019 seconds (6 allocations: 78.359 KiB) In order to test the accuracy of the predictions given by different architectures we are going to use the Normalized Mean Square Error (NMSE), defined as $$NMSE = \\frac{\u003c||\\hat{y}(t)-y(t)||^2\u003e}{\u003c||y(t)-\u003cy(t)\u003e||^2\u003e}$$ where \\( \\hat{y}(t) \\) is the readout output, \\( y(t) \\) is the target output, \\( \u003c\\cdot\u003e \\) indicates the empirical mean and \\( ||\\cdot|| \\) is the Euclidean norm. A simple NMSE function is created: function NMSE(target, output) num = 0.0 den = 0.0 sums = [] for i=1:size(target, 1) append!(sums, sum(target[i,:])) end for i=1:size(target, 2) num += norm(output[:,i]-target[:,i])^2.0 den += norm(target[:,i]-sums./size(target, 2))^2.0 end nmse = (num/size(target, 2))/(den/size(target, 2)) return nmse end Now we can iterate and test the output of all the different implementations in a one step ahead prediction task: esns = [esn, esndlr, esndlrb, esnscr] for i in esns W_out = ESNtrain(i, beta) output = ESNpredict_h_steps(i, predict_len, 1, test, W_out) println(NMSE(test, output)) end 0.000766235182367319 0.0013015853534120024 0.0011355988458350088 0.001843450482139491 The standard ESN shows the best results, but the NMSE given by the minimum complexity ESNs are actually not bad. The results are better than those presented in the paper for all the architectures so they are not directly comparable, but the best performing ESN between the minimum complexity ones seems to be the DLRB-based, something that is also true in the paper. ","date":"12127-712-07","objectID":"/posts/06_gsoc_week/:2:0","tags":null,"title":"GSoC week 6: minimum complexity echo state network","uri":"/posts/06_gsoc_week/"},{"categories":null,"content":"Attractor reconstruction Now we want to venture into something that is not done in the paper: we want to see if this deterministic implementation of reservoirs and input layers are capable of reconstructing the Henon attractor. We will use the ESNs already built and we will predict the system for predict_len steps to see if the behaviour is manteined. We will do so only through an eye test, but it should suffice to have a general idea of the capabilities of these reservoirs. To start we will plot the actual data, in order to have something to compare the resuls to: scatter(test[1,:], test[2,:], label=\"actual\") Now let’s see if the standard ESN is able to predict correctly this attractor wout = ESNtrain(esn, beta) output = ESNpredict(esn, predict_len, wout) scatter(output[1,:], output[2,:], label=\"ESN\") Not bad, but we already know the capabilities of the ESN. We are here to test the minimum complexity construction, so let us start with DLR wout = ESNtrain(esndlr, beta) output = ESNpredict(esndlr, predict_len, wout) scatter(output[1,:], output[2,:], label=\"ESN-DLR\") The predictions are not as clear cut as we would like, but the behaviour is manteined nevertheless. Actually impressive considering the simple construction of the reservoir. Trying the two other constructions gives the following: wout = ESNtrain(esndlrb, beta) output = ESNpredict(esndlrb, predict_len, wout) scatter(output[1,:], output[2,:], label=\"ESN-DLRB\") wout = ESNtrain(esnscr, beta) output = ESNpredict(esnscr, predict_len, wout) scatter(output[1,:], output[2,:], label=\"ESN-SCR\") The results are somewhat similar between each other, and a deeper quantitative analysis is needed to determine the best performing construction, but this was not the aim of this post. We wanted to see if these basic implementations of reservoirs and input layers were capable not only of maintaining a short term prediction capability, but also if they were still able to mimic the behaviour of a chaotic attractor in the long term and it seems that both of these statements are proven to be correct. This seminal paper not only sheds light on the still inexplored possibilities of ESN reservoir constructions, but also shows that very little complexity is needed for this model to obtain very good results in a short amount of time. As always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me! ","date":"12127-712-07","objectID":"/posts/06_gsoc_week/:3:0","tags":null,"title":"GSoC week 6: minimum complexity echo state network","uri":"/posts/06_gsoc_week/"},{"categories":null,"content":"Documentation [1] Rodan, Ali, and Peter Tino. “Minimum complexity echo state network.” IEEE transactions on neural networks 22.1 (2010): 131-144. [2] Yildiz, Izzet B., Herbert Jaeger, and Stefan J. Kiebel. “Re-visiting the echo state property.” Neural networks 35 (2012): 1-9. ","date":"12127-712-07","objectID":"/posts/06_gsoc_week/:4:0","tags":null,"title":"GSoC week 6: minimum complexity echo state network","uri":"/posts/06_gsoc_week/"},{"categories":null,"content":"This post is meant to be a high level comparison of different variations of the Echo State Network (ESN) model, implemented in the first month of Google Summer of Code. The theoretical background for all the proposed models has already been covered in past posts, so we will not touch on it in this one to keep things as thight as possible; if one is interested all my previous posts can be found here. The ESNs are known for their capability of yielding good short term predictions and long term reconstructions of chaotic systems: in order to prove this we are going to test all the proposed models using the Lorenz system. In order to determine the accuracy of the results we will use two different methods: For the short term accuracy we chose an arbitrary time horizon and the difference between the actual timeseries, obtained solving the differential equations of the Lorenz system, and the predicted timeseries will be evaluated using the Root Mean Square Deviation (RMSE). The rmse implementation in Julia is done with the following function function rmse(y, yt) rmse = 0.0 for i=1:size(y, 1) rmse += (y[i]-yt[i])^2.0 end rmse = sqrt(rmse/(size(y, 1))) return rmse end For the long term climate we chose to follow the approach of Pathak [1] and we will show the return map of successive maxima of \\( z(t) \\). To get this data we leveraged the function findlocalmaxima of the package Images.jl. The Julia function used to get the vector of local maxima is defined as follows using Images function local_maxima(input_data) maxs_cart = findlocalmaxima(input_data) maxs = [idx[1] for idx in maxs_cart] max_values = [] for max in maxs push!(max_values, input_data[max]) end return max_values end The data used for all the training and prediction for the ESN in this work is obtained in the following way: u0 = [1.0,0.0,0.0] tspan = (0.0,2000.0) p = [10.0,28.0,8/3] #define lorenz system function lorenz(du,u,p,t) du[1] = p[1]*(u[2]-u[1]) du[2] = u[1]*(p[2]-u[3]) - u[2] du[3] = u[1]*u[2] - p[3]*u[3] end #solve and take data prob = ODEProblem(lorenz, u0, tspan, p) sol = solve(prob, RK4(), adaptive=false, dt=0.02) v = sol.u data = Matrix(hcat(v...)) shift = 300 train_len = 5000 predict_len = 1250 return_map_size = 20000 train = data[:, shift:shift+train_len-1] test = data[:, shift+train_len:shift+train_len+predict_len-1] return_map = data[:,shift+train_len:shift+train_len+return_map_size-1]; Where the test data will be used for display and the first 400 timesteps for the short term prediction. The return_map data will instead be used for the creation of the return maps. Ordinary ESN The Ordinary Least Squares (OLS) trained ESN is the model used in [1] to accurately predict the Lorenz system in the short term and replicate its climate in the long term. We will use the same construction given in their paper, and most of these parameters will be used also for the other variation presented in this post. The parameters and the training for the ESN are as follows using ReservoirComputing using Random approx_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 0.0 alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(esn, beta) output = ESNpredict(esn, predict_len, W_out) 1.963047 seconds (5.79 M allocations: 310.671 MiB, 4.29% gc time) We can plot a comparison to have a visual feedback for the coordinates plot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") From just a quick eye test we can see that the short term prediction is rather good, and in the long term the behaviour seems to be in line with the numerical solution. For the short term prediction we are going to use the already defined rmse function an all three the variables to check the accuracy. The arbitrary length of the short ","date":"5057-75-07","objectID":"/posts/05_gsoc_week/:0:0","tags":null,"title":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","uri":"/posts/05_gsoc_week/"},{"categories":null,"content":"Ridge ESN One usual problem that can be encountered when dealing with OLS is the insurgence of numerical instabilities when inverting \\( (\\textbf{X} \\textbf{X}^T) \\) [2] where \\( \\textbf{X} \\) is the feature matrix (states matrix in the case of ESNs). A solution to this is to apply a regularization to the loss function, and one of the most common is the \\( L_2 \\) regularization. This way we obtain what is called ridge regression or Tikhonov regularization. The ridge ESN is trained in an equal manner as the OLS ESN we discussed above, only setting a parameter beta different than zero. The parameter that we chose is by no mean optimized and it is chosen by manual search, and this holds sadly for all the parameters in the models here presented. In the Conclusions section we will talk a little more about this aspect of this work. approx_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 0.001 alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(esn, beta) output = ESNpredict(esn, predict_len, W_out); 2.223003 seconds (5.79 M allocations: 310.671 MiB, 2.85% gc time) This methods is just a little slower, as it has to be expected. It is still an acceptable by any means. Plotting the results we obtain: plot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") The behaviour is similar to the standard ESN, but let’s take a look at the short and long term. for i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 5.3658081215986 6.552586461430827 4.995926155420491 It was clearly visible before that the short term behaviour was not as good as the standard counterpart. The long term predictions are still acceptable, as we can see here: output_map = ESNpredict(esn, return_map_size, W_out) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) Clearly in this case the original architecture proves superior in the short term, but in the long term the both are really viable. Depending on the situation and dataset the ridge ESN can be a valide choice for accuracy and speed of training, and it could also be the only viable choice between the two, if the problem is ill-posed. ","date":"5057-75-07","objectID":"/posts/05_gsoc_week/:1:0","tags":null,"title":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","uri":"/posts/05_gsoc_week/"},{"categories":null,"content":"Lasso ESN Another common regularization for the OLS regression is the \\( L_1 \\) regularization, resulting in a regression model called Lasso. This is a stronger regularization, and it shows from the results. The ESN is built in the same way as before, only this time the parameter beta indicates the Lasso regularizer. Since the expression doesn’t have a closed form solution we will need a different solver, in this case ProxGrad. For this we will need to import a different package, called MLJLinearModels.jl. using MLJLinearModels approx_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 1*10^(-7) alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(Lasso(beta, ProxGrad(max_iter=10000)), esn) output = ESNpredict(esn, predict_len, W_out) 24.400234 seconds (9.82 M allocations: 1.534 GiB, 1.17% gc time) The training time is slower than the couple of models we showed before. This difference is mainly to the already mentioned lack of closed form solution for the Lasso regression. Let us plot the results to start the analysis the results. plot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") It is clear that this regularization is not capable of returning an accurate prediction, both short term and long term. Let’s print the rmse for i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 11.769042467212172 13.599065187854675 10.651859641213985 and plot the return map output_map = ESNpredict(esn, return_map_size, W_out) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) From this results is clear that the \\( L_1 \\) regularization is not capable of good short term prediction and in the long term yields a periodic timeseries, as we can see from the return map, only showing values in 5 contained regions. Huber loss function Not only the squared function can be used as a loss function: in literature it has also been proposed the use of the Huber loss function, supposedly more strong in the presence of outliers. The dataset we are using is free of them, but this function should still be able to give accurate results. Since we can apply regularization also in this case, we are going to explore the two cases already explored for the squared function: \\( L_2 \\) regularization and \\( L_1 \\) regularization. ","date":"5057-75-07","objectID":"/posts/05_gsoc_week/:2:0","tags":null,"title":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","uri":"/posts/05_gsoc_week/"},{"categories":null,"content":"\\( L_2 \\) Normalization Again leveraging the MLJLinearModels package we can construct our ESN approx_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 0.001 alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(RobustHuber(0.5, beta, 0.0, Newton()), esn) output = ESNpredict(esn, predict_len, W_out) 9.397286 seconds (14.37 M allocations: 1.748 GiB, 2.88% gc time) The training time is less than the Lasso regularization, but more than the OLS and Ridge training. Plotting the data we obtain: plot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") Let’s go explore the rmse for the short term: for i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 5.36580225918975 6.552573774882654 4.995989255679152 The results seem similar to the Ridge ESN. For the long term the return map shows the following: output_map = ESNpredict(esn, return_map_size, W_out) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) Even though they are not as clear cut as the OLS ESN and Ridge ESN the long term behaviour is still acceptable. ","date":"5057-75-07","objectID":"/posts/05_gsoc_week/:3:0","tags":null,"title":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","uri":"/posts/05_gsoc_week/"},{"categories":null,"content":"\\( L_1 \\) Normalization Since the Lasso ESN showed the worst results of all the model seen until now this could indicate that the \\( L_1 \\) norm is not suited for this task. To have confirmation of this intuition we can train the Huber ESN with the \\( L_1 \\) norm to see if it yields better results than the Lasso ESN. approx_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 1*10^(-7) alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(RobustHuber(0.5, 0.0, beta, ProxGrad(max_iter=10000)), esn) output = ESNpredict(esn, predict_len, W_out) 30.699361 seconds (12.60 M allocations: 3.398 GiB, 1.40% gc time) As expected the training time is in line with the Lasso ESN. Plotting the results we can see that sadly they are worst than the Lasso counterpart plot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") Calculating the rmse for the first 400 steps returns for i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 8.247263444106135 9.896322405158461 12.569601968865513 And the return map clearly shows a periodic long term behaviour: output_map = ESNpredict(esn, return_map_size, W_out) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) The periodicity is more pronounced than the Lasso ESN, making this model the worst performing so far on this task. ","date":"5057-75-07","objectID":"/posts/05_gsoc_week/:4:0","tags":null,"title":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","uri":"/posts/05_gsoc_week/"},{"categories":null,"content":"Different reservoir construction For all the model proposed we used the standard construction of the reservoir, based on the rescaling of the spectral radius to be less than a given value. In literature there are other alternatives explored and in the ReservoirComputing.jl package is present the implementation of an algorithm for the construction of the reservoir matrix based on the Single Value Decomposition (SVD), proposed in [4]. We are going to give the results using this construction only for the OLS ESN, but a more wide study could be done comparing the performances of all the proposed models using the two different implementations of the reservoir. approx_res_size = 300 max_value = 1.2 sparsity = 0.1 activation = tanh sigma = 0.1 beta = 0.0 alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) W_in = init_dense_input_layer(approx_res_size, size(train, 1), sigma) W_new = pseudoSVD(approx_res_size, max_value, sparsity, reverse_sort = true) esn = ESN(W_new, train, W_in, activation = activation, alpha = alpha, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(esn, beta) output = ESNpredict(esn, predict_len, W_out) 0.994207 seconds (3.18 M allocations: 188.436 MiB, 3.64% gc time) The training time is as fast as the normal OLS ESN, as it is to be expected. Plotting the result we obtain plot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") From the plot it seems that the model is producing an acceptable Lorenz system surrogate dataset. The short term will not be the best but in the long term maybe the model could behave as the OLS ESN. for i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 8.299351042330624 9.954728072605832 9.507147274321735 Not as good as the normal reservoir counterpart, but let’s look at the return map for the long term behaviour: output_map = ESNpredict(esn, return_map_size, W_out) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) Event though it seemed that the predicted coordinates had a similar behaviour as the actual Lorenz system the return map clearly shows that in the long term the behaviour is not consistent. This approach to reservoir construction is of course pretty novel and has to pass several optimization steps for its parameters in order to be production ready, but this first test is somewhat disappointing. Echo State Gaussian Processes Firstly proposed in [3] the Echo State Gaussian Processes (ESGP) can be considered an extension of the ESN. In the original paper only the Radial Basis function is explored for the prediction, but in this post we wanted to give a couple of examples of others kernels, so we will use also the Matern kernel and the Polynomial kernel for the prediction of the Lorenz system. The construction of this model is based on GaussianProcesses, so in the first run we will need to import that package as well. ","date":"5057-75-07","objectID":"/posts/05_gsoc_week/:5:0","tags":null,"title":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","uri":"/posts/05_gsoc_week/"},{"categories":null,"content":"Radial Basis kernel Starting from the kernel used in the original paper, we also set the non linear algorithm to the default one, equal to none. The behaviour with different algorithms for this family of models has not been investigated and could be subject of future works, but for now we will just limit the work to the standard one. Keeping the other parameters equal the ESGP can be built in the following way using GaussianProcesses #model parameters degree = 6 approx_res_size = 300 radius = 1.2 activation = tanh sigma = 0.1 alpha = 1.0 nla_type = NLADefault() extended_states = false #create echo state network Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) mean = MeanZero() kernel = SE(1.0, 1.0) @time gp = ESGPtrain(esn, mean, kernel, lognoise = -2.0, optimize = false); output, sigmas = ESGPpredict(esn, predict_len, gp) 43.323255 seconds (6.81 M allocations: 2.053 GiB, 2.53% gc time) The slowest time so far, but maybe the results will be worth the extra seconds it took to train plot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") The results do not look bad, in the short term sadly emerges a discrepancy early on that will lower the rmse values for i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 10.5883475480527 11.759694995689005 6.745353916847517 As expected the rmses are not the greatest, but the model can still recover with a nice display of long term behaviour output_map, sigma_map = ESGPpredict(esn, return_map_size, gp) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) and indeed it does, outside of a single point in a strange location the model seems to capture the Lorenz climate quite well, not quite as good as the OLS ESN or even the Ridge ESN. ","date":"5057-75-07","objectID":"/posts/05_gsoc_week/:6:0","tags":null,"title":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","uri":"/posts/05_gsoc_week/"},{"categories":null,"content":"Matern kernel Another common kernel is the Matern kernel, and training the ESGP using this kernel is a straightforward process, identical to the one we just followed for the Radial Basis kernel: #model parameters degree = 6 approx_res_size = 300 radius = 1.2 activation = tanh sigma = 0.1 alpha = 1.0 nla_type = NLADefault() extended_states = false #create echo state network Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) mean = MeanZero() kernel = Matern(1/2, 1.0, 1.0) @time gp = ESGPtrain(esn, mean, kernel, lognoise = -2.0, optimize = false); output, sigmas = ESGPpredict(esn, predict_len, gp) 40.309179 seconds (1.08 M allocations: 1.761 GiB, 7.58% gc time) Plotting the results plot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") The results seem similar to the ones obtained using the Radial Basis kernel. To be sure of this we need to calculate the rmse for i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 7.150484170664864 8.520570924338477 7.037257939507858 The lower rmses shows a better short term prediction. Plotting the return map to analize the long term results output_map, sigma_map = ESGPpredict(esn, return_map_size, gp) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) The return map is not a clear cut as we saw in other models, but for the majority of the times it seems that the model is still capable of retaining the climate of the Lorenz system. ","date":"5057-75-07","objectID":"/posts/05_gsoc_week/:7:0","tags":null,"title":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","uri":"/posts/05_gsoc_week/"},{"categories":null,"content":"Polynomial kernel The last kernel for the ESGP is the Polynomial kernel. We are now familiar with the construction #model parameters degree = 6 approx_res_size = 300 radius = 1.2 activation = tanh sigma = 0.1 alpha = 1.0 nla_type = NLADefault() extended_states = false #create echo state network Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) mean = MeanZero() kernel = Poly(1.0, 1.0, 2) @time gp = ESGPtrain(esn, mean, kernel, lognoise = -2.0, optimize = false); output, sigmas = ESGPpredict(esn, predict_len, gp) 16.979100 seconds (4.27 M allocations: 3.025 GiB, 10.13% gc time) Plotting the results plot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") we can see a nice prediction on the short term. Using once again the rmse for i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 0.9128150765679983 1.3969462542792563 1.6437787377104938 The short term rmse are the best out of the ESGP kernel used until now. Taking a look also to the long term we can see the following output_map, sigma_map = ESGPpredict(esn, return_map_size, gp) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) The results are in line with the OLS ESN. Considering that the results of the OLS ESN were obtained following a published paper and underwent major optimization while the parameters we have chosen are the fruit of a quick manul search we can say that this model could outperform the ESN for chaotic time series prediction. Conclusions This post is just scratching the surface on the studies needed for this family of models. As we mentioned numerous times throughout the post, the parameter optimization that was done for this wark was just acceptable, being a manual search starting from values optimized for a specific model (OLS ESN). This fact was most evident when testing the SVD based reservoir construction: even though it is a very similar resulting matrix the results were suboptimal. This could have been just an error in the selection of the parameters, and with a more suiting set this construction could perform as well as the standard one. But even with more carefully chosen parameters it is necessary to do multiple runs and obtain a large pool of statistics in order to label a model variation more efficient than the other: in this post we have seen that the Polynomial ESGP has returned amazing results, but changing the seed and using a different reservoir the best model could very well be the radial basis ESGP, or the Huber ESN. This work was based on the single run of all the models because is more intended to be a showcase of the new implementations done for the GSoC project, and the reproducibility was the most important aspect of it. Sadly in this post we weren’t able to showcase the Support Vector Echo State Machines (SVESMs) but the results they obtained wasn’t in line with any of the proposed models, and it performed way worst than even the \\( L_1 \\) trained ESNs. The paper proposing the models leveraged it to solve a different family of problems, so it could be that this task is not suited for this particular variation of the ESN. Regarding the ESGP the possible directions for future studies are really numerous: there are a vast number of other avaiable kernels that we didn’t explore, and even in the one we used the possibility to optimize the parameter is built in in the model, and in our case wasn’t used just for time limitations. Not only it is possible to obtain better results that the one we showed purely by parameter optimization but also by using different kernels at the same time: in the Gaussian Processes is usual to see different kernels combined togheter, throu","date":"5057-75-07","objectID":"/posts/05_gsoc_week/:8:0","tags":null,"title":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","uri":"/posts/05_gsoc_week/"},{"categories":null,"content":"Documentation [1] Pathak, Jaideep, et al. “Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data.” Chaos: An Interdisciplinary Journal of Nonlinear Science 27.12 (2017): 121102. [2] Lukoševičius, Mantas. “A practical guide to applying echo state networks.” Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 659-686. [3] Chatzis, Sotirios P., and Yiannis Demiris. “Echo state Gaussian process.” IEEE Transactions on Neural Networks 22.9 (2011): 1435-1445. [4] Yang, Cuili, et al. “Design of polynomial echo state networks for time series prediction.” Neurocomputing 290 (2018): 148-160. ","date":"5057-75-07","objectID":"/posts/05_gsoc_week/:9:0","tags":null,"title":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","uri":"/posts/05_gsoc_week/"},{"categories":null,"content":"The standard construction of the reservoir matrix \\( \\textbf{W} \\) for Echo State Networks (ESN) is based on initializing \\( \\textbf{W} \\) using specific schemes, usually generating random numbers and then rescaling it to make sure that the spectral radius is less or equal to a chosen number. This procedure is effective, but in literature other ways are explored. In this week we implemented a Singular Value Decomposition (SVD)-based algorithm, described in [1], that is capable of obtaining a sparse matrix suitable for the ESNs. In this post we are going to describe the theory behind the implementation and then a couple of examples will be given. Singular Value Decomposition reservoir construction One of the key aspects the an ESN is for its reservoir to posses the Echo State Property (ESP) [2]. A sufficient condition to obtain it is to construct the reservoir with a spectral radius less than 1. While using the architecture explained in the opening ensures this condition, it doesn’t take into account the singular values information of \\( \\textbf{W} \\), and it doesn’t allow much control over the construction of said matrix. An alternative could be to leverage the SVD to build a reservoir matrix given the largest singular value. To fully comprehend this procedure firstly we have to illustrate what SVD consists of. Let us consider the reservoir matrix \\( \\textbf{W} \\in \\mathbb{R}^{N \\times N}\\); this matrix can be expressed as \\( \\textbf{W} = \\textbf{U}\\textbf{S}\\textbf{V} \\) where \\( \\textbf{U}, \\textbf{V} \\in \\mathbb{R}^{N \\times N}\\) are orthogonal matrices and \\( \\textbf{S}=\\text{diag}(\\sigma _1, …, \\sigma _N) \\) is a diagonal matrix whose entries are ordered in increasing order. The values \\( \\sigma _i \\) are called the singular values of \\( \\textbf{W} \\). Given any diagonal matrix \\( \\textbf{S} \\), and orthogonal matrices \\( \\textbf{U}, \\textbf{V} \\) the matrix \\( \\textbf{W} \\) obtained as \\( \\textbf{W} = \\textbf{U}\\textbf{S}\\textbf{V} \\) has the same singular values as \\( \\textbf{S} \\). This method provides an effective way of ensuring the ESP without the scaling of the reservoir weights. Instead of using orthogonal matrices \\( \\textbf{U}, \\textbf{V} \\), that could produce a dense matrix \\( \\textbf{W} \\), the authors opted for a two dimensional rotation matrix \\( \\textbf{Q}(i, j, \\theta) \\in \\mathbb{R}^{N \\times N}\\) with \\( \\textbf{Q}_{i,i} = \\textbf{Q}_{j,j} = \\text{cos}(\\theta)\\), \\( \\textbf{Q}_{i,j} = -\\text{sin}(\\theta)) \\), \\( \\textbf{Q}_{j,i} = \\text{sin}(\\theta)) \\) with \\( i, j \\) random values in [1, N] and \\( \\theta \\) random value in [-1, 1]. The algorithm proposed is as follows: Choose a predefined \\( \\sigma _N \\) in the range [0, 1] and generate \\( \\sigma _i, i=1,…, N-1 \\) in the range (0, \\( \\sigma _N \\)]. This values are used to create a diagonal matrix \\( \\textbf{S}=\\text{diag}(\\sigma _1, …, \\sigma _N) \\). With \\( h=1 \\) let \\( \\textbf{W}_1 = \\textbf{S} \\). For \\( h = h + 1 \\) randomly choose the two dimensional matrix \\( \\textbf{Q}(i, j, \\theta) \\) as defined above. \\( \\textbf{W} _h = \\textbf{W} _{h-1} \\textbf{Q}(i, j, \\theta)\\) gives the matrix \\( \\textbf{W} \\) for the step \\( h \\). This procedure is repeated until the chosen density is reached. Implementation in ReservoirComputing.jl The implementation into code is extremely straightforwad: following the instructions in the paper a function pseudoSVD is created which takes as input the following dim: the desired dimension of the reservoir max_value: the value of the largest of the singular values sparsity: the sparsity for the reservoir sorted: optional value. If = true (default) the singular values in the diagonal matrix will be sorted. reverse_sort: optional value if sort = true. If = true (default = false) the singular values in the diagonal matrix will be sorted in a decreasing order. Examples ","date":"28286-628-06","objectID":"/posts/04_gsoc_week/:0:0","tags":null,"title":"GSoC week 4: SVD-based Reservoir","uri":"/posts/04_gsoc_week/"},{"categories":null,"content":"Original ESN Testing the SVD construction on the original ESN we can try to reproduce the Lorenz attractor, with similar parameters as given in the Introduction to Reservoir Computing approx_res_size = 300 radius = 1.2 sparsity = 0.1 max_value = 1.2 activation = tanh sigma = 0.1 beta = 0.0 alpha = 1.0 nla_type = NLADefault() extended_states = false The values of the largest singular value for the construction of the SVD based reservoir is equal to the spectral radius of the standard reservoir, that in this case is greater than one. A plot of the results shows: This construction is capable of reproducing the Lorenz system in the short term, and behaves better in the long term than the standard implementation, or at least in this example it does. A more in depth analysis is needed for the consistency of the results and the behavior of the SVD reservoir when the largest singular value is set greater than one and when one of the non linear algorithms is applied. ","date":"28286-628-06","objectID":"/posts/04_gsoc_week/:1:0","tags":null,"title":"GSoC week 4: SVD-based Reservoir","uri":"/posts/04_gsoc_week/"},{"categories":null,"content":"Ridge ESN, SVESM and ESGP In order to test this implementation for others ESN architectures currently implemented in ReservoirComputing.jl we choose to use the same examples as last week, based on the Mackey-Glass system: $$\\frac{dx}{dt} = \\beta x(t)+\\frac{\\alpha x(t-\\delta)}{1+x(t-\\delta)^2}$$ with the same values: \\(\\beta = -0.1 \\) \\(\\alpha = 0.2 \\) \\(\\delta = 17 \\) \\( dt = 0.1 \\) Furthermore the time series is rescaled in the range \\( [-1, 1] \\) by application of a tangent hyperbolic transform \\( y_{ESN}(\\text{t}) = \\text{tanh}(\\text{y}(t)-1) \\). To evaluate the precision of our results we are going to use root mean square deviation (rmse), defined as: $$\\textbf{rmse} = \\sqrt{\\frac{\\sum_{i=1}^{T_n}(y_d(i)-y(i))^2}{T_n}}$$ where \\(y_d(i) \\) is the target value \\(y(i) \\) is the predicted value \\(T_d \\) is the number of test examples The ESN parameters are as follows const shift = 100 const train_len = 6000 const test_len =1500 const approx_res_size = 400 const sparsity = 0.1 const activation = tanh const radius = 0.99 const max_value = 0.99 const sigma = 0.1 const alpha = 0.2 const nla_type = NLADefault() const extended_states = true The largest singular value was set equal to the spectral radius for the standard construction. Averaging on ten runs the results are as follows: rmse ESGP: Classic reservoir: 0.077 SVD reservoir: 0.205 rmse ridge ESN: Classic reservoir: 0.143 SVD reservoir: 0.146 rmse SVESM: Classic reservoir: 0.232 SVD reservoir: 0.245 For the ESGP this procedure yields far worst performances than the standard counterpart. For the ridge ESN and SVESM the results are almost identical. The results obtained are interesting and for sure more testing is needed. Some sperimentation on the h steps ahead prediction could be done, as well as giving different values for the spectral radius and largest singular value, since in all the examples examined the spectral radius was chosen following the literature, and hence could be more optimized that the largest values that we used. ","date":"28286-628-06","objectID":"/posts/04_gsoc_week/:2:0","tags":null,"title":"GSoC week 4: SVD-based Reservoir","uri":"/posts/04_gsoc_week/"},{"categories":null,"content":"Documentation [1] Yang, Cuili, et al. “Design of polynomial echo state networks for time series prediction.” Neurocomputing 290 (2018): 148-160. [2] Jaeger, Herbert. “The “echo state” approach to analysing and training recurrent neural networks-with an erratum note.” Bonn, Germany: German National Research Center for Information Technology GMD Technical Report 148.34 (2001): 13. ","date":"28286-628-06","objectID":"/posts/04_gsoc_week/:3:0","tags":null,"title":"GSoC week 4: SVD-based Reservoir","uri":"/posts/04_gsoc_week/"},{"categories":null,"content":"Continuing to leverage similarities between Reservoir Computing models and Kernel machines, this week’s implementation merges the Bayesian approach of Gaussian Process Regression (GPR) to the Echo State Networks (ESN). After the usual overview of the theory, a computational example will be shown. Gaussian Process Regression The following section is meant as a quick reminder of the deep theory behind Gaussian Processes and its purpose is to illustrate how this approach is a good fit for ESNs. An in depth chapter on GP regression can be found in [1]. A good introduction is also given in the paper that illustrates the implementation of Echo State Gaussian Processes (ESGP) [2]. In this introduction we will heavily follow the second reference, to keep the notation consistent. A Gaussian Process (GP) is defined as a collection of random variables, any finite of which have a joint Gaussian distribution. A (GP) is completely specified by its mean function and covariance function: $$m(\\textbf{x}) = \\mathbb{E}[f(\\textbf{x})]$$ $$k(\\textbf{x}, \\textbf{x}') = \\mathbb{E}[(f(\\textbf{x})-m(\\textbf{x}))(f(\\textbf{x}')-m(\\textbf{x}'))] $$ and the GP can be written as $$f(\\textbf{x}) \\sim GP(m(\\textbf{x}), k(\\textbf{x}, \\textbf{x}'))$$ An usual choice for the mean function is the zero mean function \\( m(\\textbf{x} = 0) \\), and for the covariance function there is a large variety of kernel functions to choose from. In fact there are so many that one can be overwhelmed by the choice; if this is the case, a good introduction and overview can be found in [3]. Given our data, consisting in the samples \\( ( (\\textbf{x}_i, y_i) | i = 1,…,N ) \\), where \\( \\textbf{x}_i \\) is the d-dimensional observation and \\( y_i \\) is the correleted target values, we want to be able to predict \\( y_* \\) given \\( \\textbf{x}_* \\). The response variables \\( y_i \\) are assumed to be dependent on the predictors \\( \\textbf{x}_i \\): $$y_i \\sim \\mathcal{N} (f(\\textbf{x}_i), \\sigma ^2), i=1,…,N$$ Once defined mean and kernel functions one can obtain the predicted mean and variance: $$\\mu _* = \\textbf{k}(\\textbf{x}_*)^T(\\textbf{K}(X, X)+\\sigma ^2 \\textbf{I}_N)^{-1}\\textbf{y}$$ $$\\sigma ^2 _* = k(\\textbf{x}_, \\textbf{x} _) - \\textbf{k}(\\textbf{x}_)^T(\\textbf{K}(X, X)+\\sigma ^2 \\textbf{I}_N)^{-1} \\textbf{k}(\\textbf{x}_)$$ where \\( \\textbf{K}(X, X) \\) is the matrix of the covariances (design matrix). The optimization of the hyperparameters is usually done by maximization of the model marginal likelihood. Echo State Gaussian Processes Using the definition given in the paper [2] an ESGP is a GP the covariance of which is taken as a kernel function over the states of a ESN, postulated to capture the dynamics within a set of sequentially interdependent observations. In this case the feature mapping is explicit, no kernel trick is adopted. One of the improvements of this approach against strandard linear regression is the possibility to obtain a measure of uncertainty for the obtained predictions. Furthermore, one can consider this a generalization of simple regression: in fact using a simple linear kernel and setting \\( \\sigma ^2 = 0 \\) the results should be the same of those obtained by plain linear regression. The paper shows results obtained using the Gaussian RBF kernel, but the high number of kernel available and the possibility to use combination of them makes this approach really versatile and, as of now, somewhat understudied. ","date":"21216-621-06","objectID":"/posts/03_gsoc_week/:0:0","tags":null,"title":"GSoC week 3: Echo State Gaussian Processes","uri":"/posts/03_gsoc_week/"},{"categories":null,"content":"Implementation in ReservoirComputing.jl Building on the package GaussianProcesses it was possible to create a ESGPtrain function as well as a ESGPpredict and a ESGPpredict_h_steps function, with a similar behaviour to the ESN counterpart. The ESGPtrain function takes as input: esn: the previously defined ESN mean: a GaussianProcesses.Mean struct, to choose between the ones provided by the GaussianProcesses package kernel: a GaussianProcesses.Kernel struct, to choose between the ones provided by the GaussianProcesses package lognoise: optional value with default = -2 optimize: optional value with default = false. If = true the hyperparameters are optimized using Optim.jl. Since gradients are available for all mean and kernel functions, gradient based optimization techniques are recommended. optimizer: optional value with default = Optim.LBFGS() y_target: optional value with default = esn.train_data. This way the system learns to predict the next step in the time series, but the user is free to set other possibilities. The function returns a trained GP, that can be used in ESGPpredict or ESGPpredict_h_steps. They both take as input esn: the previously defined ESN predict_len: number of steps of the prediction gp: a trained GaussianProcesses.GPE struct in addition ESGPpredict_h_steps requires h_steps: the h steps ahead in wich the model will run autonomously test_data: the testing data, to be given as input to the model every h-th step. ","date":"21216-621-06","objectID":"/posts/03_gsoc_week/:1:0","tags":null,"title":"GSoC week 3: Echo State Gaussian Processes","uri":"/posts/03_gsoc_week/"},{"categories":null,"content":"Example Similarly to last week the example is based on the Mackey Glass system. It can be described by $$\\frac{dx}{dt} = \\beta x(t)+\\frac{\\alpha x(t-\\delta)}{1+x(t-\\delta)^2}$$ and the values adopted in [2] are \\(\\beta = -0.1 \\) \\(\\alpha = 0.2 \\) \\(\\delta = 17 \\) \\( dt = 0.1 \\) Furthermore the time series is rescaled in the range \\( [-1, 1] \\) by application of a tangent hyperbolic transform \\( y_{ESN}(\\text{t}) = \\text{tanh}(\\text{y}(t)-1) \\). To evaluate the precision of our results we are going to use root mean square deviation (rmse), defined as: $$\\textbf{rmse} = \\sqrt{\\frac{\\sum_{i=1}^{T_n}(y_d(i)-y(i))^2}{T_n}}$$ where \\(y_d(i) \\) is the target value \\(y(i) \\) is the predicted value \\(T_d \\) is the number of test examples The ESN parameters are as follows const shift = 100 const train_len = 6000 const test_len =1500 const approx_res_size = 400 const sparsity = 0.1 const activation = tanh const radius = 0.99 const sigma = 0.1 const alpha = 0.2 const nla_type = NLADefault() const extended_states = true Something worth pointing out is that for the first time we have found a value of alpha other than 1: this means we are dealing with a leaky ESN. Following the paper we will try to give a fair comparison between ESN trained with Ridge regression , ESGP and SVESM. Since the parameters for ESN with Ridge and SVESM are missing in the paper, we thought best to use “default” parameters for all the model involved: using optimization only on one model did not seem like a fair comparison. Both in SVESM and ESGP the kernel function used is Gaussian RBF. The results are as follows: ESGP RBF rmse: 0.0298 ESN ridge rmse: 0.1718 SVESM RBF rmse: 0.1922 We can clearly see that the proposed model is outperforming the other models proposed. We mentioned that one of the improvements of this models was the measure of uncertainty relative to the prediction. The ESGPpredict function also returns the variance of the prediction, that can be plotted alongside the results: From the plot is even more clear that the ESGP is more capable of reproducing the behaviour of the Mackey-Glass system. Like in the paper the worst performing model for this specific task is the SVESM. Beside a better analysis of the parameters to use in this case, it would also be interesting to see a comparison between the normal GPR with different kernel functions and the ESGP with the same kernel functions. As always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me! ","date":"21216-621-06","objectID":"/posts/03_gsoc_week/:2:0","tags":null,"title":"GSoC week 3: Echo State Gaussian Processes","uri":"/posts/03_gsoc_week/"},{"categories":null,"content":"Documentation [1] Rasmussen, Carl Edward. “Gaussian processes in machine learning.” Summer School on Machine Learning. Springer, Berlin, Heidelberg, 2003. [2] Chatzis, Sotirios P., and Yiannis Demiris. “Echo state Gaussian process.” IEEE Transactions on Neural Networks 22.9 (2011): 1435-1445. [3] https://www.cs.toronto.edu/~duvenaud/cookbook/ ","date":"21216-621-06","objectID":"/posts/03_gsoc_week/:3:0","tags":null,"title":"GSoC week 3: Echo State Gaussian Processes","uri":"/posts/03_gsoc_week/"},{"categories":null,"content":"The second week of the Google Summer of Code project with ReservoirComputing.jl dealt with the implementation of a Support Vector based regression for the Echo State Network model, resulting in the Support Vector Echo-State Machine (SVESM). In this post we will quickly touch on the theory behind Support Vector Regression (SVR), and then we will see the results of the implementation into the library. At the end a couple of examples are given, togheter with a comparison between SVR and SVESMs. Theoretical Background ","date":"14146-614-06","objectID":"/posts/02_gsoc_week/:0:0","tags":null,"title":"GSoC week 2: Support Vector Regression in Echo State Networks","uri":"/posts/02_gsoc_week/"},{"categories":null,"content":"Support Vector Regression What follows is just an overview of the theory behind SVR, which takes for granted a priori knowledge of the reader of the more general concepts of Support Vector Machines. A more knowledgable and in depth exposition can be found in [1] and [2]. Most of the information presented in this section is taken from these papers unless stated otherwise. In a general setting, a regression task can be expressed as the minimization of the following primal objective function: $$C \\sum_{j=1}^{N}L[\\textbf{x}_j, y_{dj}, f ] + ||\\textbf{w}||^2$$ where \\(L\\) is a general loss function \\(f\\) is the prediction function defined by \\(\\textbf{W}\\) and \\(b\\) \\(C\\) is a regularization constant If the loss function is quadratic the objective function can be minimized by means of linear algebra, and the methodology is called Ridge Regression. The most used function in SVR is called \\(\\epsilon\\)-insensitive loss function [3]: $$L = 0 \\ \\text{ if } |f(x)-y| \u003c \\epsilon$$ $$L = |f(x)-y| - \\epsilon \\text{ otherwise }$$ More specifically we can consider the equation $$\\text{min}||\\textbf{w}||^2+C\\sum_{j=1^{N_t}}(\\xi _j+\\hat{\\xi}_j)$$ with constraints (for \\(j=1,…,N_t\\)): $$(\\textbf{w}^T \\textbf{x}_j+b)-y_j \\le \\epsilon - \\xi_j$$ $$y_j-(\\textbf{w}^T \\textbf{x}_j+b) \\le \\epsilon - \\xi_j$$ $$\\xi _j,\\hat{\\xi}_j \\ge 0$$ the solution of which can be found using the following dual problem $$\\text{min}_{\\alpha , \\alpha^*} \\frac{1}{2} \\sum_{i, j = 1}^{N_t}(\\alpha_i-\\alpha_i^*)(\\alpha_j-\\alpha_j^*)\\textbf{x}^T\\textbf{x}-\\sum_{i=1}^{N_t}(\\alpha_i-\\alpha_i^*) y_i + \\sum_{i=1}^{N_t}(\\alpha_i-\\alpha_i^*) \\epsilon$$ with constraints: $$0 \\le \\alpha _i, \\alpha _i^* \\le C, \\sum_{i=1}^{N_t}(\\alpha_i-\\alpha_i^*)=0$$ We can clearly see that to solve this task one has to resort to quadratic programming. The solution will have the form $$f(\\textbf{x}) = \\sum_{i=1}^{N_p}(\\alpha_i-\\alpha_i^*) k(\\textbf{x}_i, \\textbf{x})$$ where \\(k\\) is a so-called kernel function, an implicit mapping of the data into an higher dimension, used to turn a non linear problem into a linear one. This is the “kernel trick”, where the mapping is not explicitly done, but is obtained using function that only require the computation of the inner products. Common kernels includes: Linear \\( k(\\textbf{x}_i, \\textbf{x}_j) = \\textbf{x}_i \\cdot \\textbf{x}_j\\) Polynomial \\(k(\\textbf{x}_i, \\textbf{x}_j) = (\\textbf{x}_i \\cdot \\textbf{x}_j)^d \\) Gaussian Radial Basis Function \\( k(\\textbf{x}_i, \\textbf{x}_j) = e^{ \\lambda ||\\textbf{x}_i - \\textbf{x}_j||^2} \\) ","date":"14146-614-06","objectID":"/posts/02_gsoc_week/:1:0","tags":null,"title":"GSoC week 2: Support Vector Regression in Echo State Networks","uri":"/posts/02_gsoc_week/"},{"categories":null,"content":"Support Vector Echo State Machines We can see that the intuition behind the Reservoir Computing approach is similar to the kernel methods: using a fixed Recurrent Neural Network (for the case of the ESN) we are also mapping the input into a higher dimension. Therefore the connection between the two models was almost immediate and the idea developed in [4] was to perform a linear SVR in the higer dimensional reservoir state space. In the paper different loss function are analized: quadratic loss function, \\( \\epsilon \\)-insensitive loss function and the Huber loss function. A new method of prediction is also proposed, called the direct method. This method is a variation of the one step ahead prediction, in which the desired output is not the next step, but a \\( h \\) steps ahead one. The input-output training sequences can be described as \\( \\textbf{d}(k), \\textbf{x}(k+h), k=1, 2, 3,…, N_t \\) where \\( \\textbf{d}(k) \\) is the embedding of the target time series and \\( \\textbf{x}(k+h) \\) is the \\( h \\) steps ahead target output for training/testing. Implementation in ReservoirComputing.jl The implementation of SVESM into the library is done leveraging the LIBSVM.jl package, a wrapper for the package of the same name written in C++. With this we were able to implement the \\( \\epsilon \\)-insensitive loss function based SVR, creating a new SVESMtrain function as well as a SVESM_direct_predict function. The SVESMtrain takes as input svr: a AbstractSVR object that can be both EpsilonSVR or NuSVR. This implementation also allows the user to choose a kernel other than the linear one like the one used in the paper. Actually in comparisons done in other papers, different kernels have been used with SVESMs. esn: the previously constructed ESN y_target: the one-dimensional target output \\( \\textbf{x}(k+h) \\) The SVESM_direct_predict function takes as input esn: the previously constructed ESN test_in: the testing portion of the input data \\( \\textbf{d}(k) \\) m: the output from SVESMtrain The quadratic loss function and Huber loss function are already implemented and the ESN can be trained using one of them through ESNtrain(Ridge(), esn) or ESNtrain(RobustHuber(), esn). Examples Following the example used in the paper we will try to predict the 84 steps ahead Mackey Glass system. It can be described by $$\\frac{dx}{dt} = \\beta x(t)+\\frac{\\alpha x(t-\\delta)}{1+x(t-\\delta)^2}$$ and the values adopted in [4] are \\(\\beta = -0.1 \\) \\(\\alpha = 0.2 \\) \\(\\delta = 17 \\) The timeseries is then embedded $$\\textbf{d}(k) = [x(k), x(k-\\tau), x(k-2 \\tau), x(k-3 \\tau)]$$ with dimension 4 and \\(\\tau = 6\\). The target output is \\( y_d(k) = x(k+84) \\). We are going to evaluate the precision of our prediction using nrmse, defined as $$\\textbf{nmrse} = \\sqrt{\\frac{\\sum_{i=1}^{T_n}(y_d(i)-y(i))^2}{T_n \\cdot \\sigma ^2}}$$ where \\(y_d(i) \\) is the target value \\(y(i) \\) is the predicted value \\(T_d \\) is the number of test examples \\(\\sigma ^2 \\) is the variance of the original time series The data is obtained using Runge Kutta of order 4 with a stepsize of 0.1. We will perform three tests, and in all three we are also going to give the results of SVR using different kernel to make a comparison with the results obtained by SVESM. The first test is conducted on noiseless test and training samples. In the second one we will add noise in the training portion of \\( \\textbf{d}(k) \\), and in the last noise will be added both in training and testing. The target values remain noiseless in all three tests. The noise level is determined by the ratio of the standard deviation of the noise and the signal standard deviation, and it is chosen to be 20%. Before the exposition of the results we need to address the fact that sadly in the original paper only a few parameters are given: the size of the reservoir, sparseness and spectral radius of the reservoir matrix and the scaling of the input weights. Missing parameters like \\( C \\) or \\( \\epsilon \\) for the SVESM training of course ","date":"14146-614-06","objectID":"/posts/02_gsoc_week/:2:0","tags":null,"title":"GSoC week 2: Support Vector Regression in Echo State Networks","uri":"/posts/02_gsoc_week/"},{"categories":null,"content":"Noiseless training and testing Using a dataset of lenght 500 we can see that the results for the noiseless training and testing are SVESM nmrse for the noiseless test: 0.343 SVM Poly kernel nmrse for the noiseless test: 0.489 SVM RadialBasis kernel nmrse for the noiseless test: 0.393 We can also plot the results to better appreciate the results (the lenght of the prediction chosen for the plots is higher in order to better visualize the differences in trajectories): From the results it is clear that the SVESM performs better. ","date":"14146-614-06","objectID":"/posts/02_gsoc_week/:3:0","tags":null,"title":"GSoC week 2: Support Vector Regression in Echo State Networks","uri":"/posts/02_gsoc_week/"},{"categories":null,"content":"Noisy training and noiseless testing Adding normally distributed white noise to the training input dataset we obtain the following results for the nmrse: Training on noisy data and testing on noiseless data... SVESM nmrse for noisy training and noiseless testing: 0.415 SVM Poly kernel nmrse for noisy training and noiseless testing: 0.643 SVM RadialBasis kernel nmrse for noisy training and noiseless testing: 0.557 The plots are Also in this case the performance of the SVESM is superior to SVR. ","date":"14146-614-06","objectID":"/posts/02_gsoc_week/:4:0","tags":null,"title":"GSoC week 2: Support Vector Regression in Echo State Networks","uri":"/posts/02_gsoc_week/"},{"categories":null,"content":"Noisy training and testing For the last comparison the results are as follows Training and testing on noisy data... SVESM nmrse for noisy training and testing: 0.439 SVM Poly kernel nmrse for noisy training and testing: 0.724 SVM RadialBasis kernel nmrse for noisy training and testing: 0.648 And in this case as well the proposed model outperforms SVR. As always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me! ","date":"14146-614-06","objectID":"/posts/02_gsoc_week/:5:0","tags":null,"title":"GSoC week 2: Support Vector Regression in Echo State Networks","uri":"/posts/02_gsoc_week/"},{"categories":null,"content":"References [1] Drucker, Harris, et al. “Support vector regression machines.” Advances in neural information processing systems. 1997. [2] Smola, Alex J., and Bernhard Schölkopf. “A tutorial on support vector regression.” Statistics and computing 14.3 (2004): 199-222. [3] VladimirN. Vapnik, The Nature of Statistical Learning Theory, Springer, 1995. [4] Shi, Zhiwei, and Min Han. “Support vector echo-state machine for chaotic time-series prediction.” IEEE transactions on neural networks 18.2 (2007): 359-372. ","date":"14146-614-06","objectID":"/posts/02_gsoc_week/:6:0","tags":null,"title":"GSoC week 2: Support Vector Regression in Echo State Networks","uri":"/posts/02_gsoc_week/"},{"categories":null,"content":"The first week tackled the implementation of different kind of linear regression for the creation of the last layer in the Echo State Network. More specifically were added the possibility to add a \\( l_1 \\) regularization to the loss function (Lasso regression), both \\( l_1 \\) and \\( l_2 \\) regularizations (Elastic Net regression) and also added the possibility to choose the Huber loss function. As in the last post we will start from a brief theoretical background to explain the code and then we will showcase some examples taken from the literature. Theoretical Background In the Brief Introduction to Reservoir Computing we showed how it was possible to get the output layer from a linear regression over the states and the desired output using Ridge regression: $$\\textbf{W}_{\\text{out}} = \\textbf{Y}^{\\text{target}} \\textbf{X}^{\\text{T}}(\\textbf{X} \\textbf{X}^{\\text{T}} + \\beta \\textbf{I})^{-1}$$ but by doing so we actually jumped a few steps, and in the example it wasn’t even used actually, it was just an Ordinary Least Squares(OLS). To know the difference we have to take a little step back. Inherently generalised linear regression models are an optimisation problem of the form $$L(\\textbf{y}, \\textbf{X} \\theta)+P(\\theta)$$ where \\( \\textbf{y} \\) is the target \\( \\textbf{X} \\) is the design matrix \\( \\theta \\) is a vector of coefficient to determine \\( L \\) is a loss function \\( P \\) is a penalty function ","date":"7076-67-06","objectID":"/posts/01_gsoc_week/:0:0","tags":null,"title":"GSoC week 1: lasso, Elastic Net and Huber loss","uri":"/posts/01_gsoc_week/"},{"categories":null,"content":"OLS and penalization In the case of Ridge regression the loss function is the OLS, to wich is added a \\( l_2 \\) regularization. The function to minimize is of the form $$||\\textbf{y} - \\textbf{X} \\theta ||_2^2 + \\lambda || \\theta ||_2^2$$ where \\( ||.||_2 \\) is the \\( l_2 \\) norm and \\( \\lambda \\) is a penalization coefficient. In the Lorenz system example the lambda parameter was set to zero so in fact we were actually fitting using only the first part of the above expression, that corresponds to OLS, as said before. The formula we showed in the opening is actually quite different from this second definition, but this is because even though this is an optimisation problem the Ridge regression has a closed form solution. So if we imagine to have a matrix of targets \\(\\textbf{Y}\\) and \\(\\theta = \\textbf{W}_{\\text{out}} \\) then the first definition can be derived from the second. Another form of regression based on the OLS loss function is Lasso (least absolute shrinkage and selection operator) which uses the \\( l_1 \\) norm as regularization. The function to minimize will hence have the form $$||\\textbf{y} - \\textbf{X} \\theta ||_2^2 + \\lambda || \\theta ||_1$$ This two methods can be linearly combined in order to obtain the Elastic Net regression method, of the form $$||\\textbf{y} - \\textbf{X} \\theta ||_2^2 + \\lambda || \\theta ||_2^2 + \\gamma || \\theta ||_1$$ This last two methods, contrarily to Ridge regression, do not present a closed form and so one has to use other solutions to the optimization problem, such as gradient descent or proximal gradient method. ","date":"7076-67-06","objectID":"/posts/01_gsoc_week/:1:0","tags":null,"title":"GSoC week 1: lasso, Elastic Net and Huber loss","uri":"/posts/01_gsoc_week/"},{"categories":null,"content":"Huber loss function Of course one can choose other alternatives to the OLS loss function, and one of the most common is the Huber loss function. Used in robust regression is known to respond well in the presence of outliers. The function is defined as follows $$L_{\\sigma}(a) = \\frac{1}{2}a^2 \\text{ for } |a| \\le \\sigma$$ $$L_{\\sigma}(a) = \\sigma (|a| - \\frac{1}{2} \\sigma ) \\text{ otherwise}$$ To this function we can apply the same regularization function priorly defined, the \\( l_2 \\) and \\( l_1 \\) norm if one so choses. ","date":"7076-67-06","objectID":"/posts/01_gsoc_week/:2:0","tags":null,"title":"GSoC week 1: lasso, Elastic Net and Huber loss","uri":"/posts/01_gsoc_week/"},{"categories":null,"content":"Implementation in ReservoirComputing.jl The implementation in the library has been done leveraging the incredible job done by the MLJLinearModels.jl team. The ESNtrain() function can now take as argument the following structs: Ridge(lambda, solver) Ridge(lambda, solver) ElastNet(lambda, gamma, solver) RobustHuber(delta, lambda, gamma, solver) where lambda gamma and delta are defined in the theoretical sections above and solver is a solver of the MLJLinearModels library. One must be careful to use the suitable solver for every loss and regularization combination. Further information can be found in the MLJLinearModels documentation. Examples The Lasso regression was first proposed in [1] and in [2] a variation is proposed on it and there are also comparison with Elastic Net regression. Other comparison are carried out in [3], which is the paper we will follow as methodology. The Huber loss function is used for comparison in [4], but to my knowledge has not been adopted in other papers. Trying to follow the data preparation used in [3] we use the Rossler system this time to carry out our tests. The system is defined by the equations $$\\frac{dx}{dt} = -y -z$$ $$\\frac{dy}{dt} = x + ay$$ $$\\frac{dz}{dt} = b + z(x - c)$$ andit exhibits chaotic behavior for \\( a = 0.2 \\), \\( b = 0.2 \\) and \\( c = 5.7 \\). Using Range Kutta of order 4 from the initial positions \\( (-1, 0, 3) \\) the time series is generated with step size set to 0.005. In the paper the attractor is reconstructed in the phase space using embedding dimensions \\( (3, 3, 3) \\) and time delays \\( (13, 13, 13) \\) for the \\(x, y, z\\) series respectively. After, all the 9 resulting timeseries are rescaled in the range \\( [-1, 1] \\) and will all be used in the training of the ESN. Since the data preparation was unusual for me I spent a couple of hours wrapping my head around it. If one wants to know more about time delays and embeddings a good brief introduction is given in [5]. Thankfully DynamicalSystems.jl makes lifes easier when dealing with this type of problems and using embed() I was quickly able to create the data as expressed in the paper. StatsBase.jl dealt with the rescaling part. The parameter for the ESN are then set as follows using ReservoirComputing shift = 100 sparsity = 0.05 approx_res_size = 500 radius = 0.9 activation = tanh sigma = 0.01 train_len = 3900 predict_len = 1000 lambda = 5*10^(-5) gamma = 5*10^(-5) alpha = 1.0 nla_type = NLADefault() extended_states = true h = 1 The test was based on a h steps ahead prediction, which differs from the normal prediction because after every h steps of the ESN running autonomosly after training, the actual data is fed into the model, “correcting” the results. This way one has also to feed test data into to model, and the error is consequently quite low. As we can see the h parameter is set on 1, since that is the step used in the paper. The model only predicts one step in the future this way, for every step of the prediction. To test the difference between values we used a user-defined Root Mean Square Deviation (RMSE) for the x coordinate, following paper guidelines. The results are as follows, given as a mean of 20 different initiations of random reservoirs: $$ \\text{rmse}_{\\text{RESN}} = 9.033 \\cdot 10^{-5} $$ $$ \\text{rmse}_{\\text{LESN}} = 0.006 $$ $$ \\text{rmse}_{\\text{EESN}} = 0.006 $$ $$ \\text{rmse}_{\\text{HESN}} = 9.040 \\cdot 10^{-5} $$ where RESN is the Ridge regression trained ESN, LESN is the Lasso trained ESN, EESN is the Elastic Net trained ESN and HESN is the ESN trained with Huber function with \\( \\delta = 0.8\\) and \\( l_2 \\) norm. We can also take a look at a plot of the x coordinate bot actual and predicted, but as one can expect from a rmse so small there is almost no difference. The results obtained, while not in line with what is showed in the literature, are actually far better, by several orders of magnitude in some instances. While not obtaining the comparison with the papers is somewhat not opti","date":"7076-67-06","objectID":"/posts/01_gsoc_week/:3:0","tags":null,"title":"GSoC week 1: lasso, Elastic Net and Huber loss","uri":"/posts/01_gsoc_week/"},{"categories":null,"content":"References [1] Han, Min, Wei-Jie Ren, and Mei-Ling Xu. “An improved echo state network via L1-norm regularization.” Acta Automatica Sinica 40.11 (2014): 2428-2435. [2] Xu, Meiling, Min Han, and Shunshoku Kanae. “L 1/2 Norm Regularized Echo State Network for Chaotic Time Series Prediction.” International Conference on Neural Information Processing. Springer, Cham, 2016. [3] Xu, Meiling, and Min Han. “Adaptive elastic echo state network for multivariate time series prediction.” IEEE transactions on cybernetics 46.10 (2016): 2173-2183. [4] Guo, Yu, et al. “Robust echo state networks based on correntropy induced loss function.” Neurocomputing 267 (2017): 295-303. [5] http://www.scholarpedia.org/article/Attractor_reconstruction ","date":"7076-67-06","objectID":"/posts/01_gsoc_week/:4:0","tags":null,"title":"GSoC week 1: lasso, Elastic Net and Huber loss","uri":"/posts/01_gsoc_week/"},{"categories":null,"content":"Emails I have a plethora of email accounts apparently, you can write me at the following: francesco(at)juliacomputing(dot)com for anything work related. martinuzzi(at)informatik(dot)uni-leipzig(dot)de for anything academic related. francesco(dot)martinuzzi(at)uni-leipzig(dot)de for anything academic related. You can also catch me on Telegram: (at)martinuzzifrancesco where (dot) = . and (at) = @ ","date":"26265-526-05","objectID":"/contact/:0:0","tags":null,"title":"Contact","uri":"/contact/"},{"categories":null,"content":"This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package ReservoirComputing.jl as example tool. This package is a work in progress and it is currently the main project I am working on as part of the Google Summer of Code program. Future posts are going to further explain the various implementations and improvements to the code by means of comparisons with the literature and examples. What is Reservoir Computing? Reservoir Computing is an umbrella term used to identify a general framework of computation derived from Recurrent Neural Networks (RNN), indipendently developed by Jaeger [1] and Maass et al. [2]. These papers introduced the concepts of Echo State Networks (ESN) and Liquid State Machines (LSM) respectively. Further improvements over these two models constitute what is now called the field of Reservoir Computing. The main idea lies in leveraging a fixed non-linear system, of higher dimension than the input, onto which to input signal is mapped. After this mapping is only necessary to use a simple readout layer to harvest the state of the reservoir and to train it to the desired output. In principle, given a complex enough system, this architecture should be capable of any computation [3]. The intuition was born from the fact that in training RNNs most of the times the weights showing most change were the ones in the last layer [4]. In the next section we will also see that ESNs actually use a fixed random RNN as the reservoir. Given the static nature of this implementation usually ESNs can yield faster results and in some cases even better, in particular when dealing with chaotic time series predictions [5]. But not every complex system is suited to be a good reservoir. A good reservoir is one that is able to separate inputs; different external inputs should drive the system to different regions of the configuration space [3]. This is called the separability condition. Furthermore an important property for the reservoirs of ESNs is the Echo State property which states that inputs to the reservoir echo in the system forever, or util they dissipate. A more formal definition of this property can be found in [6]. In order to better show the inner workings of models of this family I am going to explain in mathematical details the ESN, a model that is already implemented in the package, so it will be useful for making examples. Echo State Networks ","date":"26265-526-05","objectID":"/posts/a-brief-introduction-to-reservoir-computing/:0:0","tags":null,"title":"A brief introduction to Reservoir Computing","uri":"/posts/a-brief-introduction-to-reservoir-computing/"},{"categories":null,"content":"Theoretical Background This intends to be a quick overview of the theory behind the ESN to get the reader acquainted with the concepts and workings of this particular model, and it is by no means comprehensive. For in depth reviews and explanations please refer to [7] and [8]. All of the information laid out in this section is adapted from these two sources, unless stated otherwise. Let us suppose we have an input signal \\( \\textbf{u}(t) \\in R^M \\) where \\( t = 1, …, T \\) is the discrete time and \\( T \\) the number of data points in the training set. In order to project this input onto the reservoir we will need an input to reservoir coupler, identified by the matrix \\( \\textbf{W}_{\\text{in}} \\in R^{N \\times M} \\). Usually this matrix is built in the same way the reservior is, and at the same time. For the implementation used in ReservoirComputing.jl we have followed the same construction proposed in [9] where the i-th of the \\( M \\) input signals is connected to \\( N/M \\) reservoir nodes with connection weights in the i-th column of \\( \\textbf{W}_{\\text{in}} \\). The non-zero elements are chosen randomly from a uniform distribution and then scaled in the range \\( [-\\sigma , \\sigma ] \\). The reservoir is constitued by \\( N \\) neurons connected in a Erdős–Rényi graph configuration and it is represented by an adjacency matrix \\( \\textbf{W} \\) of size \\( N \\times N \\) with values drawn from a uniform random distribution on the interval \\( [-1, 1] \\) [5]. This is the most important aspect of the ESN, so in order to build one in an efficient manner we must first understand all of its components. The size \\( N \\) is of course the single most important one: the more challenging the task, the bigger the size should be. Of course a bigger matrix will mean more computational time so the advice of Lukoševičius is to start small and then scale. The sparsity of the reservoir. In most papers we see that each reservoir node is connected to a small number of other nodes, ranging from 5 to 12. The sparseness, beside theoretical implications, is also useful to speed up computations. Spectral radius. After the generation of a random sparse reservoir matrix, its spectral radius \\( \\rho (\\textbf{W}) \\) is computed and \\( \\textbf{W} \\) is divided by it. This allows us to obtain a matrix with a unit spectral radius, that can be scaled to a more suited value. Altough there are exceptions (when the inputs \\( \\textbf{u}(t) \\) are non-zero for example), a spectral radius smaller than unity \\( \\rho (\\textbf{W}) \u003c 1 \\) ensures the echo state property. More generally this parameter should be selected to maximize the performance, keeping the unitary value as a useful reference point. After the construction of the input layer and the reservoir we can focus on harvesting the states. The update equations of the ESN are: $$\\textbf{x}(t+\\Delta t) = (1-\\alpha) \\textbf{x}(t)+\\alpha f( \\textbf{W} \\textbf{x}(t)+ \\textbf{W}_{\\text{in}} \\textbf{u}(t))$$ $$\\textbf{v}(t+\\Delta t) = g( \\textbf{W}_{\\text{out}} \\textbf{x}(t))$$ where \\( \\textbf{v}(t) \\in R^{M} \\) is the predicted output \\( \\textbf{x}(t) \\in R^{N} \\) is the state vector \\( \\textbf{W}_{\\text{out}} \\in R^{M \\times N} \\) is the output layer \\( f \\) and \\( g \\) are two activation functions, most commonly the hyperbolic tangent and identity respectively \\( \\alpha \\) is the leaking rate The computation of \\( \\textbf{W}_{\\text{out}} \\) can be expressed in terms of solving a system of linear equations $$\\textbf{Y}^{\\text{target}}=\\textbf{W}_{\\text{out}} \\textbf{X}$$ where \\( \\textbf{X} \\) is the states matrix, built using the single states vector \\( \\textbf{x}(t) \\) as column for every \\( t=1, …, T \\), and \\( \\textbf{Y}^{\\text{target}} \\) is built in the same way only using \\( \\textbf{y}^{\\text{target}}(t) \\). The chosen solution for this problem is usually the Tikhonov regularization, also called ridge regression which has the following close form: $$\\textbf{W}_{\\text{out}} = \\textbf{Y}^{\\text{target}} \\textbf{X}^{","date":"26265-526-05","objectID":"/posts/a-brief-introduction-to-reservoir-computing/:1:0","tags":null,"title":"A brief introduction to Reservoir Computing","uri":"/posts/a-brief-introduction-to-reservoir-computing/"},{"categories":null,"content":"Lorenz system prediction This is a task already tackled in literature, so our intent is to try and replicate the results found in [10]. This example can be followed in its entirety here. In this section we will just give part of the code to illustrate the theory explained above, so some important parts are not displayed. Supposing we have already created the train data, constituted by 5000 timesteps of the chaotic Lorenz system, we are going to use the same parameters found in the paper: using ReservoirComputing approx_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 0.0 alpha = 1.0 nla_type = NLAT2() extended_states = false The ESN can easily be called in the following way: esn = ESN(approx_res_size, train, degree, radius, activation, #default = tanh alpha, #default = 1.0 sigma, #default = 0.1 nla_type #default = NLADefault(), extended_states #default = false ) The training and the prediction, for 1250 timestps, are carried out as follows W_out = ESNtrain(esn, beta) output = ESNpredict(esn, predict_len, W_out) In order to visualize the solution we can plot the individual trajectories and the attractors As expected the short term predictions are very good, and in the long term the behaviour of the system is mantained. But what happens if the parameters are not ideal? In the paper is given an example where the spectral radius is bigger than the ideal value and the predictions are compromised as a result. We can also show that if the value is less than one, as suggested in order to mantain the echo state property, the results are nowhere near optimal. radius = 0.8 While incrementing the reservoir size is known to improve the results, up to a certain point, a smaller one will almost surely worsen them. approx_res_size = 60 As we can see, the choice of the parameters is of the upmost importance in this model, as it is in most models in the field of Machine Learning. There are ways of searching the optimal parameters in the state space such as grid search or random search, though experience with the model will give you the ability to know what to thinker with most of the times. If you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me! ","date":"26265-526-05","objectID":"/posts/a-brief-introduction-to-reservoir-computing/:2:0","tags":null,"title":"A brief introduction to Reservoir Computing","uri":"/posts/a-brief-introduction-to-reservoir-computing/"},{"categories":null,"content":"References [1] Jaeger, Herbert. “The “echo state” approach to analysing and training recurrent neural networks-with an erratum note.” Bonn, Germany: German National Research Center for Information Technology GMD Technical Report 148.34 (2001): 13. [2] Maass W, Natschläger T, Markram H. Real-time computing without stable states: a new framework for neural computation based on perturbations. Neural Comput. 2002;14(11):2531‐2560. [3] Konkoli Z.: Reservoir Computing. In: Meyers R. (eds) Encyclopedia of Complexity and Systems Science. Springer, Berlin, Heidelberg (2017) [4] Schiller, Ulf D., and Jochen J. Steil. “Analyzing the weight dynamics of recurrent learning algorithms.” Neurocomputing 63 (2005): 5-23. [5] Chattopadhyay, Ashesh, et al. “Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods: Reservoir computing, ANN, and RNN-LSTM.” arXiv preprint arXiv:1906.08829 (2019). [6] Yildiz, Izzet B., Herbert Jaeger, and Stefan J. Kiebel. “Re-visiting the echo state property.” Neural networks 35 (2012): 1-9. [7] Lukoševičius, Mantas. “A practical guide to applying echo state networks.” Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 659-686. [8] Lukoševičius, Mantas, and Herbert Jaeger. “Reservoir computing approaches to recurrent neural network training.” Computer Science Review 3.3 (2009): 127-149. [9] Lu, Zhixin, et al. “Reservoir observers: Model-free inference of unmeasured variables in chaotic systems.” Chaos: An Interdisciplinary Journal of Nonlinear Science 27.4 (2017): 041102. [10] Pathak, Jaideep, et al. “Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data.” Chaos: An Interdisciplinary Journal of Nonlinear Science 27.12 (2017): 121102. ","date":"26265-526-05","objectID":"/posts/a-brief-introduction-to-reservoir-computing/:3:0","tags":null,"title":"A brief introduction to Reservoir Computing","uri":"/posts/a-brief-introduction-to-reservoir-computing/"},{"categories":null,"content":"Introduction Hello there! I am Francesco Martinuzzi, a PhD student in Physics and Earth Sciences at Leipzig University in Germany. I am under the supervision of Prof. Miguel D. Mahecha and Dr. Karin Mora at the Remote Sensing Centre for Earth System Research RSC4Earth. My research is kindly funded by the Center for Scalable Data Analytics and Artificial Intelligence ScaDS.AI. In my PhD project I explore the consequences of extreme events on the environment using Machine Learning models and Dynamical Systems theory. I am also working part time as a consultant for Julia Computing, where I am contributing in the development of JuliaSim, a Machine-Learning powered simulation platform. My master thesis, titled On complexity and computational power of Elementary Cellular Automata using Reservoir Computing, explored the criticality of Reservoir Computing models using Information Theory approaches and Elementary Cellular Automata. I have taken part in the Google Summer of Code 2020 project with the Julia Language working on an implementation of a Reservoir Computing library. The library is now hosted under the Scientific Machine Learning organization. Here you can find a brief description of the project. Most of the posts in my website detail the progress during the three months of the program. Academic Career PhD student in Physics and Earth Sciences, Leipzig University, Germany. 2021 - today MSc in Computational Physics, University of Trieste, Italy. 2018 - 2021 BSc in Physics, University of Trieste, Italy. 2014 - 2018 Erasmus+ project, RWTH Aachen, Germany. 2016 - 2017 ","date":"26265-526-05","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"}]