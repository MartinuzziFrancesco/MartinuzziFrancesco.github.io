<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Francesco Martinuzzi</title>
    <link>https://martinuzzifrancesco.github.io/</link>
    <description>Recent content on Francesco Martinuzzi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Aug 2020 16:52:06 +0200</lastBuildDate><atom:link href="https://martinuzzifrancesco.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Research</title>
      <link>https://martinuzzifrancesco.github.io/research/</link>
      <pubDate>Sun, 03 Oct 2021 13:33:58 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/research/</guid>
      <description>Summary My PhD research is focused on analyzing the dynamics of extreme events and their consequences on the biosphere. For this task I am using a particular Machine Learning family of models, called Reservoir Computing. More broadly, my interests lie in the analysis on nonlinear systems through the lens of machine learning.
Pubblications For a full and always up to date list please check my Scholar profile.
  ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models arXiv pdf Martinuzzi, F.</description>
    </item>
    
    <item>
      <title>Google Summer of Code 2020 Final Report</title>
      <link>https://martinuzzifrancesco.github.io/posts/12_gsoc_week/</link>
      <pubDate>Tue, 25 Aug 2020 16:52:06 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/12_gsoc_week/</guid>
      <description>Introduction My proposal for the 2020 Google Summer of Code with the Julia Language was based on the implementation of a library for the family of models know as Reservoir Computing. After working for a month on the issue #34 of NeuralPDE.jl I created the initial draft of the ReservoirComputing.jl package, consisting at the time of only the implementation of Echo State Networks (ESNs). Having prior knowledge on the model I started digging a little more on the literature and found a lot of interesting variations of ESNs, which led me to base my proposal entirely on the addition of these models into the existing library.</description>
    </item>
    
    <item>
      <title>GSoC week 11: Gated Recurring Unit-based reservoir</title>
      <link>https://martinuzzifrancesco.github.io/posts/11_gsoc_week/</link>
      <pubDate>Sun, 16 Aug 2020 21:14:36 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/11_gsoc_week/</guid>
      <description>Following an architecture found on [1] this week we decided to implement a reservoir model based on the Gated Recurring Unit (GRU) structure, first described in [2]. This architecture is an evolution of the standard Recurrent Neural Network (RNN) update equations and works in a similar way to Long Short Term Memory (LSTM) with a forget gate but with fewer parameters; the LSTM usually outperfofms the GRU in most task but it could be interesting to see the behavior of this unit in the Echo State Network (ESN) model.</description>
    </item>
    
    <item>
      <title>GSoC week 10: Reservoir Memory Machines</title>
      <link>https://martinuzzifrancesco.github.io/posts/10_gsoc_week/</link>
      <pubDate>Thu, 13 Aug 2020 14:41:28 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/10_gsoc_week/</guid>
      <description>For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year [1]. This was one of the hardest, and longest, implementations to date and I believe there is still some work to be done. In this post we will briefly touch on the theory behind the model, and after an example of their usage will be presented.</description>
    </item>
    
    <item>
      <title>GSoC week 9: Cycle Reservoirs with Regular Jumps</title>
      <link>https://martinuzzifrancesco.github.io/posts/09_gsoc_week/</link>
      <pubDate>Sun, 02 Aug 2020 16:45:38 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/09_gsoc_week/</guid>
      <description>This week body of work is less then the usual amount, since most of my time was spent watching the incredible talks given at JuliaCon 2020. This was my first time attending and I just wanted to spend a few lines congratulating all the speakers for the amazing work they are doing with Julia, and most importantly I wanted to thank the organizers for the fantastic job they did: it really felt like an actual physical conference and the sense of community was truly awesome to experience.</description>
    </item>
    
    <item>
      <title>GSoC week 8: Reservoir Computing with Cellular Automata Part 2</title>
      <link>https://martinuzzifrancesco.github.io/posts/08_gsoc_week/</link>
      <pubDate>Sun, 26 Jul 2020 18:14:43 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/08_gsoc_week/</guid>
      <description>Continuing the work started last week we are going to further explore the capabilities of Reservoir Computing using Cellular Automata (CA) as the reservoir. As always a little theorical introduction is given and then we will illustrate the use of the model implemented in ReservoirComputing.jl.
Reservoir Computing with Two Dimensional Cellular Automata Two Dimensional Cellular Automata (Conway&amp;rsquo;s Game of Life) In the previous week we used Elementary CA (ECA) to train our model, and this time we want to see if we are able to obtain similar results using a two dimensional CA.</description>
    </item>
    
    <item>
      <title>GSoC week 7: Reservoir Computing with Cellular Automata Part 1</title>
      <link>https://martinuzzifrancesco.github.io/posts/07_gsoc_week/</link>
      <pubDate>Sun, 19 Jul 2020 17:27:05 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/07_gsoc_week/</guid>
      <description>In the past few years a new framework based on the concept of Reservoir Computing has been proposed: the Cellular Automata based Reservoir Computer (ReCA) [1]. The advantage it proposes over standard implementations is given by the binary state of the reservoir and the fact that it doesn&amp;rsquo;t require much parameter tuning to obtain state of the art results. Since the initial conception of the use of ECA for reservoir computing numerous improvement have taken place.</description>
    </item>
    
    <item>
      <title>GSoC week 6: minimum complexity echo state network</title>
      <link>https://martinuzzifrancesco.github.io/posts/06_gsoc_week/</link>
      <pubDate>Sun, 12 Jul 2020 14:37:22 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/06_gsoc_week/</guid>
      <description>Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning. And even when the optimal parameters are found, the prediction is run-dependent and can show different results with different generations of the reservoir. Is this the only way possible to contruct an Echo State Network (ESN)? Is there a deterministic way to build a ESN? These are the question posed in [1], and the following post is an illustration of the implementation in ReservoirComputing.</description>
    </item>
    
    <item>
      <title>Data-driven prediction of chaotic systems: comparison of Echo State Network variations </title>
      <link>https://martinuzzifrancesco.github.io/posts/05_gsoc_week/</link>
      <pubDate>Sun, 05 Jul 2020 21:54:29 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/05_gsoc_week/</guid>
      <description>This post is meant to be a high level comparison of different variations of the Echo State Network (ESN) model, implemented in the first month of Google Summer of Code. The theoretical background for all the proposed models has already been covered in past posts, so we will not touch on it in this one to keep things as thight as possible; if one is interested all my previous posts can be found here.</description>
    </item>
    
    <item>
      <title>GSoC week 4: SVD-based Reservoir</title>
      <link>https://martinuzzifrancesco.github.io/posts/04_gsoc_week/</link>
      <pubDate>Sun, 28 Jun 2020 21:02:54 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/04_gsoc_week/</guid>
      <description>The standard construction of the reservoir matrix \( \textbf{W} \) for Echo State Networks (ESN) is based on initializing \( \textbf{W} \) using specific schemes, usually generating random numbers and then rescaling it to make sure that the spectral radius is less or equal to a chosen number. This procedure is effective, but in literature other ways are explored. In this week we implemented a Singular Value Decomposition (SVD)-based algorithm, described in [1], that is capable of obtaining a sparse matrix suitable for the ESNs.</description>
    </item>
    
    <item>
      <title>GSoC week 3: Echo State Gaussian Processes</title>
      <link>https://martinuzzifrancesco.github.io/posts/03_gsoc_week/</link>
      <pubDate>Sun, 21 Jun 2020 16:12:50 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/03_gsoc_week/</guid>
      <description>Continuing to leverage similarities between Reservoir Computing models and Kernel machines, this week&amp;rsquo;s implementation merges the Bayesian approach of Gaussian Process Regression (GPR) to the Echo State Networks (ESN). After the usual overview of the theory, a computational example will be shown.
Gaussian Process Regression The following section is meant as a quick reminder of the deep theory behind Gaussian Processes and its purpose is to illustrate how this approach is a good fit for ESNs.</description>
    </item>
    
    <item>
      <title>GSoC week 2: Support Vector Regression in Echo State Networks</title>
      <link>https://martinuzzifrancesco.github.io/posts/02_gsoc_week/</link>
      <pubDate>Sun, 14 Jun 2020 15:07:04 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/02_gsoc_week/</guid>
      <description>The second week of the Google Summer of Code project with ReservoirComputing.jl dealt with the implementation of a Support Vector based regression for the Echo State Network model, resulting in the Support Vector Echo-State Machine (SVESM). In this post we will quickly touch on the theory behind Support Vector Regression (SVR), and then we will see the results of the implementation into the library. At the end a couple of examples are given, togheter with a comparison between SVR and SVESMs.</description>
    </item>
    
    <item>
      <title>GSoC week 1: lasso, Elastic Net and Huber loss</title>
      <link>https://martinuzzifrancesco.github.io/posts/01_gsoc_week/</link>
      <pubDate>Sun, 07 Jun 2020 19:06:37 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/01_gsoc_week/</guid>
      <description>The first week tackled the implementation of different kind of linear regression for the creation of the last layer in the Echo State Network. More specifically were added the possibility to add a \( l_1 \) regularization to the loss function (Lasso regression), both \( l_1 \) and \( l_2 \) regularizations (Elastic Net regression) and also added the possibility to choose the Huber loss function. As in the last post we will start from a brief theoretical background to explain the code and then we will showcase some examples taken from the literature.</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://martinuzzifrancesco.github.io/contact/</link>
      <pubDate>Tue, 26 May 2020 22:37:01 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/contact/</guid>
      <description>Emails You can write me at the following:
 martinuzzi(at)informatik(dot)uni-leipzig(dot)de francesco(dot)martinuzzi(at)uni-leipzig(dot)de  These are past email addresses I had, if you see papers/links with these email you can reach me using the ones above:
 francesco(at)juliacomputing(dot)com  You can also catch me on Telegram: (at)martinuzzifrancesco
where (dot) = . and (at) = @</description>
    </item>
    
    <item>
      <title>A brief introduction to Reservoir Computing</title>
      <link>https://martinuzzifrancesco.github.io/posts/a-brief-introduction-to-reservoir-computing/</link>
      <pubDate>Tue, 26 May 2020 22:17:09 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/posts/a-brief-introduction-to-reservoir-computing/</guid>
      <description>This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package ReservoirComputing.jl as example tool. This package is a work in progress and it is currently the main project I am working on as part of the Google Summer of Code program. Future posts are going to further explain the various implementations and improvements to the code by means of comparisons with the literature and examples.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://martinuzzifrancesco.github.io/about/</link>
      <pubDate>Tue, 26 May 2020 21:54:40 +0200</pubDate>
      
      <guid>https://martinuzzifrancesco.github.io/about/</guid>
      <description>Introduction Hello there! I am Francesco Martinuzzi, a PhD student in Physics and Earth Sciences at Leipzig University in Germany. I am under the supervision of Prof. Miguel D. Mahecha and Dr. Karin Mora at the Remote Sensing Centre for Earth System Research RSC4Earth. My research is kindly funded by the Center for Scalable Data Analytics and Artificial Intelligence ScaDS.AI. I am also part of the ELLIS Society, and as part of its exchange program I will spend six month at the Universitat de València working with Prof.</description>
    </item>
    
  </channel>
</rss>
