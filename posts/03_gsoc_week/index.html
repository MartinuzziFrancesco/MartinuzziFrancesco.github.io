<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.72.0" />

  <title>GSoC week 3: Echo State Gaussian Processes &middot; Francesco Martinuzzi</title>

  <meta name="description" content="" />

  

<meta itemprop="name" content="GSoC week 3: Echo State Gaussian Processes">
<meta itemprop="description" content="Continuing to leverage similarities between Reservoir Computing models and Kernel machines, this week&rsquo;s implementation merges the Bayesian approach of Gaussian Process Regression (GPR) to the Echo State Networks (ESN).">
<meta itemprop="datePublished" content="2020-06-21T16:12:50&#43;02:00" />
<meta itemprop="dateModified" content="2020-06-21T16:12:50&#43;02:00" />
<meta itemprop="wordCount" content="1212">
<meta itemprop="image" content="https://martinuzzifrancesco.github.io/images/"/>



<meta itemprop="keywords" content="" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://martinuzzifrancesco.github.io/images/"/>

<meta name="twitter:title" content="GSoC week 3: Echo State Gaussian Processes"/>
<meta name="twitter:description" content="Continuing to leverage similarities between Reservoir Computing models and Kernel machines, this week&rsquo;s implementation merges the Bayesian approach of Gaussian Process Regression (GPR) to the Echo State Networks (ESN)."/>


<meta property="og:title" content="GSoC week 3: Echo State Gaussian Processes" />
<meta property="og:description" content="Continuing to leverage similarities between Reservoir Computing models and Kernel machines, this week&rsquo;s implementation merges the Bayesian approach of Gaussian Process Regression (GPR) to the Echo State Networks (ESN)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/03_gsoc_week/" />
<meta property="og:image" content="https://martinuzzifrancesco.github.io/images/"/>
<meta property="article:published_time" content="2020-06-21T16:12:50+02:00" />
<meta property="article:modified_time" content="2020-06-21T16:12:50+02:00" /><meta property="og:site_name" content="Francesco Martinuzzi" />



  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/print.css"
        media="print">

  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/poole.css">

  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/hyde.css">

  
<style type="text/css">
  .sidebar {
    background-color: #536060;
  }

  .read-more-link a {
    border-color: #536060;
  }

  .pagination li a {
    color: #536060;
    border: 1px solid #536060;
  }

  .pagination li.active a {
    background-color: #536060;
  }

  .pagination li a:hover {
    background-color: #536060;
    opacity: 0.75;
  }

  footer a,
  .content a,
  .related-posts li a:hover {
    color: #536060;
  }
</style>



  

  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700&display=swap">

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css"
        integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk="
        crossorigin="anonymous" />

  <link rel="apple-touch-icon-precomposed"
        sizes="144x144"
        href="/apple-touch-icon-144-precomposed.png">

  <link rel="shortcut icon" href="/favicon.png">

  
  </head>
<body>
  <aside class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      
        
        <div class="author-image">
          <img src="https://martinuzzifrancesco.github.io/images/prova.jpg" class="img-circle img-headshot center" alt="Profile Picture">
        </div>
        
      

      <h1>Francesco Martinuzzi</h1>

      
      <p class="lead">Physicist learning how to make machines learn</p>
      
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li>
          <a href="https://martinuzzifrancesco.github.io/">Home</a>
        </li>
        <li>
          <a href="/posts/"> Posts </a>
        </li><li>
          <a href="/about/"> About </a>
        </li><li>
          <a href="/contact/"> Contact </a>
        </li>
      </ul>
    </nav>

    <section class="social-icons">
      
      <a href="https://github.com/MartinuzziFrancesco" rel="me" title="GitHub">
        <i class="fab fa-github" aria-hidden="true"></i>
      </a>
      
      <a href="https://twitter.com/MartinuzziFra" rel="me" title="Twitter">
        <i class="fab fa-twitter" aria-hidden="true"></i>
      </a>
      
    </section>
  </div>
</aside>


  <main class="content container">
  <div class="post">
  <h1>GSoC week 3: Echo State Gaussian Processes</h1>

  <div class="post-date">
    <time datetime="2020-06-21T16:12:50&#43;0200">Jun 21, 2020</time> · 6 min read
  </div>

  <p>Continuing to leverage similarities between Reservoir Computing models and Kernel machines, this week&rsquo;s implementation merges the Bayesian approach of Gaussian Process Regression (GPR) to the Echo State Networks (ESN). After the usual overview of the theory, a computational example will be shown.</p>
<h1 id="gaussian-process-regression">Gaussian Process Regression</h1>
<p>The following section is meant as a quick reminder of the deep theory behind Gaussian Processes and its purpose is to illustrate how this approach is a good fit for ESNs.
An in depth chapter on GP regression can be found in <a href="#1">[1]</a>. A good introduction is also given in the paper that illustrates the implementation of Echo State Gaussian Processes (ESGP) <a href="#2">[2]</a>. In this introduction we will heavily follow the second reference, to keep the notation consistent.</p>
<p>A Gaussian Process (GP) is defined as a collection of random variables, any finite of which have a joint Gaussian distribution.
A (GP) is completely specified by its mean function and covariance function:
$$m(\textbf{x}) = \mathbb{E}[f(\textbf{x})]$$
$$k(\textbf{x}, \textbf{x}') = \mathbb{E}[(f(\textbf{x})-m(\textbf{x}))(f(\textbf{x}')-m(\textbf{x}'))] $$
and the GP can be written as
$$f(\textbf{x}) \sim GP(m(\textbf{x}), k(\textbf{x}, \textbf{x}'))$$
An usual choice for the mean function is the zero mean function \( m(\textbf{x} = 0) \), and for the covariance function there is a large variety of kernel functions to choose from. In fact there are so many that one can be overwhelmed by the choice; if this is the case, a good introduction and overview can be found in <a href="#3">[3]</a>.</p>
<p>Given our data, consisting in the samples \( ( (\textbf{x}_i, y_i) | i = 1,&hellip;,N ) \), where \( \textbf{x}_i \) is the d-dimensional observation and \( y_i \) is the correleted target values, we want to be able to predict \( y_* \) given \( \textbf{x}_* \). The response variables \( y_i \) are assumed to be dependent on the predictors \( \textbf{x}_i \):</p>
<p>$$y_i \sim \mathcal{N} (f(\textbf{x}_i), \sigma ^2), i=1,&hellip;,N$$</p>
<p>Once defined mean and kernel functions one can obtain the predicted mean and variance:
$$\mu _* = \textbf{k}(\textbf{x}_*)^T(\textbf{K}(X, X)+\sigma ^2 \textbf{I}_N)^{-1}\textbf{y}$$</p>
<p>$$\sigma ^2 _* = k(\textbf{x}_*, \textbf{x} _*) - \textbf{k}(\textbf{x}_*)^T(\textbf{K}(X, X)+\sigma ^2 \textbf{I}_N)^{-1} \textbf{k}(\textbf{x}_*)$$</p>
<p>where \( \textbf{K}(X, X) \) is the matrix of the covariances (design matrix). The optimization of the hyperparameters is usually done by maximization of the model marginal likelihood.</p>
<h1 id="echo-state-gaussian-processes">Echo State Gaussian Processes</h1>
<p>Using the definition given in the paper <a href="#2">[2]</a> an ESGP is a GP the covariance of which is taken as a kernel function over the states of a ESN, postulated to capture the dynamics within a set of sequentially interdependent observations. In this case the feature mapping is explicit, no kernel trick is adopted.</p>
<p>One of the improvements of this approach against strandard linear regression is the possibility to obtain a measure of uncertainty for the obtained predictions. Furthermore, one can consider this a generalization of simple regression: in fact using a simple linear kernel and setting \( \sigma ^2 = 0 \) the results should be the same of those obtained by plain linear regression.</p>
<p>The paper shows results obtained using the Gaussian RBF kernel, but the high number of kernel available and the possibility to use combination of them makes this approach really versatile and, as of now, somewhat understudied.</p>
<h2 id="implementation-in-reservoircomputingjl">Implementation in ReservoirComputing.jl</h2>
<p>Building on the package <a href="https://github.com/STOR-i/GaussianProcesses.jl">GaussianProcesses</a> it was possible to create a <code>ESGPtrain</code> function as well as a <code>ESGPpredict</code> and a <code>ESGPpredict_h_steps</code> function, with a similar behaviour to the ESN counterpart. The <code>ESGPtrain</code> function takes as input:</p>
<ul>
<li>
<p>esn: the previously defined ESN</p>
</li>
<li>
<p>mean: a GaussianProcesses.Mean struct, to choose between the ones provided by the GaussianProcesses package</p>
</li>
<li>
<p>kernel: a GaussianProcesses.Kernel struct, to choose between the ones provided by the GaussianProcesses package</p>
</li>
<li>
<p>lognoise: optional value with default = -2</p>
</li>
<li>
<p>optimize: optional value with default = false. If = true the hyperparameters are optimized using <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a>. Since gradients are available for all mean and kernel functions, gradient based optimization techniques are recommended.</p>
</li>
<li>
<p>optimizer: optional value with default = Optim.LBFGS()</p>
</li>
<li>
<p>y_target: optional value with default = esn.train_data. This way the system learns to predict the next step in the time series, but the user is free to set other possibilities.</p>
</li>
</ul>
<p>The function returns a trained GP, that can be used in <code>ESGPpredict</code> or <code>ESGPpredict_h_steps</code>. They both take as input</p>
<ul>
<li>esn: the previously defined ESN</li>
<li>predict_len: number of steps of the prediction</li>
<li>gp: a trained GaussianProcesses.GPE struct</li>
</ul>
<p>in addition <code>ESGPpredict_h_steps</code> requires</p>
<ul>
<li>h_steps: the h steps ahead in wich the model will run autonomously</li>
<li>test_data: the testing data, to be given as input to the model every h-th step.</li>
</ul>
<h2 id="example">Example</h2>
<p>Similarly to last week the example is based on the <a href="http://www.scholarpedia.org/article/Mackey-Glass_equation">Mackey Glass</a> system. It can be described by</p>
<p>$$\frac{dx}{dt} = \beta x(t)+\frac{\alpha x(t-\delta)}{1+x(t-\delta)^2}$$</p>
<p>and the values adopted in <a href="#2">[2]</a> are</p>
<ul>
<li>\(\beta = -0.1 \)</li>
<li>\(\alpha = 0.2 \)</li>
<li>\(\delta = 17 \)</li>
<li>\( dt = 0.1 \)</li>
</ul>
<p>Furthermore the time series is rescaled in the range \( [-1, 1] \) by application of a tangent hyperbolic transform \( y_{ESN}(\text{t}) = \text{tanh}(\text{y}(t)-1) \). To evaluate the precision of our results we are going to use root mean square deviation (rmse), defined as:</p>
<p>$$\textbf{nmrse} = \sqrt{\frac{\sum_{i=1}^{T_n}(y_d(i)-y(i))^2}{T_n}}$$</p>
<p>where</p>
<ul>
<li>\(y_d(i) \) is the target value</li>
<li>\(y(i) \) is the predicted value</li>
<li>\(T_d \) is the number of test examples</li>
</ul>
<p>The ESN parameters are as follows</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia"><span style="color:#66d9ef">const</span> shift <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
<span style="color:#66d9ef">const</span> train_len <span style="color:#f92672">=</span> <span style="color:#ae81ff">6000</span>
<span style="color:#66d9ef">const</span> test_len <span style="color:#f92672">=</span><span style="color:#ae81ff">1500</span>

<span style="color:#66d9ef">const</span> approx_res_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">400</span>
<span style="color:#66d9ef">const</span> sparsity <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
<span style="color:#66d9ef">const</span> activation <span style="color:#f92672">=</span> tanh
<span style="color:#66d9ef">const</span> radius <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.99</span>
<span style="color:#66d9ef">const</span> sigma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>

<span style="color:#66d9ef">const</span> alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>
<span style="color:#66d9ef">const</span> nla_type <span style="color:#f92672">=</span> NLADefault()
<span style="color:#66d9ef">const</span> extended_states <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span>
</code></pre></div><p>Something worth pointing out is that for the first time we have found a value of <code>alpha</code> other than 1: this means we are dealing with a leaky ESN. Following the paper we will try to give a fair comparison between ESN trained with Ridge regression , ESGP and SVESM. Since the parameters for ESN with Ridge and SVESM are missing in the paper, we thought best to use &ldquo;default&rdquo; parameters for all the model involved: using optimization only on one model did not seem like a fair comparison. Both in SVESM and ESGP the kernel function used is Gaussian RBF.</p>
<p>The results are as follows:</p>
<pre><code>ESGP RBF rmse: 0.0298
ESN ridge rmse: 0.1718
SVESM RBF rmse: 0.1922
</code></pre><p>We can clearly see that the proposed model is outperforming the other models proposed. We mentioned that one of the improvements of this models was the measure of uncertainty relative to the prediction. The <code>ESGPpredict</code> function also returns the variance of the prediction, that can be plotted alongside the results:</p>
<p><img src="https://user-images.githubusercontent.com/10376688/85236826-af86f800-b421-11ea-81c5-e05cd85e8a6c.png" alt="full_pred2"></p>
<p>From the plot is even more clear that the ESGP is more capable of reproducing the behaviour of the Mackey-Glass system. Like in the paper the worst performing model for this specific task is the SVESM.</p>
<p>Beside a better analysis of the parameters to use in this case, it would also be interesting to see a comparison between the normal GPR with different kernel functions and the ESGP with the same kernel functions.</p>
<p>As always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!</p>
<h2 id="documentation">Documentation</h2>
<p><!-- raw HTML omitted -->[1]<!-- raw HTML omitted -->
Rasmussen, Carl Edward. &ldquo;Gaussian processes in machine learning.&rdquo; Summer School on Machine Learning. Springer, Berlin, Heidelberg, 2003.</p>
<p><!-- raw HTML omitted -->[2]<!-- raw HTML omitted -->
Chatzis, Sotirios P., and Yiannis Demiris. &ldquo;Echo state Gaussian process.&rdquo; IEEE Transactions on Neural Networks 22.9 (2011): 1435-1445.</p>
<p><!-- raw HTML omitted -->[3]<!-- raw HTML omitted -->
<a href="https://www.cs.toronto.edu/~duvenaud/cookbook/">https://www.cs.toronto.edu/~duvenaud/cookbook/</a></p>

</div>


  </main>

  <footer>
  <div>
    &copy; Martinuzzi Francesco 2020
    ·
    
    <a href="https://creativecommons.org/licenses/by-sa/4.0"
       target="_blank">CC BY-SA 4.0</a>
    
    
  </div>
</footer>

</body>

    <script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
            fontCache: 'global'
        }
    };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script> 


</html>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/js/all.min.js"
          integrity="sha256-MAgcygDRahs+F/Nk5Vz387whB4kSK9NXlDN3w58LLq0="
          crossorigin="anonymous"></script>

  

  
</body>
</html>
