<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GSoC week 2: Support Vector Regression in Echo State Networks | Francesco Martinuzzi</title><meta name=keywords content><meta name=description content="The second week of the Google Summer of Code project with ReservoirComputing.jl dealt with the implementation of a Support Vector based regression for the Echo State Network model, resulting in the Support Vector Echo-State Machine (SVESM). In this post we will quickly touch on the theory behind Support Vector Regression (SVR), and then we will see the results of the implementation into the library. At the end a couple of examples are given, togheter with a comparison between SVR and SVESMs."><meta name=author content="Francesco Martinuzzi"><link rel=canonical href=https://martinuzzifrancesco.github.io/posts/02_gsoc_week/><link crossorigin=anonymous href=/assets/css/stylesheet.849fd8bd636f9bdcc8fd3087509c431a61faae78bd735e7ba75e6fd13ec64f83.css integrity="sha256-hJ/YvWNvm9zI/TCHUJxDGmH6rni9c157p15v0T7GT4M=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://martinuzzifrancesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://martinuzzifrancesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://martinuzzifrancesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://martinuzzifrancesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://martinuzzifrancesco.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="GSoC week 2: Support Vector Regression in Echo State Networks"><meta property="og:description" content="The second week of the Google Summer of Code project with ReservoirComputing.jl dealt with the implementation of a Support Vector based regression for the Echo State Network model, resulting in the Support Vector Echo-State Machine (SVESM). In this post we will quickly touch on the theory behind Support Vector Regression (SVR), and then we will see the results of the implementation into the library. At the end a couple of examples are given, togheter with a comparison between SVR and SVESMs."><meta property="og:type" content="article"><meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/02_gsoc_week/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-06-14T15:07:04+02:00"><meta property="article:modified_time" content="2020-06-14T15:07:04+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GSoC week 2: Support Vector Regression in Echo State Networks"><meta name=twitter:description content="The second week of the Google Summer of Code project with ReservoirComputing.jl dealt with the implementation of a Support Vector based regression for the Echo State Network model, resulting in the Support Vector Echo-State Machine (SVESM). In this post we will quickly touch on the theory behind Support Vector Regression (SVR), and then we will see the results of the implementation into the library. At the end a couple of examples are given, togheter with a comparison between SVR and SVESMs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://martinuzzifrancesco.github.io/posts/"},{"@type":"ListItem","position":2,"name":"GSoC week 2: Support Vector Regression in Echo State Networks","item":"https://martinuzzifrancesco.github.io/posts/02_gsoc_week/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GSoC week 2: Support Vector Regression in Echo State Networks","name":"GSoC week 2: Support Vector Regression in Echo State Networks","description":"The second week of the Google Summer of Code project with ReservoirComputing.jl dealt with the implementation of a Support Vector based regression for the Echo State Network model, resulting in the Support Vector Echo-State Machine (SVESM). In this post we will quickly touch on the theory behind Support Vector Regression (SVR), and then we will see the results of the implementation into the library. At the end a couple of examples are given, togheter with a comparison between SVR and SVESMs.","keywords":[],"articleBody":"The second week of the Google Summer of Code project with ReservoirComputing.jl dealt with the implementation of a Support Vector based regression for the Echo State Network model, resulting in the Support Vector Echo-State Machine (SVESM). In this post we will quickly touch on the theory behind Support Vector Regression (SVR), and then we will see the results of the implementation into the library. At the end a couple of examples are given, togheter with a comparison between SVR and SVESMs.\nTheoretical Background Support Vector Regression What follows is just an overview of the theory behind SVR, which takes for granted a priori knowledge of the reader of the more general concepts of Support Vector Machines. A more knowledgable and in depth exposition can be found in [1] and [2]. Most of the information presented in this section is taken from these papers unless stated otherwise.\nIn a general setting, a regression task can be expressed as the minimization of the following primal objective function: $$C \\sum_{j=1}^{N}L[\\textbf{x}_j, y_{dj}, f ] + ||\\textbf{w}||^2$$ where\n \\(L\\) is a general loss function \\(f\\) is the prediction function defined by \\(\\textbf{W}\\) and \\(b\\) \\(C\\) is a regularization constant  If the loss function is quadratic the objective function can be minimized by means of linear algebra, and the methodology is called Ridge Regression.\nThe most used function in SVR is called \\(\\epsilon\\)-insensitive loss function [3]:\n$$L = 0 \\ \\text{ if } |f(x)-y| More specifically we can consider the equation\n$$\\text{min}||\\textbf{w}||^2+C\\sum_{j=1^{N_t}}(\\xi _j+\\hat{\\xi}_j)$$\nwith constraints (for \\(j=1,…,N_t\\)): $$(\\textbf{w}^T \\textbf{x}_j+b)-y_j \\le \\epsilon - \\xi_j$$ $$y_j-(\\textbf{w}^T \\textbf{x}_j+b) \\le \\epsilon - \\xi_j$$ $$\\xi _j,\\hat{\\xi}_j \\ge 0$$\nthe solution of which can be found using the following dual problem\n$$\\text{min}_{\\alpha , \\alpha^*} \\frac{1}{2} \\sum_{i, j = 1}^{N_t}(\\alpha_i-\\alpha_i^*)(\\alpha_j-\\alpha_j^*)\\textbf{x}^T\\textbf{x}-\\sum_{i=1}^{N_t}(\\alpha_i-\\alpha_i^*) y_i + \\sum_{i=1}^{N_t}(\\alpha_i-\\alpha_i^*) \\epsilon$$\nwith constraints:\n$$0 \\le \\alpha _i, \\alpha _i^* \\le C, \\sum_{i=1}^{N_t}(\\alpha_i-\\alpha_i^*)=0$$\nWe can clearly see that to solve this task one has to resort to quadratic programming. The solution will have the form\n$$f(\\textbf{x}) = \\sum_{i=1}^{N_p}(\\alpha_i-\\alpha_i^*) k(\\textbf{x}_i, \\textbf{x})$$\nwhere \\(k\\) is a so-called kernel function, an implicit mapping of the data into an higher dimension, used to turn a non linear problem into a linear one. This is the “kernel trick”, where the mapping is not explicitly done, but is obtained using function that only require the computation of the inner products. Common kernels includes:\n Linear \\( k(\\textbf{x}_i, \\textbf{x}_j) = \\textbf{x}_i \\cdot \\textbf{x}_j\\) Polynomial \\(k(\\textbf{x}_i, \\textbf{x}_j) = (\\textbf{x}_i \\cdot \\textbf{x}_j)^d \\) Gaussian Radial Basis Function \\( k(\\textbf{x}_i, \\textbf{x}_j) = e^{ \\lambda ||\\textbf{x}_i - \\textbf{x}_j||^2} \\)  Support Vector Echo State Machines We can see that the intuition behind the Reservoir Computing approach is similar to the kernel methods: using a fixed Recurrent Neural Network (for the case of the ESN) we are also mapping the input into a higher dimension. Therefore the connection between the two models was almost immediate and the idea developed in [4] was to perform a linear SVR in the higer dimensional reservoir state space.\nIn the paper different loss function are analized: quadratic loss function, \\( \\epsilon \\)-insensitive loss function and the Huber loss function. A new method of prediction is also proposed, called the direct method. This method is a variation of the one step ahead prediction, in which the desired output is not the next step, but a \\( h \\) steps ahead one. The input-output training sequences can be described as \\( \\textbf{d}(k), \\textbf{x}(k+h), k=1, 2, 3,…, N_t \\) where \\( \\textbf{d}(k) \\) is the embedding of the target time series and \\( \\textbf{x}(k+h) \\) is the \\( h \\) steps ahead target output for training/testing.\nImplementation in ReservoirComputing.jl The implementation of SVESM into the library is done leveraging the LIBSVM.jl package, a wrapper for the package of the same name written in C++. With this we were able to implement the \\( \\epsilon \\)-insensitive loss function based SVR, creating a new SVESMtrain function as well as a SVESM_direct_predict function. The SVESMtrain takes as input\n svr: a AbstractSVR object that can be both EpsilonSVR or NuSVR. This implementation also allows the user to choose a kernel other than the linear one like the one used in the paper. Actually in comparisons done in other papers, different kernels have been used with SVESMs. esn: the previously constructed ESN y_target: the one-dimensional target output \\( \\textbf{x}(k+h) \\)  The SVESM_direct_predict function takes as input\n esn: the previously constructed ESN test_in: the testing portion of the input data \\( \\textbf{d}(k) \\) m: the output from SVESMtrain  The quadratic loss function and Huber loss function are already implemented and the ESN can be trained using one of them through ESNtrain(Ridge(), esn) or ESNtrain(RobustHuber(), esn).\nExamples Following the example used in the paper we will try to predict the 84 steps ahead Mackey Glass system. It can be described by\n$$\\frac{dx}{dt} = \\beta x(t)+\\frac{\\alpha x(t-\\delta)}{1+x(t-\\delta)^2}$$\nand the values adopted in [4] are\n \\(\\beta = -0.1 \\) \\(\\alpha = 0.2 \\) \\(\\delta = 17 \\)  The timeseries is then embedded\n$$\\textbf{d}(k) = [x(k), x(k-\\tau), x(k-2 \\tau), x(k-3 \\tau)]$$ with dimension 4 and \\(\\tau = 6\\). The target output is \\( y_d(k) = x(k+84) \\). We are going to evaluate the precision of our prediction using nrmse, defined as $$\\textbf{nmrse} = \\sqrt{\\frac{\\sum_{i=1}^{T_n}(y_d(i)-y(i))^2}{T_n \\cdot \\sigma ^2}}$$\nwhere\n \\(y_d(i) \\) is the target value \\(y(i) \\) is the predicted value \\(T_d \\) is the number of test examples \\(\\sigma ^2 \\) is the variance of the original time series  The data is obtained using Runge Kutta of order 4 with a stepsize of 0.1.\nWe will perform three tests, and in all three we are also going to give the results of SVR using different kernel to make a comparison with the results obtained by SVESM. The first test is conducted on noiseless test and training samples. In the second one we will add noise in the training portion of \\( \\textbf{d}(k) \\), and in the last noise will be added both in training and testing. The target values remain noiseless in all three tests. The noise level is determined by the ratio of the standard deviation of the noise and the signal standard deviation, and it is chosen to be 20%.\nBefore the exposition of the results we need to address the fact that sadly in the original paper only a few parameters are given: the size of the reservoir, sparseness and spectral radius of the reservoir matrix and the scaling of the input weights. Missing parameters like \\( C \\) or \\( \\epsilon \\) for the SVESM training of course means that our results are not comparable with those showed in the paper. Nevertheless, using the default parameters, the results obtained show that the model proposed can outperform SVR with both polynomial and radial basis kernels in all three tests.\nThe parameters used for the ESN are as follows\nconst shift = 200 const train_len = 1000 const test_len = 500 const h = 84  const approx_res_size = 700 const sparsity = 0.02 const activation = tanh const radius = 0.98 const sigma = 0.25  const alpha = 1.0 const nla_type = NLADefault() const extended_states = true  W = init_reservoir_givensp(approx_res_size, radius, sparsity) W_in = init_dense_input_layer(approx_res_size, size(train_in, 1), sigma) esn = ESN(W, train_in, W_in, activation, alpha, nla_type, extended_states) Noiseless training and testing Using a dataset of lenght 500 we can see that the results for the noiseless training and testing are\nSVESM nmrse for the noiseless test: 0.343 SVM Poly kernel nmrse for the noiseless test: 0.489 SVM RadialBasis kernel nmrse for the noiseless test: 0.393 We can also plot the results to better appreciate the results (the lenght of the prediction chosen for the plots is higher in order to better visualize the differences in trajectories):\nFrom the results it is clear that the SVESM performs better.\nNoisy training and noiseless testing Adding normally distributed white noise to the training input dataset we obtain the following results for the nmrse:\nTraining on noisy data and testing on noiseless data... SVESM nmrse for noisy training and noiseless testing: 0.415 SVM Poly kernel nmrse for noisy training and noiseless testing: 0.643 SVM RadialBasis kernel nmrse for noisy training and noiseless testing: 0.557 The plots are\nAlso in this case the performance of the SVESM is superior to SVR.\nNoisy training and testing For the last comparison the results are as follows\nTraining and testing on noisy data... SVESM nmrse for noisy training and testing: 0.439 SVM Poly kernel nmrse for noisy training and testing: 0.724 SVM RadialBasis kernel nmrse for noisy training and testing: 0.648 And in this case as well the proposed model outperforms SVR.\nAs always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!\nCiting Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:\n@article{JMLR:v23:22-0611,  author = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora},  title = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models},  journal = {Journal of Machine Learning Research},  year = {2022},  volume = {23},  number = {288},  pages = {1--8},  url = {http://jmlr.org/papers/v23/22-0611.html} } References [1]Drucker, Harris, et al. “Support vector regression machines.” Advances in neural information processing systems. 1997.\n[2]Smola, Alex J., and Bernhard Schölkopf. “A tutorial on support vector regression.” Statistics and computing 14.3 (2004): 199-222.\n[3]VladimirN. Vapnik, The Nature of Statistical Learning Theory, Springer, 1995.\n[4]Shi, Zhiwei, and Min Han. “Support vector echo-state machine for chaotic time-series prediction.” IEEE transactions on neural networks 18.2 (2007): 359-372.\n","wordCount":"1606","inLanguage":"en","datePublished":"2020-06-14T15:07:04+02:00","dateModified":"2020-06-14T15:07:04+02:00","author":{"@type":"Person","name":"Francesco Martinuzzi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://martinuzzifrancesco.github.io/posts/02_gsoc_week/"},"publisher":{"@type":"Organization","name":"Francesco Martinuzzi","logo":{"@type":"ImageObject","url":"https://martinuzzifrancesco.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://martinuzzifrancesco.github.io/ accesskey=h title="Francesco Martinuzzi (Alt + H)">Francesco Martinuzzi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://martinuzzifrancesco.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://martinuzzifrancesco.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://martinuzzifrancesco.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>GSoC week 2: Support Vector Regression in Echo State Networks</h1><div class=post-meta><span title='2020-06-14 15:07:04 +0200 CEST'>June 14, 2020</span>&nbsp;·&nbsp;Francesco Martinuzzi</div></header><div class=post-content><p>The second week of the Google Summer of Code project with ReservoirComputing.jl dealt with the implementation of a Support Vector based regression for the Echo State Network model, resulting in the Support Vector Echo-State Machine (SVESM). In this post we will quickly touch on the theory behind Support Vector Regression (SVR), and then we will see the results of the implementation into the library. At the end a couple of examples are given, togheter with a comparison between SVR and SVESMs.</p><h1 id=theoretical-background>Theoretical Background<a hidden class=anchor aria-hidden=true href=#theoretical-background>#</a></h1><h2 id=support-vector-regression>Support Vector Regression<a hidden class=anchor aria-hidden=true href=#support-vector-regression>#</a></h2><p>What follows is just an overview of the theory behind SVR, which takes for granted a priori knowledge of the reader of the more general concepts of Support Vector Machines. A more knowledgable and in depth exposition can be found in <a href=#1>[1]</a> and <a href=#2>[2]</a>. Most of the information presented in this section is taken from these papers unless stated otherwise.</p><p>In a general setting, a regression task can be expressed as the minimization of the following primal objective function:
$$C \sum_{j=1}^{N}L[\textbf{x}_j, y_{dj}, f ] + ||\textbf{w}||^2$$
where</p><ul><li>\(L\) is a general loss function</li><li>\(f\) is the prediction function defined by \(\textbf{W}\) and \(b\)</li><li>\(C\) is a regularization constant</li></ul><p>If the loss function is quadratic the objective function can be minimized by means of linear algebra, and the methodology is called Ridge Regression.</p><p>The most used function in SVR is called \(\epsilon\)-insensitive loss function <a href=#3>[3]</a>:</p><p>$$L = 0 \ \text{ if } |f(x)-y| &lt; \epsilon$$
$$L = |f(x)-y| - \epsilon \text{ otherwise }$$</p><p>More specifically we can consider the equation</p><p>$$\text{min}||\textbf{w}||^2+C\sum_{j=1^{N_t}}(\xi _j+\hat{\xi}_j)$$</p><p>with constraints (for \(j=1,&mldr;,N_t\)):
$$(\textbf{w}^T \textbf{x}_j+b)-y_j \le \epsilon - \xi_j$$
$$y_j-(\textbf{w}^T \textbf{x}_j+b) \le \epsilon - \xi_j$$
$$\xi _j,\hat{\xi}_j \ge 0$$</p><p>the solution of which can be found using the following dual problem</p><p>$$\text{min}_{\alpha , \alpha^*} \frac{1}{2} \sum_{i, j = 1}^{N_t}(\alpha_i-\alpha_i^*)(\alpha_j-\alpha_j^*)\textbf{x}^T\textbf{x}-\sum_{i=1}^{N_t}(\alpha_i-\alpha_i^*) y_i + \sum_{i=1}^{N_t}(\alpha_i-\alpha_i^*) \epsilon$$</p><p>with constraints:</p><p>$$0 \le \alpha _i, \alpha _i^* \le C, \sum_{i=1}^{N_t}(\alpha_i-\alpha_i^*)=0$$</p><p>We can clearly see that to solve this task one has to resort to quadratic programming. The solution will have the form</p><p>$$f(\textbf{x}) = \sum_{i=1}^{N_p}(\alpha_i-\alpha_i^*) k(\textbf{x}_i, \textbf{x})$$</p><p>where \(k\) is a so-called kernel function, an implicit mapping of the data into an higher dimension, used to turn a non linear problem into a linear one. This is the &ldquo;kernel trick&rdquo;, where the mapping is not explicitly done, but is obtained using function that only require the computation of the inner products. Common kernels includes:</p><ul><li>Linear \( k(\textbf{x}_i, \textbf{x}_j) = \textbf{x}_i \cdot \textbf{x}_j\)</li><li>Polynomial \(k(\textbf{x}_i, \textbf{x}_j) = (\textbf{x}_i \cdot \textbf{x}_j)^d \)</li><li>Gaussian Radial Basis Function \( k(\textbf{x}_i, \textbf{x}_j) = e^{ \lambda ||\textbf{x}_i - \textbf{x}_j||^2} \)</li></ul><h2 id=support-vector-echo-state-machines>Support Vector Echo State Machines<a hidden class=anchor aria-hidden=true href=#support-vector-echo-state-machines>#</a></h2><p>We can see that the intuition behind the Reservoir Computing approach is similar to the kernel methods: using a fixed Recurrent Neural Network (for the case of the ESN) we are also mapping the input into a higher dimension. Therefore the connection between the two models was almost immediate and the idea developed in <a href=#4>[4]</a> was to perform a linear SVR in the higer dimensional reservoir state space.</p><p>In the paper different loss function are analized: quadratic loss function, \( \epsilon \)-insensitive loss function and the Huber loss function. A new method of prediction is also proposed, called the direct method. This method is a variation of the one step ahead prediction, in which the desired output is not the next step, but a \( h \) steps ahead one. The input-output training sequences can be described as \( \textbf{d}(k), \textbf{x}(k+h), k=1, 2, 3,&mldr;, N_t \) where \( \textbf{d}(k) \) is the embedding of the target time series and \( \textbf{x}(k+h) \) is the \( h \) steps ahead target output for training/testing.</p><h1 id=implementation-in-reservoircomputingjl>Implementation in ReservoirComputing.jl<a hidden class=anchor aria-hidden=true href=#implementation-in-reservoircomputingjl>#</a></h1><p>The implementation of SVESM into the library is done leveraging the LIBSVM.jl package, a wrapper for the <a href=https://www.csie.ntu.edu.tw/~cjlin/libsvm/>package</a> of the same name written in C++. With this we were able to implement the \( \epsilon \)-insensitive loss function based SVR, creating a new <code>SVESMtrain</code> function as well as a <code>SVESM_direct_predict</code> function. The <code>SVESMtrain</code> takes as input</p><ul><li>svr: a <code>AbstractSVR</code> object that can be both <code>EpsilonSVR</code> or <code>NuSVR</code>. This implementation also allows the user to choose a kernel other than the linear one like the one used in the paper. Actually in comparisons done in other papers, different kernels have been used with SVESMs.</li><li>esn: the previously constructed ESN</li><li>y_target: the one-dimensional target output \( \textbf{x}(k+h) \)</li></ul><p>The <code>SVESM_direct_predict</code> function takes as input</p><ul><li>esn: the previously constructed ESN</li><li>test_in: the testing portion of the input data \( \textbf{d}(k) \)</li><li>m: the output from <code>SVESMtrain</code></li></ul><p>The quadratic loss function and Huber loss function are already implemented and the ESN can be trained using one of them through <code>ESNtrain(Ridge(), esn)</code> or <code>ESNtrain(RobustHuber(), esn)</code>.</p><h1 id=examples>Examples<a hidden class=anchor aria-hidden=true href=#examples>#</a></h1><p>Following the example used in the paper we will try to predict the 84 steps ahead <a href=http://www.scholarpedia.org/article/Mackey-Glass_equation>Mackey Glass</a> system. It can be described by</p><p>$$\frac{dx}{dt} = \beta x(t)+\frac{\alpha x(t-\delta)}{1+x(t-\delta)^2}$$</p><p>and the values adopted in <a href=#4>[4]</a> are</p><ul><li>\(\beta = -0.1 \)</li><li>\(\alpha = 0.2 \)</li><li>\(\delta = 17 \)</li></ul><p>The timeseries is then embedded</p><p>$$\textbf{d}(k) = [x(k), x(k-\tau), x(k-2 \tau), x(k-3 \tau)]$$
with dimension 4 and \(\tau = 6\). The target output is \( y_d(k) = x(k+84) \). We are going to evaluate the precision of our prediction using nrmse, defined as
$$\textbf{nmrse} = \sqrt{\frac{\sum_{i=1}^{T_n}(y_d(i)-y(i))^2}{T_n \cdot \sigma ^2}}$$</p><p>where</p><ul><li>\(y_d(i) \) is the target value</li><li>\(y(i) \) is the predicted value</li><li>\(T_d \) is the number of test examples</li><li>\(\sigma ^2 \) is the variance of the original time series</li></ul><p>The data is obtained using Runge Kutta of order 4 with a stepsize of 0.1.</p><p>We will perform three tests, and in all three we are also going to give the results of SVR using different kernel to make a comparison with the results obtained by SVESM. The first test is conducted on noiseless test and training samples. In the second one we will add noise in the training portion of \( \textbf{d}(k) \), and in the last noise will be added both in training and testing. The target values remain noiseless in all three tests. The noise level is determined by the ratio of the standard deviation of the noise and the signal standard deviation, and it is chosen to be 20%.</p><p>Before the exposition of the results we need to address the fact that sadly in the original paper only a few parameters are given: the size of the reservoir, sparseness and spectral radius of the reservoir matrix and the scaling of the input weights. Missing parameters like \( C \) or \( \epsilon \) for the SVESM training of course means that our results are not comparable with those showed in the paper. Nevertheless, using the default parameters, the results obtained show that the model proposed can outperform SVR with both polynomial and radial basis kernels in all three tests.</p><p>The parameters used for the ESN are as follows</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>const</span> shift <span style=color:#f92672>=</span> <span style=color:#ae81ff>200</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> train_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> test_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>500</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> h <span style=color:#f92672>=</span> <span style=color:#ae81ff>84</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>700</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> sparsity <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.02</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.98</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.25</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> nla_type <span style=color:#f92672>=</span> NLADefault()
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> extended_states <span style=color:#f92672>=</span> true
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>W <span style=color:#f92672>=</span> init_reservoir_givensp(approx_res_size, radius, sparsity)
</span></span><span style=display:flex><span>W_in <span style=color:#f92672>=</span> init_dense_input_layer(approx_res_size, size(train_in, <span style=color:#ae81ff>1</span>), sigma)
</span></span><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(W, train_in, W_in, activation, alpha, nla_type, extended_states)
</span></span></code></pre></div><h2 id=noiseless-training-and-testing>Noiseless training and testing<a hidden class=anchor aria-hidden=true href=#noiseless-training-and-testing>#</a></h2><p>Using a dataset of lenght 500 we can see that the results for the noiseless training and testing are</p><pre tabindex=0><code>SVESM nmrse for the noiseless test: 0.343
SVM Poly kernel nmrse for the noiseless test: 0.489
SVM RadialBasis kernel nmrse for the noiseless test: 0.393
</code></pre><p>We can also plot the results to better appreciate the results (the lenght of the prediction chosen for the plots is higher in order to better visualize the differences in trajectories):</p><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/84597690-fd21c480-ae65-11ea-9fc1-5a8219c3e65f.png alt=svesm_noiseless_comparison></p><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/84597698-0874f000-ae66-11ea-9615-f9e5af3ce26c.png alt=svm_noiseless_comparison></p><p>From the results it is clear that the SVESM performs better.</p><h2 id=noisy-training-and-noiseless-testing>Noisy training and noiseless testing<a hidden class=anchor aria-hidden=true href=#noisy-training-and-noiseless-testing>#</a></h2><p>Adding normally distributed white noise to the training input dataset we obtain the following results for the nmrse:</p><pre tabindex=0><code>Training on noisy data and testing on noiseless data...
SVESM nmrse for noisy training and noiseless testing: 0.415
SVM Poly kernel nmrse for noisy training and noiseless testing: 0.643
SVM RadialBasis kernel nmrse for noisy training and noiseless testing: 0.557
</code></pre><p>The plots are</p><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/84598112-918d2680-ae68-11ea-925c-b507e8d3ad12.png alt=svesm_noisytrain_comparison></p><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/84598119-97830780-ae68-11ea-96a6-dbbf3e4b9f16.png alt=svm_noisytrain_comparison></p><p>Also in this case the performance of the SVESM is superior to SVR.</p><h2 id=noisy-training-and-testing>Noisy training and testing<a hidden class=anchor aria-hidden=true href=#noisy-training-and-testing>#</a></h2><p>For the last comparison the results are as follows</p><pre tabindex=0><code>Training and testing on noisy data...
SVESM nmrse for noisy training and testing: 0.439
SVM Poly kernel nmrse for noisy training and testing: 0.724
SVM RadialBasis kernel nmrse for noisy training and testing: 0.648
</code></pre><p>And in this case as well the proposed model outperforms SVR.</p><p>As always, if you have any questions regarding the model, the package or you have found errors in my post, please don&rsquo;t hesitate to contact me!</p><h2 id=citing>Citing<a hidden class=anchor aria-hidden=true href=#citing>#</a></h2><p>Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bibtex data-lang=bibtex><span style=display:flex><span><span style=color:#a6e22e>@article</span>{JMLR:v23:22-0611,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>author</span>  = <span style=color:#e6db74>{Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>title</span>   = <span style=color:#e6db74>{ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>journal</span> = <span style=color:#e6db74>{Journal of Machine Learning Research}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>year</span>    = <span style=color:#e6db74>{2022}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>volume</span>  = <span style=color:#e6db74>{23}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>number</span>  = <span style=color:#e6db74>{288}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>pages</span>   = <span style=color:#e6db74>{1--8}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>url</span>     = <span style=color:#e6db74>{http://jmlr.org/papers/v23/22-0611.html}</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1]
Drucker, Harris, et al. &ldquo;Support vector regression machines.&rdquo; Advances in neural information processing systems. 1997.</p><p>[2]
Smola, Alex J., and Bernhard Schölkopf. &ldquo;A tutorial on support vector regression.&rdquo; Statistics and computing 14.3 (2004): 199-222.</p><p>[3]
VladimirN. Vapnik, The Nature of Statistical Learning Theory, Springer, 1995.</p><p>[4]
Shi, Zhiwei, and Min Han. &ldquo;Support vector echo-state machine for chaotic time-series prediction.&rdquo; IEEE transactions on neural networks 18.2 (2007): 359-372.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://martinuzzifrancesco.github.io/>Francesco Martinuzzi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>