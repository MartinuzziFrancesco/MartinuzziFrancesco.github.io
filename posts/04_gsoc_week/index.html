<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GSoC week 4: SVD-based Reservoir | Francesco Martinuzzi</title><meta name=keywords content><meta name=description content="The standard construction of the reservoir matrix \( \textbf{W} \) for Echo State Networks (ESN) is based on initializing \( \textbf{W} \) using specific schemes, usually generating random numbers and then rescaling it to make sure that the spectral radius is less or equal to a chosen number. This procedure is effective, but in literature other ways are explored. In this week we implemented a Singular Value Decomposition (SVD)-based algorithm, described in [1], that is capable of obtaining a sparse matrix suitable for the ESNs."><meta name=author content="Francesco Martinuzzi"><link rel=canonical href=https://martinuzzifrancesco.github.io/posts/04_gsoc_week/><link crossorigin=anonymous href=/assets/css/stylesheet.849fd8bd636f9bdcc8fd3087509c431a61faae78bd735e7ba75e6fd13ec64f83.css integrity="sha256-hJ/YvWNvm9zI/TCHUJxDGmH6rni9c157p15v0T7GT4M=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://martinuzzifrancesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://martinuzzifrancesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://martinuzzifrancesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://martinuzzifrancesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://martinuzzifrancesco.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="GSoC week 4: SVD-based Reservoir"><meta property="og:description" content="The standard construction of the reservoir matrix \( \textbf{W} \) for Echo State Networks (ESN) is based on initializing \( \textbf{W} \) using specific schemes, usually generating random numbers and then rescaling it to make sure that the spectral radius is less or equal to a chosen number. This procedure is effective, but in literature other ways are explored. In this week we implemented a Singular Value Decomposition (SVD)-based algorithm, described in [1], that is capable of obtaining a sparse matrix suitable for the ESNs."><meta property="og:type" content="article"><meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/04_gsoc_week/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-06-28T21:02:54+02:00"><meta property="article:modified_time" content="2020-06-28T21:02:54+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GSoC week 4: SVD-based Reservoir"><meta name=twitter:description content="The standard construction of the reservoir matrix \( \textbf{W} \) for Echo State Networks (ESN) is based on initializing \( \textbf{W} \) using specific schemes, usually generating random numbers and then rescaling it to make sure that the spectral radius is less or equal to a chosen number. This procedure is effective, but in literature other ways are explored. In this week we implemented a Singular Value Decomposition (SVD)-based algorithm, described in [1], that is capable of obtaining a sparse matrix suitable for the ESNs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://martinuzzifrancesco.github.io/posts/"},{"@type":"ListItem","position":2,"name":"GSoC week 4: SVD-based Reservoir","item":"https://martinuzzifrancesco.github.io/posts/04_gsoc_week/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GSoC week 4: SVD-based Reservoir","name":"GSoC week 4: SVD-based Reservoir","description":"The standard construction of the reservoir matrix \\( \\textbf{W} \\) for Echo State Networks (ESN) is based on initializing \\( \\textbf{W} \\) using specific schemes, usually generating random numbers and then rescaling it to make sure that the spectral radius is less or equal to a chosen number. This procedure is effective, but in literature other ways are explored. In this week we implemented a Singular Value Decomposition (SVD)-based algorithm, described in [1], that is capable of obtaining a sparse matrix suitable for the ESNs.","keywords":[],"articleBody":"The standard construction of the reservoir matrix \\( \\textbf{W} \\) for Echo State Networks (ESN) is based on initializing \\( \\textbf{W} \\) using specific schemes, usually generating random numbers and then rescaling it to make sure that the spectral radius is less or equal to a chosen number. This procedure is effective, but in literature other ways are explored. In this week we implemented a Singular Value Decomposition (SVD)-based algorithm, described in [1], that is capable of obtaining a sparse matrix suitable for the ESNs. In this post we are going to describe the theory behind the implementation and then a couple of examples will be given.\nSingular Value Decomposition reservoir construction One of the key aspects the an ESN is for its reservoir to posses the Echo State Property (ESP) [2]. A sufficient condition to obtain it is to construct the reservoir with a spectral radius less than 1. While using the architecture explained in the opening ensures this condition, it doesn’t take into account the singular values information of \\( \\textbf{W} \\), and it doesn’t allow much control over the construction of said matrix. An alternative could be to leverage the SVD to build a reservoir matrix given the largest singular value. To fully comprehend this procedure firstly we have to illustrate what SVD consists of.\nLet us consider the reservoir matrix \\( \\textbf{W} \\in \\mathbb{R}^{N \\times N}\\); this matrix can be expressed as \\( \\textbf{W} = \\textbf{U}\\textbf{S}\\textbf{V} \\) where \\( \\textbf{U}, \\textbf{V} \\in \\mathbb{R}^{N \\times N}\\) are orthogonal matrices and \\( \\textbf{S}=\\text{diag}(\\sigma _1, …, \\sigma _N) \\) is a diagonal matrix whose entries are ordered in increasing order. The values \\( \\sigma _i \\) are called the singular values of \\( \\textbf{W} \\). Given any diagonal matrix \\( \\textbf{S} \\), and orthogonal matrices \\( \\textbf{U}, \\textbf{V} \\) the matrix \\( \\textbf{W} \\) obtained as \\( \\textbf{W} = \\textbf{U}\\textbf{S}\\textbf{V} \\) has the same singular values as \\( \\textbf{S} \\).\nThis method provides an effective way of ensuring the ESP without the scaling of the reservoir weights. Instead of using orthogonal matrices \\( \\textbf{U}, \\textbf{V} \\), that could produce a dense matrix \\( \\textbf{W} \\), the authors opted for a two dimensional rotation matrix \\( \\textbf{Q}(i, j, \\theta) \\in \\mathbb{R}^{N \\times N}\\) with \\( \\textbf{Q}_{i,i} = \\textbf{Q}_{j,j} = \\text{cos}(\\theta)\\), \\( \\textbf{Q}_{i,j} = -\\text{sin}(\\theta)) \\), \\( \\textbf{Q}_{j,i} = \\text{sin}(\\theta)) \\) with \\( i, j \\) random values in [1, N] and \\( \\theta \\) random value in [-1, 1]. The algorithm proposed is as follows:\n Choose a predefined \\( \\sigma _N \\) in the range [0, 1] and generate \\( \\sigma _i, i=1,…, N-1 \\) in the range (0, \\( \\sigma _N \\)]. This values are used to create a diagonal matrix \\( \\textbf{S}=\\text{diag}(\\sigma _1, …, \\sigma _N) \\). With \\( h=1 \\) let \\( \\textbf{W}_1 = \\textbf{S} \\). For \\( h = h + 1 \\) randomly choose the two dimensional matrix \\( \\textbf{Q}(i, j, \\theta) \\) as defined above. \\( \\textbf{W} _h = \\textbf{W} _{h-1} \\textbf{Q}(i, j, \\theta)\\) gives the matrix \\( \\textbf{W} \\) for the step \\( h \\). This procedure is repeated until the chosen density is reached.  Implementation in ReservoirComputing.jl The implementation into code is extremely straightforwad: following the instructions in the paper a function pseudoSVD is created which takes as input the following\n dim: the desired dimension of the reservoir max_value: the value of the largest of the singular values sparsity: the sparsity for the reservoir sorted: optional value. If = true (default) the singular values in the diagonal matrix will be sorted. reverse_sort: optional value if sort = true. If = true (default = false) the singular values in the diagonal matrix will be sorted in a decreasing order.  Examples Original ESN Testing the SVD construction on the original ESN we can try to reproduce the Lorenz attractor, with similar parameters as given in the Introduction to Reservoir Computing\napprox_res_size = 300 radius = 1.2 sparsity = 0.1 max_value = 1.2 activation = tanh sigma = 0.1 beta = 0.0 alpha = 1.0 nla_type = NLADefault() extended_states = false The values of the largest singular value for the construction of the SVD based reservoir is equal to the spectral radius of the standard reservoir, that in this case is greater than one. A plot of the results shows:\nThis construction is capable of reproducing the Lorenz system in the short term, and behaves better in the long term than the standard implementation, or at least in this example it does. A more in depth analysis is needed for the consistency of the results and the behavior of the SVD reservoir when the largest singular value is set greater than one and when one of the non linear algorithms is applied.\nRidge ESN, SVESM and ESGP In order to test this implementation for others ESN architectures currently implemented in ReservoirComputing.jl we choose to use the same examples as last week, based on the Mackey-Glass system:\n$$\\frac{dx}{dt} = \\beta x(t)+\\frac{\\alpha x(t-\\delta)}{1+x(t-\\delta)^2}$$\nwith the same values:\n \\(\\beta = -0.1 \\) \\(\\alpha = 0.2 \\) \\(\\delta = 17 \\) \\( dt = 0.1 \\)  Furthermore the time series is rescaled in the range \\( [-1, 1] \\) by application of a tangent hyperbolic transform \\( y_{ESN}(\\text{t}) = \\text{tanh}(\\text{y}(t)-1) \\). To evaluate the precision of our results we are going to use root mean square deviation (rmse), defined as:\n$$\\textbf{rmse} = \\sqrt{\\frac{\\sum_{i=1}^{T_n}(y_d(i)-y(i))^2}{T_n}}$$\nwhere\n \\(y_d(i) \\) is the target value \\(y(i) \\) is the predicted value \\(T_d \\) is the number of test examples  The ESN parameters are as follows\nconst shift = 100 const train_len = 6000 const test_len =1500  const approx_res_size = 400 const sparsity = 0.1 const activation = tanh const radius = 0.99 const max_value = 0.99 const sigma = 0.1  const alpha = 0.2 const nla_type = NLADefault() const extended_states = true The largest singular value was set equal to the spectral radius for the standard construction. Averaging on ten runs the results are as follows:\nrmse ESGP: Classic reservoir: 0.077 SVD reservoir: 0.205 rmse ridge ESN: Classic reservoir: 0.143 SVD reservoir: 0.146 rmse SVESM: Classic reservoir: 0.232 SVD reservoir: 0.245 For the ESGP this procedure yields far worst performances than the standard counterpart. For the ridge ESN and SVESM the results are almost identical.\nThe results obtained are interesting and for sure more testing is needed. Some sperimentation on the h steps ahead prediction could be done, as well as giving different values for the spectral radius and largest singular value, since in all the examples examined the spectral radius was chosen following the literature, and hence could be more optimized that the largest values that we used.\nCiting Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:\n@article{JMLR:v23:22-0611,  author = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora},  title = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models},  journal = {Journal of Machine Learning Research},  year = {2022},  volume = {23},  number = {288},  pages = {1--8},  url = {http://jmlr.org/papers/v23/22-0611.html} } Documentation [1]Yang, Cuili, et al. “Design of polynomial echo state networks for time series prediction.” Neurocomputing 290 (2018): 148-160.\n[2]Jaeger, Herbert. “The “echo state” approach to analysing and training recurrent neural networks-with an erratum note.” Bonn, Germany: German National Research Center for Information Technology GMD Technical Report 148.34 (2001): 13.\n","wordCount":"1236","inLanguage":"en","datePublished":"2020-06-28T21:02:54+02:00","dateModified":"2020-06-28T21:02:54+02:00","author":{"@type":"Person","name":"Francesco Martinuzzi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://martinuzzifrancesco.github.io/posts/04_gsoc_week/"},"publisher":{"@type":"Organization","name":"Francesco Martinuzzi","logo":{"@type":"ImageObject","url":"https://martinuzzifrancesco.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://martinuzzifrancesco.github.io/ accesskey=h title="Francesco Martinuzzi (Alt + H)">Francesco Martinuzzi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://martinuzzifrancesco.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://martinuzzifrancesco.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://martinuzzifrancesco.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>GSoC week 4: SVD-based Reservoir</h1><div class=post-meta><span title='2020-06-28 21:02:54 +0200 CEST'>June 28, 2020</span>&nbsp;·&nbsp;Francesco Martinuzzi</div></header><div class=post-content><p>The standard construction of the reservoir matrix \( \textbf{W} \) for Echo State Networks (ESN) is based on initializing \( \textbf{W} \) using specific schemes, usually generating random numbers and then rescaling it to make sure that the spectral radius is less or equal to a chosen number. This procedure is effective, but in literature other ways are explored. In this week we implemented a Singular Value Decomposition (SVD)-based algorithm, described in <a href=#1>[1]</a>, that is capable of obtaining a sparse matrix suitable for the ESNs. In this post we are going to describe the theory behind the implementation and then a couple of examples will be given.</p><h1 id=singular-value-decomposition-reservoir-construction>Singular Value Decomposition reservoir construction<a hidden class=anchor aria-hidden=true href=#singular-value-decomposition-reservoir-construction>#</a></h1><p>One of the key aspects the an ESN is for its reservoir to posses the Echo State Property (ESP) <a href=#2>[2]</a>. A sufficient condition to obtain it is to construct the reservoir with a spectral radius less than 1. While using the architecture explained in the opening ensures this condition, it doesn&rsquo;t take into account the singular values information of \( \textbf{W} \), and it doesn&rsquo;t allow much control over the construction of said matrix. An alternative could be to leverage the SVD to build a reservoir matrix given the largest singular value. To fully comprehend this procedure firstly we have to illustrate what SVD consists of.</p><p>Let us consider the reservoir matrix \( \textbf{W} \in \mathbb{R}^{N \times N}\); this matrix can be expressed as \( \textbf{W} = \textbf{U}\textbf{S}\textbf{V} \) where \( \textbf{U}, \textbf{V} \in \mathbb{R}^{N \times N}\) are orthogonal matrices and \( \textbf{S}=\text{diag}(\sigma _1, &mldr;, \sigma _N) \) is a diagonal matrix whose entries are ordered in increasing order. The values \( \sigma _i \) are called the singular values of \( \textbf{W} \). Given any diagonal matrix \( \textbf{S} \), and orthogonal matrices \( \textbf{U}, \textbf{V} \) the matrix \( \textbf{W} \) obtained as \( \textbf{W} = \textbf{U}\textbf{S}\textbf{V} \) has the same singular values as \( \textbf{S} \).</p><p>This method provides an effective way of ensuring the ESP without the scaling of the reservoir weights. Instead of using orthogonal matrices \( \textbf{U}, \textbf{V} \), that could produce a dense matrix \( \textbf{W} \), the authors opted for a two dimensional rotation matrix \( \textbf{Q}(i, j, \theta) \in \mathbb{R}^{N \times N}\) with \( \textbf{Q}_{i,i} = \textbf{Q}_{j,j} = \text{cos}(\theta)\), \( \textbf{Q}_{i,j} = -\text{sin}(\theta)) \), \( \textbf{Q}_{j,i} = \text{sin}(\theta)) \) with \( i, j \) random values in [1, N] and \( \theta \) random value in [-1, 1]. The algorithm proposed is as follows:</p><ul><li>Choose a predefined \( \sigma _N \) in the range [0, 1] and generate \( \sigma _i, i=1,&mldr;, N-1 \) in the range (0, \( \sigma _N \)]. This values are used to create a diagonal matrix \( \textbf{S}=\text{diag}(\sigma _1, &mldr;, \sigma _N) \). With \( h=1 \) let \( \textbf{W}_1 = \textbf{S} \).</li><li>For \( h = h + 1 \) randomly choose the two dimensional matrix \( \textbf{Q}(i, j, \theta) \) as defined above. \( \textbf{W} _h = \textbf{W} _{h-1} \textbf{Q}(i, j, \theta)\) gives the matrix \( \textbf{W} \) for the step \( h \). This procedure is repeated until the chosen density is reached.</li></ul><h1 id=implementation-in-reservoircomputingjl>Implementation in ReservoirComputing.jl<a hidden class=anchor aria-hidden=true href=#implementation-in-reservoircomputingjl>#</a></h1><p>The implementation into code is extremely straightforwad: following the instructions in the paper a function <code>pseudoSVD</code> is created which takes as input the following</p><ul><li>dim: the desired dimension of the reservoir</li><li>max_value: the value of the largest of the singular values</li><li>sparsity: the sparsity for the reservoir</li><li>sorted: optional value. If = true (default) the singular values in the diagonal matrix will be sorted.</li><li>reverse_sort: optional value if sort = true. If = true (default = false) the singular values in the diagonal matrix will be sorted in a decreasing order.</li></ul><h1 id=examples>Examples<a hidden class=anchor aria-hidden=true href=#examples>#</a></h1><h2 id=original-esn>Original ESN<a hidden class=anchor aria-hidden=true href=#original-esn>#</a></h2><p>Testing the SVD construction on the original ESN we can try to reproduce the Lorenz attractor, with similar parameters as given in the <a href=https://martinuzzifrancesco.github.io/posts/a-brief-introduction-to-reservoir-computing/>Introduction to Reservoir Computing</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>sparsity <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>max_value <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLADefault()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span></code></pre></div><p>The values of the largest singular value for the construction of the SVD based reservoir is equal to the spectral radius of the standard reservoir, that in this case is greater than one. A plot of the results shows:</p><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/85957829-fd0ae400-b990-11ea-9339-08416158cdf9.png alt=lorenz_coord></p><p>This construction is capable of reproducing the Lorenz system in the short term, and behaves better in the long term than the standard implementation, or at least in this example it does. A more in depth analysis is needed for the consistency of the results and the behavior of the SVD reservoir when the largest singular value is set greater than one and when one of the non linear algorithms is applied.</p><h2 id=ridge-esn-svesm-and-esgp>Ridge ESN, SVESM and ESGP<a hidden class=anchor aria-hidden=true href=#ridge-esn-svesm-and-esgp>#</a></h2><p>In order to test this implementation for others ESN architectures currently implemented in ReservoirComputing.jl we choose to use the same examples as last <a href=https://martinuzzifrancesco.github.io/posts/03_gsoc_week/>week</a>, based on the Mackey-Glass system:</p><p>$$\frac{dx}{dt} = \beta x(t)+\frac{\alpha x(t-\delta)}{1+x(t-\delta)^2}$$</p><p>with the same values:</p><ul><li>\(\beta = -0.1 \)</li><li>\(\alpha = 0.2 \)</li><li>\(\delta = 17 \)</li><li>\( dt = 0.1 \)</li></ul><p>Furthermore the time series is rescaled in the range \( [-1, 1] \) by application of a tangent hyperbolic transform \( y_{ESN}(\text{t}) = \text{tanh}(\text{y}(t)-1) \). To evaluate the precision of our results we are going to use root mean square deviation (rmse), defined as:</p><p>$$\textbf{rmse} = \sqrt{\frac{\sum_{i=1}^{T_n}(y_d(i)-y(i))^2}{T_n}}$$</p><p>where</p><ul><li>\(y_d(i) \) is the target value</li><li>\(y(i) \) is the predicted value</li><li>\(T_d \) is the number of test examples</li></ul><p>The ESN parameters are as follows</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>const</span> shift <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> train_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>6000</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> test_len <span style=color:#f92672>=</span><span style=color:#ae81ff>1500</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>400</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> sparsity <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.99</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> max_value <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.99</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.2</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> nla_type <span style=color:#f92672>=</span> NLADefault()
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> extended_states <span style=color:#f92672>=</span> true
</span></span></code></pre></div><p>The largest singular value was set equal to the spectral radius for the standard construction. Averaging on ten runs the results are as follows:</p><pre tabindex=0><code>rmse ESGP:
Classic reservoir: 0.077
SVD reservoir: 0.205
rmse ridge ESN:
Classic reservoir: 0.143
SVD reservoir: 0.146
rmse SVESM:
Classic reservoir: 0.232
SVD reservoir: 0.245
</code></pre><p>For the ESGP this procedure yields far worst performances than the standard counterpart. For the ridge ESN and SVESM the results are almost identical.</p><p>The results obtained are interesting and for sure more testing is needed. Some sperimentation on the h steps ahead prediction could be done, as well as giving different values for the spectral radius and largest singular value, since in all the examples examined the spectral radius was chosen following the literature, and hence could be more optimized that the largest values that we used.</p><h2 id=citing>Citing<a hidden class=anchor aria-hidden=true href=#citing>#</a></h2><p>Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bibtex data-lang=bibtex><span style=display:flex><span><span style=color:#a6e22e>@article</span>{JMLR:v23:22-0611,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>author</span>  = <span style=color:#e6db74>{Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>title</span>   = <span style=color:#e6db74>{ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>journal</span> = <span style=color:#e6db74>{Journal of Machine Learning Research}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>year</span>    = <span style=color:#e6db74>{2022}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>volume</span>  = <span style=color:#e6db74>{23}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>number</span>  = <span style=color:#e6db74>{288}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>pages</span>   = <span style=color:#e6db74>{1--8}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>url</span>     = <span style=color:#e6db74>{http://jmlr.org/papers/v23/22-0611.html}</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=documentation>Documentation<a hidden class=anchor aria-hidden=true href=#documentation>#</a></h2><p>[1]
Yang, Cuili, et al. &ldquo;Design of polynomial echo state networks for time series prediction.&rdquo; Neurocomputing 290 (2018): 148-160.</p><p>[2]
Jaeger, Herbert. &ldquo;The “echo state” approach to analysing and training recurrent neural networks-with an erratum note.&rdquo; Bonn, Germany: German National Research Center for Information Technology GMD Technical Report 148.34 (2001): 13.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://martinuzzifrancesco.github.io/>Francesco Martinuzzi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>