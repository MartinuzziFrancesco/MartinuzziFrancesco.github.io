<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GSoC week 7: Reservoir Computing with Cellular Automata Part 1 | Francesco Martinuzzi</title><meta name=keywords content><meta name=description content="In the past few years a new framework based on the concept of Reservoir Computing has been proposed: the Cellular Automata based Reservoir Computer (ReCA) [1]. The advantage it proposes over standard implementations is given by the binary state of the reservoir and the fact that it doesn&rsquo;t require much parameter tuning to obtain state of the art results. Since the initial conception of the use of ECA for reservoir computing numerous improvement have taken place."><meta name=author content="Francesco Martinuzzi"><link rel=canonical href=https://martinuzzifrancesco.github.io/posts/07_gsoc_week/><link crossorigin=anonymous href=/assets/css/stylesheet.849fd8bd636f9bdcc8fd3087509c431a61faae78bd735e7ba75e6fd13ec64f83.css integrity="sha256-hJ/YvWNvm9zI/TCHUJxDGmH6rni9c157p15v0T7GT4M=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://martinuzzifrancesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://martinuzzifrancesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://martinuzzifrancesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://martinuzzifrancesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://martinuzzifrancesco.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="GSoC week 7: Reservoir Computing with Cellular Automata Part 1"><meta property="og:description" content="In the past few years a new framework based on the concept of Reservoir Computing has been proposed: the Cellular Automata based Reservoir Computer (ReCA) [1]. The advantage it proposes over standard implementations is given by the binary state of the reservoir and the fact that it doesn&rsquo;t require much parameter tuning to obtain state of the art results. Since the initial conception of the use of ECA for reservoir computing numerous improvement have taken place."><meta property="og:type" content="article"><meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/07_gsoc_week/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-07-19T17:27:05+02:00"><meta property="article:modified_time" content="2020-07-19T17:27:05+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GSoC week 7: Reservoir Computing with Cellular Automata Part 1"><meta name=twitter:description content="In the past few years a new framework based on the concept of Reservoir Computing has been proposed: the Cellular Automata based Reservoir Computer (ReCA) [1]. The advantage it proposes over standard implementations is given by the binary state of the reservoir and the fact that it doesn&rsquo;t require much parameter tuning to obtain state of the art results. Since the initial conception of the use of ECA for reservoir computing numerous improvement have taken place."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://martinuzzifrancesco.github.io/posts/"},{"@type":"ListItem","position":2,"name":"GSoC week 7: Reservoir Computing with Cellular Automata Part 1","item":"https://martinuzzifrancesco.github.io/posts/07_gsoc_week/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GSoC week 7: Reservoir Computing with Cellular Automata Part 1","name":"GSoC week 7: Reservoir Computing with Cellular Automata Part 1","description":"In the past few years a new framework based on the concept of Reservoir Computing has been proposed: the Cellular Automata based Reservoir Computer (ReCA) [1]. The advantage it proposes over standard implementations is given by the binary state of the reservoir and the fact that it doesn\u0026rsquo;t require much parameter tuning to obtain state of the art results. Since the initial conception of the use of ECA for reservoir computing numerous improvement have taken place.","keywords":[],"articleBody":"In the past few years a new framework based on the concept of Reservoir Computing has been proposed: the Cellular Automata based Reservoir Computer (ReCA) [1]. The advantage it proposes over standard implementations is given by the binary state of the reservoir and the fact that it doesn’t require much parameter tuning to obtain state of the art results. Since the initial conception of the use of ECA for reservoir computing numerous improvement have taken place. A recurrent design, together with the ReCA denomination, has been proposed in [2], and new methods for states encoding are studied in [3]. Also the use of two reservoir is studied in [4], as well as the implementation of two different rules, staked both horizontally [5] and vertically [6]. Lastly an exploration of complex rules is done in [7]. In this post we will illustrate the implementation in ReservoirComputing.jl of the general model, based on the architecture illustrated in [4] which build over the original implementation, improving the results. As always we will give an initial theoretical introduction, and then some examples of applications will be shown.\nReservoir Computing with Elementary Cellular Automata Elementary Cellular Automata Initially introduced by Van Neumann as self-reproducing machines [8] Cellular Automata (CA) is a dynamical computational model based on a regular grid, of arbitrary dimensions, composed by cells. These cells can be in a different number of states and are updated according to a specific rule \\( f \\) which takes as an input the cell itself and its neighborhood and gives as output the state of the cell in the next generation. All the cells are updated simultaneously making the CA a discrete system with respect to time. The rule space is determined by the number of states and the number of possible neighbors. Let \\( K \\) be the number of states and \\( S \\) the the number of neighbors including the cell itself, then the possible number of neighborhood sates is given by \\( K^S \\). Since each element is transitioning to one of \\( K \\) states itself the transition function space is \\( K^{K^S} \\) [9]. Elementary cellular automata (ECA) are defined by a one dimensional grid of cells that are in one of two states, usually represented by 0 and 1. Each cell \\( x \\) updates its state \\( x_i^t \\) depending on the states of its two neighbors \\( x_{i-1}^t \\) and \\( x_{i+1}^t \\) according to the transition function \\( f:{0,1}^3 \\rightarrow {0,1} \\). There are \\( 2^8=256 \\) elementary rules [10] that can be identified by numbers ranging from 0 to 255 taking the output table of each function as binary encoding of a digital number [11]. An example of rule 30 can be observed below.\nThanks to symmetries this rules con be grouped into 88 classes with equivalent characteristics [12]. Another distinction can be made, grouping the ECAs according to the general behavior they display. The first step in this direction was done by Wolfram [13], that identified four classes with the following description:\nClass 1: CA states evolve to a homogeneous behavior Class 2: CA states evolve periodically Class 3: CA states evolve with no defined pattern Class 4: can show all evolution patterns in an unpredictable manner A more refined analysis by Li and Packard divided the Class 2 into two different sub-classes, distinguishing between fixed point and periodic CA. Class 3 rules are defined as globally chaotic and class 4 are considered difficult to include in specific categories.\nReCA Architecture In the first stage the input needs to be mapped into the CA system. In the literature the ReCA approach has only been tested with binary test sets, so the chosen procedure for the input data is to translate directly the input onto the first state of the CA. In the original design [1] this was done by a random permutation of the elements of the input vector in a vector of the same dimension, $\\text{L}_{\\text{in}}$. The reservoir was then composed of \\( \\text{R} \\) different ECA systems, each of which had a different random mapping as encoder. The evolution was done using the combination of the \\( \\text{R} \\) reservoirs, so that the information could flow between one and the other. This approach yielded better results than letting them evolve separately. The starting vector for the ECA system is then the combination of the \\( \\text{R} \\) mappings of the starting input vector, making it of dimensions $\\text{R} \\cdot \\text{L}_{\\text{in}}$.\nAn improvement over the here discussed method, proposed in [4], is to map the input into a different sized vector $\\text{L}_{\\text{d}}$, with $\\text{L}_{\\text{d}} \u003e \\text{L}_{\\text{in}}$, padded with zeros. The higher dimension of the input vector allows the CA system to evolve with more freedom. Using a number of recombinations \\( \\text{R} \\) the input vector to the CA system will be of dimensions $\\text{R} \\cdot \\text{L}_{\\text{d}}$. At the boundaries of the CA are used periodic boundary conditions (PBC), so that the last cell is neighbor with the first one.\nLet $\\text{X}_1$ be the first input vector. This will be randomly mapped onto a vector of zeros \\( \\text{R} \\) times using a fixed mapping scheme $[\\text{P}_1, \\text{P}_2, …, \\text{P}_{\\text{R}}]$ and concatenated to form the initial configuration $\\text{A}_0$ for the CA:\n$$\\text{A}_0^{(1)} = [\\text{X}_{1}^{\\text{P}_{1}}, \\text{X}_{1}^{\\text{P}_{2}}, …, \\text{X}_{1}^{\\text{P}_{\\text{R}}}]$$\nThe transition function Z is then applied for I generations:\n$$\\text{A}_{1}^{(1)} = \\text{Z}(\\text{A}_0^{(1)})$$\n$$\\text{A}_{2}^{(1)} = \\text{Z}(\\text{A}_{1}^{(1)})$$\n$$\\vdots$$\n$$\\text{A}_{\\text{I}}^{(1)} = \\text{Z}(\\text{A}_{\\text{I}-1}^{(1)})$$\nThis constitutes the evolution of the CA given the input $\\text{X}_1$. In the standard ReCA approach the state vector is the concatenation of all the steps $\\text{A}_{1}^{(1)}$ through $\\text{A}_{\\text{I}}^{(1)}$ to form $\\text{A}^{(1)} = [\\text{A}_{1}^{(1)}, \\text{A}_{2}^{(1)}, …, \\text{A}_{\\text{I}}^{(1)}]$.\nThe final states matrix, of dimensions $\\text{R} \\cdot \\text{L}_{\\text{d}} \\times \\text{T}$, is obtained stacking the state vectors column wise, in order to obtain: $\\textbf{X}=[\\text{A}^{(1) \\text{T}}, \\text{A}^{(2) \\text{T}}, …, \\text{A}^{(\\text{T}) \\text{T}}]$.\nFor the training technically every method we have implemented could be used, but in this first trial we just used the Ridge Regression. In the original paper the use of the pseudo-inverse was opted.\nImplementation in ReservoirComputing.jl Following the procedure described above we implemented in ReservoirComputing.jl a RECA_discrete object and a RECAdirect_predict_discrete function. The goal was to reproduce the results found in the literature, so the discrete approach was the only way to ensure that our implementation is correct. One of the goals is to expand this architecture to be also able to predict continuous values, such as timeseries. In this week an effort in this direction was made, but further exploration is needed. The RECA_discrete constructor takes as input\ntrain_data the data needed for the ReCA training rule the ECA rule for the reservoir generations the number of generations the ECA will expand in expansion_size the \\( L_d \\) parameter permutations the number of additional ECA for the reservoir training nla_type the non linear algorithm for the reservoir states. Default is NLADefalut() The training is done using the already implemented ESNtrain, that will probably need a name change in the future since now it can train another family of Reservoir Computing models. The RECAdirect_predict_discrete function takes as input\nreca an already constructed RECA_discrete W_out the output of ESNtrain test_data the input data for the direct prediction Additionally a ECA constructor is also added to the package, taking as input the chosen rule, a vector of starting values starting_val and the number of generations for the ECA.\nExamples For testing the ReCA implementation we chose to solve the 5 bit memory task, a problem introduced in [14], a test proved to be hard for both Recurrent Neural Networks (RNN) and Echo State Networks (ESN), and fairly diffused in the ReCA literature.\nThe test consists of four binary inputs and four binary outputs. In the first five timesteps of one run of the input sequence the first channel is one of the 32 possible five digit binary numbers, and the second input is complementary to the values in the first input (0, when the first channel is 1 and viceversa). The other two channels are zeros. This is the message that the model will have to remember. This is follow by a distractor period of $\\text{T}_0$ steps, in which all the channels are zero with the exception of the third one, which is one up until $\\text{T}_0-1$, where the fourth channel will be one and the third zero. This represents the cue. After that all channels except the third are zero.\nFor the output signal, all the channel are zero, but the third one which is one for all the steps with the exception of the last five, where the message from the input is repeated. A task is successful when the system is capable of reproducing all the $32 \\times (5+\\text{T}_0) \\times 4$ bits of the output.\nBelow we can see an illustration [3] of the data contained in the 5 bit memory task:\nUsing a distractor period of \\( \\text{T}_0 = 200 \\) and a value of \\( \\text{L}_d = 40 \\) we tried to reproduce the results in the literature. In the table below are shown the successful run out of 100 performed, and the values in square indicates the number of generations and permutations, and are chosen in accordance to the values presented in the papers analized.\nThe lines of code needed for the training and prediction of the 5 bit memory task with the ReCA are the following:\nreca = RECA_discrete(input, 60, 8, 40, 8) W_out = ESNtrain(reca, 0.01, train_data = convert(AbstractArray{Float64}, output)) result = RECAdirect_predict_discrete(reca, W_out, input) Where input and output are the datasets explained above, and the parameters to change for the results are rule, generations and permutations, in this example set to 60, 8, 8. Doing a cylce over each of them, for 100 runs we obtain the results below:\nThe values are in line with the results found in the literature, with little differences that could be attributed mainly to the training method. As already noted in the original paper, the computational power increases with the increasing of values of generations and permutations. It seems though that more generations is preferable over more permutations, since the (8, 16) correct runs are consistently less than the (16, 8) ones.\nThis model is really interesting, since it shows the capabilities of the Reservoir Computing approach. This family of models is still in its infancy, and a method for prediction of a continuous dataset is still missing. We hope that the implementation given in this package could help move the research in this direction.\nAs always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!\nCiting Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:\n@article{JMLR:v23:22-0611, author = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}, title = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}, journal = {Journal of Machine Learning Research}, year = {2022}, volume = {23}, number = {288}, pages = {1--8}, url = {http://jmlr.org/papers/v23/22-0611.html} } Documentation [1] Yilmaz, Ozgur. “Reservoir computing using cellular automata.” arXiv preprint arXiv:1410.0162 (2014).\n[2] Margem, Mrwan, and Ozgür Yilmaz. “An experimental study on cellular automata reservoir in pathological sequence learning tasks.” (2017).\n[3] Margem, Mrwan, and Osman S. Gedik. “Feed-forward versus recurrent architecture and local versus cellular automata distributed representation in reservoir computing for sequence memory learning.” Artificial Intelligence Review (2020): 1-30.\n[4] Nichele, Stefano, and Andreas Molund. “Deep reservoir computing using cellular automata.” arXiv preprint arXiv:1703.02806 (2017).\n[5] Nichele, Stefano, and Magnus S. Gundersen. “Reservoir computing using non-uniform binary cellular automata.” arXiv preprint arXiv:1702.03812 (2017).\n[6] McDonald, Nathan. “Reservoir computing \u0026 extreme learning machines using pairs of cellular automata rules.” 2017 International Joint Conference on Neural Networks (IJCNN). IEEE, 2017.\n[7] Babson, Neil, and Christof Teuscher. “Reservoir Computing with Complex Cellular Automata.” Complex Systems 28.4 (2019).\n[8] Neumann, János, and Arthur W. Burks. Theory of self-reproducing automata. Vol. 1102024. Urbana: University of Illinois press, 1966.\n[9] Bia_ynicki-Birula, Iwo, and Iwo Bialynicki-Birula. Modeling Reality: How computers mirror life. Vol. 1. Oxford University Press on Demand, 2004.\n[10] Wolfram, Stephen. A new kind of science. Vol. 5. Champaign, IL: Wolfram media, 2002.\n[11] Adamatzky, Andrew, and Genaro J. Martinez. “On generative morphological diversity of elementary cellular automata.” Kybernetes (2010).\n[12] Wuensche, Andrew, Mike Lesser, and Michael J. Lesser. Global Dynamics of Cellular Automata: An Atlas of Basin of Attraction Fields of One-Dimensional Cellular Automata. Vol. 1. Andrew Wuensche, 1992.\n[13] Wolfram, Stephen. “Universality and complexity in cellular automata.” Physica D: Nonlinear Phenomena 10.1-2 (1984): 1-35.\n[14] Hochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” Neural computation 9.8 (1997): 1735-1780.\n","wordCount":"2125","inLanguage":"en","datePublished":"2020-07-19T17:27:05+02:00","dateModified":"2020-07-19T17:27:05+02:00","author":{"@type":"Person","name":"Francesco Martinuzzi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://martinuzzifrancesco.github.io/posts/07_gsoc_week/"},"publisher":{"@type":"Organization","name":"Francesco Martinuzzi","logo":{"@type":"ImageObject","url":"https://martinuzzifrancesco.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://martinuzzifrancesco.github.io/ accesskey=h title="Francesco Martinuzzi (Alt + H)">Francesco Martinuzzi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://martinuzzifrancesco.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://martinuzzifrancesco.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://martinuzzifrancesco.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>GSoC week 7: Reservoir Computing with Cellular Automata Part 1</h1><div class=post-meta><span title='2020-07-19 17:27:05 +0200 CEST'>July 19, 2020</span>&nbsp;·&nbsp;Francesco Martinuzzi</div></header><div class=post-content><p>In the past few years a new framework based on the concept of Reservoir Computing has been proposed: the Cellular Automata based Reservoir Computer (ReCA) <a href=#1>[1]</a>. The advantage it proposes over standard implementations is given by the binary state of the reservoir and the fact that it doesn&rsquo;t require much parameter tuning to obtain state of the art results. Since the initial conception of the use of ECA for reservoir computing numerous improvement have taken place. A recurrent design, together with the ReCA denomination, has been proposed in <a href=#2>[2]</a>, and new methods for states encoding are studied in <a href=#3>[3]</a>. Also the use of two reservoir is studied in <a href=#4>[4]</a>, as well as the implementation of two different rules, staked both horizontally <a href=#5>[5]</a> and vertically <a href=#6>[6]</a>. Lastly an exploration of complex rules is done in <a href=#7>[7]</a>. In this post we will illustrate the implementation in ReservoirComputing.jl of the general model, based on the architecture illustrated in <a href=#4>[4]</a> which build over the original implementation, improving the results. As always we will give an initial theoretical introduction, and then some examples of applications will be shown.</p><h1 id=reservoir-computing-with-elementary-cellular-automata>Reservoir Computing with Elementary Cellular Automata<a hidden class=anchor aria-hidden=true href=#reservoir-computing-with-elementary-cellular-automata>#</a></h1><h2 id=elementary-cellular-automata>Elementary Cellular Automata<a hidden class=anchor aria-hidden=true href=#elementary-cellular-automata>#</a></h2><p>Initially introduced by Van Neumann as self-reproducing machines <a href=#8>[8]</a> Cellular Automata (CA) is a dynamical computational model based on a regular grid, of arbitrary dimensions, composed by cells. These cells can be in a different number of states and are updated according to a specific rule \( f \) which takes as an input the cell itself and its neighborhood and gives as output the state of the cell in the next generation. All the cells are updated simultaneously making the CA a discrete system with respect to time. The rule space is determined by the number of states and the number of possible neighbors. Let \( K \) be the number of states and \( S \) the the number of neighbors including the cell itself, then the possible number of neighborhood sates is given by \( K^S \). Since each element is transitioning to one of \( K \) states itself the transition function space is \( K^{K^S} \) <a href=$9>[9]</a>. Elementary cellular automata (ECA) are defined by a one dimensional grid of cells that are in one of two states, usually represented by 0 and 1. Each cell \( x \) updates its state \( x_i^t \) depending on the states of its two neighbors \( x_{i-1}^t \) and \( x_{i+1}^t \) according to the transition function \( f:{0,1}^3 \rightarrow {0,1} \). There are \( 2^8=256 \) elementary rules <a href=#10>[10]</a> that can be identified by numbers ranging from 0 to 255 taking the output table of each function as binary encoding of a digital number <a href=#11>[11]</a>. An example of rule 30 can be observed below.</p><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/87879034-64441300-c9e8-11ea-916d-a92312cd4f57.png alt=rule30wolfram></p><p>Thanks to symmetries this rules con be grouped into 88 classes with equivalent characteristics <a href=#12>[12]</a>. Another distinction can be made, grouping the ECAs according to the general behavior they display. The first step in this direction was done by Wolfram <a href=#13>[13]</a>, that identified four classes with the following description:</p><ul><li>Class 1: CA states evolve to a homogeneous behavior</li><li>Class 2: CA states evolve periodically</li><li>Class 3: CA states evolve with no defined pattern</li><li>Class 4: can show all evolution patterns in an unpredictable manner</li></ul><p>A more refined analysis by Li and Packard divided the Class 2 into two different sub-classes, distinguishing between fixed point and periodic CA. Class 3 rules are defined as globally chaotic and class 4 are considered difficult to include in specific categories.</p><h2 id=reca-architecture>ReCA Architecture<a hidden class=anchor aria-hidden=true href=#reca-architecture>#</a></h2><p>In the first stage the input needs to be mapped into the CA system. In the literature the ReCA approach has only been tested with binary test sets, so the chosen procedure for the input data is to translate directly the input onto the first state of the CA. In the original design <a href=#1>[1]</a> this was done by a random permutation of the elements of the input vector in a vector of the same dimension, $\text{L}_{\text{in}}$. The reservoir was then composed of \( \text{R} \) different ECA systems, each of which had a different random mapping as encoder. The evolution was done using the combination of the \( \text{R} \) reservoirs, so that the information could flow between one and the other. This approach yielded better results than letting them evolve separately. The starting vector for the ECA system is then the combination of the \( \text{R} \) mappings of the starting input vector, making it of dimensions $\text{R} \cdot \text{L}_{\text{in}}$.</p><p>An improvement over the here discussed method, proposed in <a href=#4>[4]</a>, is to map the input into a different sized vector $\text{L}_{\text{d}}$, with $\text{L}_{\text{d}} > \text{L}_{\text{in}}$, padded with zeros. The higher dimension of the input vector allows the CA system to evolve with more freedom. Using a number of recombinations \( \text{R} \) the input vector to the CA system will be of dimensions $\text{R} \cdot \text{L}_{\text{d}}$. At the boundaries of the CA are used periodic boundary conditions (PBC), so that the last cell is neighbor with the first one.</p><p>Let $\text{X}_1$ be the first input vector. This will be randomly mapped onto a vector of zeros \( \text{R} \) times using a fixed mapping scheme $[\text{P}_1, \text{P}_2, &mldr;, \text{P}_{\text{R}}]$ and concatenated to form the initial configuration $\text{A}_0$ for the CA:</p><p>$$\text{A}_0^{(1)} = [\text{X}_{1}^{\text{P}_{1}}, \text{X}_{1}^{\text{P}_{2}}, &mldr;, \text{X}_{1}^{\text{P}_{\text{R}}}]$$</p><p>The transition function Z is then applied for I generations:</p><p>$$\text{A}_{1}^{(1)} = \text{Z}(\text{A}_0^{(1)})$$</p><p>$$\text{A}_{2}^{(1)} = \text{Z}(\text{A}_{1}^{(1)})$$</p><p>$$\vdots$$</p><p>$$\text{A}_{\text{I}}^{(1)} = \text{Z}(\text{A}_{\text{I}-1}^{(1)})$$</p><p>This constitutes the evolution of the CA given the input $\text{X}_1$. In the standard ReCA approach the state vector is the concatenation of all the steps $\text{A}_{1}^{(1)}$ through $\text{A}_{\text{I}}^{(1)}$ to form $\text{A}^{(1)} = [\text{A}_{1}^{(1)}, \text{A}_{2}^{(1)}, &mldr;, \text{A}_{\text{I}}^{(1)}]$.</p><p>The final states matrix, of dimensions $\text{R} \cdot \text{L}_{\text{d}} \times \text{T}$, is obtained stacking the state vectors column wise, in order to obtain: $\textbf{X}=[\text{A}^{(1) \text{T}}, \text{A}^{(2) \text{T}}, &mldr;, \text{A}^{(\text{T}) \text{T}}]$.</p><p>For the training technically every method we have implemented could be used, but in this first trial we just used the Ridge Regression. In the original paper the use of the pseudo-inverse was opted.</p><h1 id=implementation-in-reservoircomputingjl>Implementation in ReservoirComputing.jl<a hidden class=anchor aria-hidden=true href=#implementation-in-reservoircomputingjl>#</a></h1><p>Following the procedure described above we implemented in ReservoirComputing.jl a <code>RECA_discrete</code> object and a <code>RECAdirect_predict_discrete</code> function. The goal was to reproduce the results found in the literature, so the discrete approach was the only way to ensure that our implementation is correct. One of the goals is to expand this architecture to be also able to predict continuous values, such as timeseries. In this week an effort in this direction was made, but further exploration is needed. The <code>RECA_discrete</code> constructor takes as input</p><ul><li><code>train_data</code> the data needed for the ReCA training</li><li><code>rule</code> the ECA rule for the reservoir</li><li><code>generations</code> the number of generations the ECA will expand in</li><li><code>expansion_size</code> the \( L_d \) parameter</li><li><code>permutations</code> the number of additional ECA for the reservoir training</li><li><code>nla_type</code> the non linear algorithm for the reservoir states. Default is <code>NLADefalut()</code></li></ul><p>The training is done using the already implemented <code>ESNtrain</code>, that will probably need a name change in the future since now it can train another family of Reservoir Computing models. The <code>RECAdirect_predict_discrete</code> function takes as input</p><ul><li><code>reca</code> an already constructed <code>RECA_discrete</code></li><li><code>W_out</code> the output of <code>ESNtrain</code></li><li><code>test_data</code> the input data for the direct prediction</li></ul><p>Additionally a <code>ECA</code> constructor is also added to the package, taking as input the chosen <code>rule</code>, a vector of starting values <code>starting_val</code> and the number of <code>generations</code> for the ECA.</p><h1 id=examples>Examples<a hidden class=anchor aria-hidden=true href=#examples>#</a></h1><p>For testing the ReCA implementation we chose to solve the 5 bit memory task, a problem introduced in <a href=#14>[14]</a>, a test proved to be hard for both Recurrent Neural Networks (RNN) and Echo State Networks (ESN), and fairly diffused in the ReCA literature.</p><p>The test consists of four binary inputs and four binary outputs. In the first five timesteps of one run of the input sequence the first channel is one of the 32 possible five digit binary numbers, and the second input is complementary to the values in the first input (0, when the first channel is 1 and viceversa). The other two channels are zeros. This is the message that the model will have to remember. This is follow by a distractor period of $\text{T}_0$ steps, in which all the channels are zero with the exception of the third one, which is one up until $\text{T}_0-1$, where the fourth channel will be one and the third zero. This represents the cue. After that all channels except the third are zero.</p><p>For the output signal, all the channel are zero, but the third one which is one for all the steps with the exception of the last five, where the message from the input is repeated. A task is successful when the system is capable of reproducing all the $32 \times (5+\text{T}_0) \times 4$ bits of the output.</p><p>Below we can see an illustration <a href=#3>[3]</a> of the data contained in the 5 bit memory task:</p><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/87881469-b04b8380-c9f9-11ea-96c8-737c9bebdefd.png alt=5bittask></p><p>Using a distractor period of \( \text{T}_0 = 200 \) and a value of \( \text{L}_d = 40 \) we tried to reproduce the results in the literature. In the table below are shown the successful run out of 100 performed, and the values in square indicates the number of generations and permutations, and are chosen in accordance to the values presented in the papers analized.</p><p>The lines of code needed for the training and prediction of the 5 bit memory task with the ReCA are the following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>reca <span style=color:#f92672>=</span> RECA_discrete(input, <span style=color:#ae81ff>60</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>40</span>, <span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>W_out <span style=color:#f92672>=</span> ESNtrain(reca, <span style=color:#ae81ff>0.01</span>, train_data <span style=color:#f92672>=</span> convert(<span style=color:#66d9ef>AbstractArray</span>{<span style=color:#66d9ef>Float64</span>}, output))
</span></span><span style=display:flex><span>result <span style=color:#f92672>=</span> RECAdirect_predict_discrete(reca, W_out, input)
</span></span></code></pre></div><p>Where <code>input</code> and <code>output</code> are the datasets explained above, and the parameters to change for the results are <code>rule</code>, <code>generations</code> and <code>permutations</code>, in this example set to 60, 8, 8. Doing a cylce over each of them, for 100 runs we obtain the results below:</p><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/87881657-f6edad80-c9fa-11ea-9c25-2db29128b7ac.png alt=table></p><p>The values are in line with the results found in the literature, with little differences that could be attributed mainly to the training method. As already noted in the original paper, the computational power increases with the increasing of values of generations and permutations. It seems though that more generations is preferable over more permutations, since the (8, 16) correct runs are consistently less than the (16, 8) ones.</p><p>This model is really interesting, since it shows the capabilities of the Reservoir Computing approach. This family of models is still in its infancy, and a method for prediction of a continuous dataset is still missing. We hope that the implementation given in this package could help move the research in this direction.</p><p>As always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!</p><h2 id=citing>Citing<a hidden class=anchor aria-hidden=true href=#citing>#</a></h2><p>Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bibtex data-lang=bibtex><span style=display:flex><span><span style=color:#a6e22e>@article</span>{JMLR:v23:22-0611,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>author</span>  = <span style=color:#e6db74>{Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>title</span>   = <span style=color:#e6db74>{ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>journal</span> = <span style=color:#e6db74>{Journal of Machine Learning Research}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>year</span>    = <span style=color:#e6db74>{2022}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>volume</span>  = <span style=color:#e6db74>{23}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>number</span>  = <span style=color:#e6db74>{288}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>pages</span>   = <span style=color:#e6db74>{1--8}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>url</span>     = <span style=color:#e6db74>{http://jmlr.org/papers/v23/22-0611.html}</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=documentation>Documentation<a hidden class=anchor aria-hidden=true href=#documentation>#</a></h2><p>[1]
Yilmaz, Ozgur. &ldquo;Reservoir computing using cellular automata.&rdquo; arXiv preprint arXiv:1410.0162 (2014).</p><p>[2]
Margem, Mrwan, and Ozgür Yilmaz. &ldquo;An experimental study on cellular automata reservoir in pathological sequence learning tasks.&rdquo; (2017).</p><p>[3]
Margem, Mrwan, and Osman S. Gedik. &ldquo;Feed-forward versus recurrent architecture and local versus cellular automata distributed representation in reservoir computing for sequence memory learning.&rdquo; Artificial Intelligence Review (2020): 1-30.</p><p>[4]
Nichele, Stefano, and Andreas Molund. &ldquo;Deep reservoir computing using cellular automata.&rdquo; arXiv preprint arXiv:1703.02806 (2017).</p><p>[5]
Nichele, Stefano, and Magnus S. Gundersen. &ldquo;Reservoir computing using non-uniform binary cellular automata.&rdquo; arXiv preprint arXiv:1702.03812 (2017).</p><p>[6]
McDonald, Nathan. &ldquo;Reservoir computing & extreme learning machines using pairs of cellular automata rules.&rdquo; 2017 International Joint Conference on Neural Networks (IJCNN). IEEE, 2017.</p><p>[7]
Babson, Neil, and Christof Teuscher. &ldquo;Reservoir Computing with Complex Cellular Automata.&rdquo; Complex Systems 28.4 (2019).</p><p>[8]
Neumann, János, and Arthur W. Burks. Theory of self-reproducing automata. Vol. 1102024. Urbana: University of Illinois press, 1966.</p><p>[9]
Bia_ynicki-Birula, Iwo, and Iwo Bialynicki-Birula. Modeling Reality: How computers mirror life. Vol. 1. Oxford University Press on Demand, 2004.</p><p>[10]
Wolfram, Stephen. A new kind of science. Vol. 5. Champaign, IL: Wolfram media, 2002.</p><p>[11]
Adamatzky, Andrew, and Genaro J. Martinez. &ldquo;On generative morphological diversity of elementary cellular automata.&rdquo; Kybernetes (2010).</p><p>[12]
Wuensche, Andrew, Mike Lesser, and Michael J. Lesser. Global Dynamics of Cellular Automata: An Atlas of Basin of Attraction Fields of One-Dimensional Cellular Automata. Vol. 1. Andrew Wuensche, 1992.</p><p>[13]
Wolfram, Stephen. &ldquo;Universality and complexity in cellular automata.&rdquo; Physica D: Nonlinear Phenomena 10.1-2 (1984): 1-35.</p><p>[14]
Hochreiter, Sepp, and Jürgen Schmidhuber. &ldquo;Long short-term memory.&rdquo; Neural computation 9.8 (1997): 1735-1780.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://martinuzzifrancesco.github.io/>Francesco Martinuzzi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>