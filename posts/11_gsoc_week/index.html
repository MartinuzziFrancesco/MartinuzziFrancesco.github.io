<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GSoC week 11: Gated Recurring Unit-based reservoir | Francesco Martinuzzi</title><meta name=keywords content><meta name=description content="Following an architecture found on [1] this week we decided to implement a reservoir model based on the Gated Recurring Unit (GRU) structure, first described in [2]. This architecture is an evolution of the standard Recurrent Neural Network (RNN) update equations and works in a similar way to Long Short Term Memory (LSTM) with a forget gate but with fewer parameters; the LSTM usually outperfofms the GRU in most task but it could be interesting to see the behavior of this unit in the Echo State Network (ESN) model."><meta name=author content="Francesco Martinuzzi"><link rel=canonical href=https://martinuzzifrancesco.github.io/posts/11_gsoc_week/><link crossorigin=anonymous href=/assets/css/stylesheet.849fd8bd636f9bdcc8fd3087509c431a61faae78bd735e7ba75e6fd13ec64f83.css integrity="sha256-hJ/YvWNvm9zI/TCHUJxDGmH6rni9c157p15v0T7GT4M=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://martinuzzifrancesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://martinuzzifrancesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://martinuzzifrancesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://martinuzzifrancesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://martinuzzifrancesco.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="GSoC week 11: Gated Recurring Unit-based reservoir"><meta property="og:description" content="Following an architecture found on [1] this week we decided to implement a reservoir model based on the Gated Recurring Unit (GRU) structure, first described in [2]. This architecture is an evolution of the standard Recurrent Neural Network (RNN) update equations and works in a similar way to Long Short Term Memory (LSTM) with a forget gate but with fewer parameters; the LSTM usually outperfofms the GRU in most task but it could be interesting to see the behavior of this unit in the Echo State Network (ESN) model."><meta property="og:type" content="article"><meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/11_gsoc_week/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-08-16T21:14:36+02:00"><meta property="article:modified_time" content="2020-08-16T21:14:36+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GSoC week 11: Gated Recurring Unit-based reservoir"><meta name=twitter:description content="Following an architecture found on [1] this week we decided to implement a reservoir model based on the Gated Recurring Unit (GRU) structure, first described in [2]. This architecture is an evolution of the standard Recurrent Neural Network (RNN) update equations and works in a similar way to Long Short Term Memory (LSTM) with a forget gate but with fewer parameters; the LSTM usually outperfofms the GRU in most task but it could be interesting to see the behavior of this unit in the Echo State Network (ESN) model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://martinuzzifrancesco.github.io/posts/"},{"@type":"ListItem","position":2,"name":"GSoC week 11: Gated Recurring Unit-based reservoir","item":"https://martinuzzifrancesco.github.io/posts/11_gsoc_week/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GSoC week 11: Gated Recurring Unit-based reservoir","name":"GSoC week 11: Gated Recurring Unit-based reservoir","description":"Following an architecture found on [1] this week we decided to implement a reservoir model based on the Gated Recurring Unit (GRU) structure, first described in [2]. This architecture is an evolution of the standard Recurrent Neural Network (RNN) update equations and works in a similar way to Long Short Term Memory (LSTM) with a forget gate but with fewer parameters; the LSTM usually outperfofms the GRU in most task but it could be interesting to see the behavior of this unit in the Echo State Network (ESN) model.","keywords":[],"articleBody":"Following an architecture found on [1] this week we decided to implement a reservoir model based on the Gated Recurring Unit (GRU) structure, first described in [2]. This architecture is an evolution of the standard Recurrent Neural Network (RNN) update equations and works in a similar way to Long Short Term Memory (LSTM) with a forget gate but with fewer parameters; the LSTM usually outperfofms the GRU in most task but it could be interesting to see the behavior of this unit in the Echo State Network (ESN) model. In the first part of this post we will briefly explain the theory behind the model and after we will show an example to see the performance of this architecture.\nGated Recurring Unit As described in [2] the update equations in the GRU hidden unit are described as follows:\n  The reset gate is computed by $$\\textbf{r}_t = \\sigma (\\textbf{W}_r \\textbf{x}_t + \\textbf{U}_r \\textbf{h}_{t-1} + \\textbf{b}_r)$$ where \\( \\sigma \\) is the sigmoid function. \\( \\textbf{x}_t \\) is the input at time \\( t \\) and \\( \\textbf{h}_{t-1} \\) is the previous hidden state. In the ESN case it will be the provious state vector.\n  In a similar way, the update gate is computed by $$\\textbf{z}_t = \\sigma (\\textbf{W}_z \\textbf{x}_t + \\textbf{U}_z \\textbf{h}_{t-1} + \\textbf{b}_z)$$\n  The candidate activation vector is given by $$\\tilde{\\textbf{h}}_t = f(\\textbf{W}_h \\textbf{x}_t + \\textbf{U}_h (\\textbf{r}_t \\circ \\textbf{h}_{t-1}) + \\textbf{b}_h)$$ where \\( \\circ \\) represents the Hadamard product. In the ESN case \\( \\textbf{U}_h = \\textbf{W}, \\textbf{W}_h = \\textbf{W}_in \\) where \\( \\textbf{W} \\) is the reservoir matrix and \\( \\textbf{W}_in \\) is the input layer matrix. In the original implementation the activation function \\( f \\) is taken to be the hyperbolic tangent.\n  The final states vector is given by $$\\textbf{h}_t = (1-\\textbf{z}_t) \\circ \\textbf{h}_{t-1} + \\textbf{z}_t \\circ \\tilde{\\textbf{h}}_t$$\n  Alternative forms are known but for the first implementation we decided to focus more our attention on the standard model. The \\( \\textbf{W}, \\textbf{U} \\) layers are fixed and constructed using the irrational number input layer generator (see [3] or week 9), with a different start for the change of sign but in the future we would like to give more possibilities for the construction of these layers.\nImplementation in ReservoirComputing The overall implementation is not the hardest part, band following the instructions of the original paper we were able to implement a gru base function that updates the states vector at every time step. Building on that function we implemented two public function, the constructor GRUESN and the predictor GRUESNpredict. The first one takes as input the same inputs as the ESN constructor with the addition of the gates_weight optional value, set to 0.9 as default. The GRUESNpredict function takes as input the same values as the ESNpredict function and return the prediction made by the GRUESN.\nExample Since this model isn not found in literature, only as comparison in [1] but for different tasks than time series prediction, we chose to use yet again the Henon map to test the capabilities of this model in the reproduction of a choatic system. This particular model was chosen since is less complex than the Lorenz system and it requires little parameter tuning in order to obtain decent results.\nLet us start by insalling and importing the usual packages\nusing Pkg Pkg.add(\"ReservoirComputing\") Pkg.add(\"DynamicalSystems\") Pkg.add(\"Plots\") using ReservoirComputing using DynamicalSystems using Plots The construction of the Henon map is straight forward. Again the data points are shifted by -0.5 and scaled by 2:\nds = Systems.henon() traj = trajectory(ds, 7000) data = Matrix(traj)'  data = (data .-0.5) .* 2 shift = 200 train_len = 2000 predict_len = 3000 train = data[:, shift:shift+train_len-1] test = data[:, shift+train_len:shift+train_len+predict_len-1] For this example we will use the irrational sign input matrix in order to be consistent with the construction of the GRU unit, and for the reservoir matrix we will use the standard implementation\napprox_res_size = 100 radius = 0.99 sparsity = 0.1 sigma = 1.0 beta = 1*10^(-1) extended_states = false input_weight = 0.1  W = init_reservoir_givensp(approx_res_size, radius, sparsity) W_in = irrational_sign_input(approx_res_size, size(train, 1), input_weight) @time gruesn = GRUESN(W, train, W_in, extended_states = extended_states, gates_weight = 0.8) 0.286364 seconds (51.78 k allocations: 36.200 MiB, 13.94% gc time) The parameters were chosen by manual grid search, so it is possibile that they are not the best ones for this task. A more in depth research will be needed for this specific prediction. Using these values we can train the GRUESN and make a prediction. We will scatter the results after in order to compare the prediction obtained\nW_out = ESNtrain(gruesn, beta) output = GRUESNpredict(gruesn, predict_len, W_out) scatter(output[1,:], output[2, :], lable = \"ESN-CRJ\") The actual Henon map is the following\nscatter(test[1,:], test[2,:], label=\"actual\") As we can see the model is able to replicate the behavior of the chaotic system up to a certain degree. The prediction is not as clear cut as others taht we were able to obtain but it shows the potential of this model, given more time for the parameters tuning. Using a different construction for the hidden layers could also help in improving the predictive capabilities.\nAs always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!\nCiting Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:\n@article{JMLR:v23:22-0611,  author = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora},  title = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models},  journal = {Journal of Machine Learning Research},  year = {2022},  volume = {23},  number = {288},  pages = {1--8},  url = {http://jmlr.org/papers/v23/22-0611.html} } Documentation [1]Paaßen, Benjamin, and Alexander Schulz. “Reservoir memory machines.” arXiv preprint arXiv:2003.04793 (2020).\n[2]Cho, Kyunghyun, et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014).\n[2]Rodan, Ali, and Peter Tiňo. “Simple deterministically constructed cycle reservoirs with regular jumps.” Neural computation 24.7 (2012): 1822-1852.\n","wordCount":"1009","inLanguage":"en","datePublished":"2020-08-16T21:14:36+02:00","dateModified":"2020-08-16T21:14:36+02:00","author":{"@type":"Person","name":"Francesco Martinuzzi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://martinuzzifrancesco.github.io/posts/11_gsoc_week/"},"publisher":{"@type":"Organization","name":"Francesco Martinuzzi","logo":{"@type":"ImageObject","url":"https://martinuzzifrancesco.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://martinuzzifrancesco.github.io/ accesskey=h title="Francesco Martinuzzi (Alt + H)">Francesco Martinuzzi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://martinuzzifrancesco.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://martinuzzifrancesco.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://martinuzzifrancesco.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>GSoC week 11: Gated Recurring Unit-based reservoir</h1><div class=post-meta><span title='2020-08-16 21:14:36 +0200 CEST'>August 16, 2020</span>&nbsp;·&nbsp;Francesco Martinuzzi</div></header><div class=post-content><p>Following an architecture found on <a href=#1>[1]</a> this week we decided to implement a reservoir model based on the Gated Recurring Unit (GRU) structure, first described in <a href=#2>[2]</a>. This architecture is an evolution of the standard Recurrent Neural Network (RNN) update equations and works in a similar way to Long Short Term Memory (LSTM) with a forget gate but with fewer parameters; the LSTM usually outperfofms the GRU in most task but it could be interesting to see the behavior of this unit in the Echo State Network (ESN) model. In the first part of this post we will briefly explain the theory behind the model and after we will show an example to see the performance of this architecture.</p><h1 id=gated-recurring-unit>Gated Recurring Unit<a hidden class=anchor aria-hidden=true href=#gated-recurring-unit>#</a></h1><p>As described in <a href=#2>[2]</a> the update equations in the GRU hidden unit are described as follows:</p><ul><li><p>The reset gate is computed by
$$\textbf{r}_t = \sigma (\textbf{W}_r \textbf{x}_t + \textbf{U}_r \textbf{h}_{t-1} + \textbf{b}_r)$$
where \( \sigma \) is the sigmoid function. \( \textbf{x}_t \) is the input at time \( t \) and \( \textbf{h}_{t-1} \) is the previous hidden state. In the ESN case it will be the provious state vector.</p></li><li><p>In a similar way, the update gate is computed by
$$\textbf{z}_t = \sigma (\textbf{W}_z \textbf{x}_t + \textbf{U}_z \textbf{h}_{t-1} + \textbf{b}_z)$$</p></li><li><p>The candidate activation vector is given by
$$\tilde{\textbf{h}}_t = f(\textbf{W}_h \textbf{x}_t + \textbf{U}_h (\textbf{r}_t \circ \textbf{h}_{t-1}) + \textbf{b}_h)$$
where \( \circ \) represents the Hadamard product. In the ESN case \( \textbf{U}_h = \textbf{W}, \textbf{W}_h = \textbf{W}_in \) where \( \textbf{W} \) is the reservoir matrix and \( \textbf{W}_in \) is the input layer matrix. In the original implementation the activation function \( f \) is taken to be the hyperbolic tangent.</p></li><li><p>The final states vector is given by
$$\textbf{h}_t = (1-\textbf{z}_t) \circ \textbf{h}_{t-1} + \textbf{z}_t \circ \tilde{\textbf{h}}_t$$</p></li></ul><p>Alternative forms are known but for the first implementation we decided to focus more our attention on the standard model. The \( \textbf{W}, \textbf{U} \) layers are fixed and constructed using the irrational number input layer generator (see <a href=#3>[3]</a> or <a href=https://martinuzzifrancesco.github.io/posts/09_gsoc_week/>week 9</a>), with a different start for the change of sign but in the future we would like to give more possibilities for the construction of these layers.</p><h1 id=implementation-in-reservoircomputing>Implementation in ReservoirComputing<a hidden class=anchor aria-hidden=true href=#implementation-in-reservoircomputing>#</a></h1><p>The overall implementation is not the hardest part, band following the instructions of the original paper we were able to implement a <code>gru</code> base function that updates the states vector at every time step. Building on that function we implemented two public function, the constructor <code>GRUESN</code> and the predictor <code>GRUESNpredict</code>. The first one takes as input the same inputs as the <code>ESN</code> constructor with the addition of the <code>gates_weight</code> optional value, set to 0.9 as default. The <code>GRUESNpredict</code> function takes as input the same values as the <code>ESNpredict</code> function and return the prediction made by the GRUESN.</p><h1 id=example>Example<a hidden class=anchor aria-hidden=true href=#example>#</a></h1><p>Since this model isn not found in literature, only as comparison in <a href=#2>[1]</a> but for different tasks than time series prediction, we chose to use yet again the <a href=https://en.wikipedia.org/wiki/H%C3%A9non_map>Henon map</a> to test the capabilities of this model in the reproduction of a choatic system. This particular model was chosen since is less complex than the Lorenz system and it requires little parameter tuning in order to obtain decent results.</p><p>Let us start by insalling and importing the usual packages</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> Pkg
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;ReservoirComputing&#34;</span>)
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;DynamicalSystems&#34;</span>)
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;Plots&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> ReservoirComputing
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> DynamicalSystems
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> Plots
</span></span></code></pre></div><p>The construction of the Henon map is straight forward. Again the data points are shifted by -0.5 and scaled by 2:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>ds <span style=color:#f92672>=</span> Systems<span style=color:#f92672>.</span>henon()
</span></span><span style=display:flex><span>traj <span style=color:#f92672>=</span> trajectory(ds, <span style=color:#ae81ff>7000</span>)
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> <span style=color:#66d9ef>Matrix</span>(traj)<span style=color:#f92672>&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> (data <span style=color:#f92672>.-</span><span style=color:#ae81ff>0.5</span>) <span style=color:#f92672>.*</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>shift <span style=color:#f92672>=</span> <span style=color:#ae81ff>200</span>
</span></span><span style=display:flex><span>train_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>2000</span>
</span></span><span style=display:flex><span>predict_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>3000</span>
</span></span><span style=display:flex><span>train <span style=color:#f92672>=</span> data[<span style=color:#f92672>:</span>, shift<span style=color:#f92672>:</span>shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>test <span style=color:#f92672>=</span> data[<span style=color:#f92672>:</span>, shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>:</span>shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>+</span>predict_len<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span></code></pre></div><p>For this example we will use the irrational sign input matrix in order to be consistent with the construction of the GRU unit, and for the reservoir matrix we will use the standard implementation</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.99</span>
</span></span><span style=display:flex><span>sparsity <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>*</span><span style=color:#ae81ff>10</span><span style=color:#f92672>^</span>(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span><span style=display:flex><span>input_weight <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>W <span style=color:#f92672>=</span> init_reservoir_givensp(approx_res_size, radius, sparsity)
</span></span><span style=display:flex><span>W_in <span style=color:#f92672>=</span> irrational_sign_input(approx_res_size, size(train, <span style=color:#ae81ff>1</span>), input_weight)
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> gruesn <span style=color:#f92672>=</span> GRUESN(W, train, W_in, extended_states <span style=color:#f92672>=</span> extended_states, gates_weight <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.8</span>)
</span></span></code></pre></div><pre tabindex=0><code>0.286364 seconds (51.78 k allocations: 36.200 MiB, 13.94% gc time)
</code></pre><p>The parameters were chosen by manual grid search, so it is possibile that they are not the best ones for this task. A more in depth research will be needed for this specific prediction. Using these values we can train the GRUESN and make a prediction. We will scatter the results after in order to compare the prediction obtained</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>W_out <span style=color:#f92672>=</span> ESNtrain(gruesn, beta)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> GRUESNpredict(gruesn, predict_len, W_out)
</span></span><span style=display:flex><span>scatter(output[<span style=color:#ae81ff>1</span>,<span style=color:#f92672>:</span>], output[<span style=color:#ae81ff>2</span>, <span style=color:#f92672>:</span>], lable <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;ESN-CRJ&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/90342956-aaeb5400-e00c-11ea-97a2-3c9ae8b0d208.png alt=gruesn></p><p>The actual Henon map is the following</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>scatter(test[<span style=color:#ae81ff>1</span>,<span style=color:#f92672>:</span>], test[<span style=color:#ae81ff>2</span>,<span style=color:#f92672>:</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;actual&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/87250878-4dda0c80-c468-11ea-8b38-d7071f051363.png alt=actual></p><p>As we can see the model is able to replicate the behavior of the chaotic system up to a certain degree. The prediction is not as clear cut as others taht we were able to obtain but it shows the potential of this model, given more time for the parameters tuning. Using a different construction for the hidden layers could also help in improving the predictive capabilities.</p><p>As always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!</p><h2 id=citing>Citing<a hidden class=anchor aria-hidden=true href=#citing>#</a></h2><p>Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bibtex data-lang=bibtex><span style=display:flex><span><span style=color:#a6e22e>@article</span>{JMLR:v23:22-0611,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>author</span>  = <span style=color:#e6db74>{Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>title</span>   = <span style=color:#e6db74>{ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>journal</span> = <span style=color:#e6db74>{Journal of Machine Learning Research}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>year</span>    = <span style=color:#e6db74>{2022}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>volume</span>  = <span style=color:#e6db74>{23}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>number</span>  = <span style=color:#e6db74>{288}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>pages</span>   = <span style=color:#e6db74>{1--8}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>url</span>     = <span style=color:#e6db74>{http://jmlr.org/papers/v23/22-0611.html}</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=documentation>Documentation<a hidden class=anchor aria-hidden=true href=#documentation>#</a></h2><p>[1]
Paaßen, Benjamin, and Alexander Schulz. &ldquo;Reservoir memory machines.&rdquo; arXiv preprint arXiv:2003.04793 (2020).</p><p>[2]
Cho, Kyunghyun, et al. &ldquo;Learning phrase representations using RNN encoder-decoder for statistical machine translation.&rdquo; arXiv preprint arXiv:1406.1078 (2014).</p><p>[2]
Rodan, Ali, and Peter Tiňo. &ldquo;Simple deterministically constructed cycle reservoirs with regular jumps.&rdquo; Neural computation 24.7 (2012): 1822-1852.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://martinuzzifrancesco.github.io/>Francesco Martinuzzi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>