<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.71.1" />

  <title>A brief introduction to Reservoir Computing &middot; Francesco Martinuzzi</title>

  <meta name="description" content="" />

  

<meta itemprop="name" content="A brief introduction to Reservoir Computing">
<meta itemprop="description" content="This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package ReservoirComputing.">
<meta itemprop="datePublished" content="2020-05-26T22:17:09&#43;02:00" />
<meta itemprop="dateModified" content="2020-05-26T22:17:09&#43;02:00" />
<meta itemprop="wordCount" content="1722">
<meta itemprop="image" content="https://martinuzzifrancesco.github.io/images/"/>



<meta itemprop="keywords" content="" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://martinuzzifrancesco.github.io/images/"/>

<meta name="twitter:title" content="A brief introduction to Reservoir Computing"/>
<meta name="twitter:description" content="This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package ReservoirComputing."/>


<meta property="og:title" content="A brief introduction to Reservoir Computing" />
<meta property="og:description" content="This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package ReservoirComputing." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/a-brief-introduction-to-reservoir-computing/" />
<meta property="og:image" content="https://martinuzzifrancesco.github.io/images/"/>
<meta property="article:published_time" content="2020-05-26T22:17:09+02:00" />
<meta property="article:modified_time" content="2020-05-26T22:17:09+02:00" /><meta property="og:site_name" content="Francesco Martinuzzi" />



  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/print.css"
        media="print">

  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/poole.css">

  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/hyde.css">

  
<style type="text/css">
  .sidebar {
    background-color: #536060;
  }

  .read-more-link a {
    border-color: #536060;
  }

  .pagination li a {
    color: #536060;
    border: 1px solid #536060;
  }

  .pagination li.active a {
    background-color: #536060;
  }

  .pagination li a:hover {
    background-color: #536060;
    opacity: 0.75;
  }

  footer a,
  .content a,
  .related-posts li a:hover {
    color: #536060;
  }
</style>



  

  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700&display=swap">

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css"
        integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk="
        crossorigin="anonymous" />

  <link rel="apple-touch-icon-precomposed"
        sizes="144x144"
        href="/apple-touch-icon-144-precomposed.png">

  <link rel="shortcut icon" href="/favicon.png">

  
  </head>
<body>
  <aside class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      
        
        <div class="author-image">
          <img src="https://martinuzzifrancesco.github.io/images/prova.jpg" class="img-circle img-headshot center" alt="Profile Picture">
        </div>
        
      

      <h1>Francesco Martinuzzi</h1>

      
      <p class="lead">Physicist learning how to make machines learn</p>
      
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li>
          <a href="https://martinuzzifrancesco.github.io/">Home</a>
        </li>
        <li>
          <a href="/posts/"> Posts </a>
        </li><li>
          <a href="/about/"> About </a>
        </li><li>
          <a href="/contact/"> Contact </a>
        </li>
      </ul>
    </nav>

    <section class="social-icons">
      
      <a href="https://github.com/MartinuzziFrancesco" rel="me" title="GitHub">
        <i class="fab fa-github" aria-hidden="true"></i>
      </a>
      
      <a href="https://twitter.com/MartinuzziFra" rel="me" title="Twitter">
        <i class="fab fa-twitter" aria-hidden="true"></i>
      </a>
      
    </section>
  </div>
</aside>


  <main class="content container">
  <div class="post">
  <h1>A brief introduction to Reservoir Computing</h1>

  <div class="post-date">
    <time datetime="2020-05-26T22:17:09&#43;0200">May 26, 2020</time> · 9 min read
  </div>

  <p>This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package <a href="https://github.com/SciML/ReservoirComputing.jl">ReservoirComputing.jl</a> as example tool. Since this package is a work in progress and it is currently the main project I am working on as part of <a href="https://summerofcode.withgoogle.com/projects/#5374375945043968">Google Summer of Code</a> this is but the first post of many detailing the implementations that will be done in this direction.</p>
<h1 id="what-is-reservoir-computing">What is Reservoir Computing?</h1>
<p>Reservoir Computing is an umbrella term used to identify a general framework of computation derived from Recurrent Neural Networks (RNN), indipendently developed by Jaeger <a href="#1">[1]</a> and Maass et al. <a href="#2">[2]</a>. These papers introduced the concepts of Echo State Networks (ESN) and Liquid State Machines (LSM) respectively. Further improvements over these two models constitute what is now called the field of Reservoir Computing. The main idea lies in leveraging a fixed non-linear system, of higher dimension than the input, onto wich to input signal is mapped. After this mapping is only necessary to use a simple readout layer to harvest the state of the reservoir and to train it to the desered output. In principle, given a system complex enough, this architecture should be capable of any computation <a href="#3">[3]</a>. The intuition was born from the fact that in training RNNs most of the times the weights showing most change were the ones in the last layer <a href="#4">[4]</a>. In the next section we will also see that ESNs actually use a fixed random RNN as the reservoir. Given the static nature of this implementations usually ESNs are able to yield faster results and in same cases even better, in particular when dealing with chaotic time series predictions <a href="#5">[5]</a>.</p>
<p>Not every complex system is suited to be a good reservoir though. A good reservoir is one that is able to separate inputs; different external inputs should drive the system to different regions of the configuration space <a href="#3">[3]</a>. This is called the <strong>separability condition</strong>. Furthermore an important property for the reservoirs of ESNs is the <strong>Echo State property</strong> which states that inputs to the reservoir echo in the system forever, or util they dissipate. A more formal definition of this property can be found in <a href="#6">[6]</a>.</p>
<p>In order to better show the inner workings of models of this family I will try to explain in mathematical detail the ESN, a model that is already implemented in the package, so it will make for useful examples.</p>
<h1 id="echo-state-networks">Echo State Networks</h1>
<h2 id="theoretical-background">Theoretical Background</h2>
<p>This intends to be a quick overview of the theory behind the ESN model to get the reader acquainted with the concepts and workings of this particular model, and it is by no means comprehensive. For in depth reviews and explanations please refer to <a href="#7">[7]</a> and <a href="#8">[8]</a>. All of the information laid out in this section is adapted from this two sources, unless stated otherwise.</p>
<p>Let us suppose we have an input signal \( \textbf{u}(t) \in R^M \) where \( t = 1, &hellip;, T \) is the discrete time and T the number of data points in the training set. In order to project this input into the reservoir we will need an input to reservoir coupler, called \( \textbf{W}_{\text{in}} \in R^{N \times M} \). Usually this matrix is built in the same way the reservior is, and at the same time. For the implementation used in ReservoirComputing.jl we have followed the same construction used in <a href="#9">[9]</a> where the i-th of the M input signals is connected to N/M reservoir nodes with connection weights in the i-th column of \( \textbf{W}_{\text{in}} \). The non zero elements are chosen randomly from an uniform distribution and then scaled in the range \( [-\sigma , \sigma ] \).</p>
<p>The reservoir is constitued by N neurons connected in a Erdos Renyi graph configuration and is represented by an adjacency matrix \( \textbf{W} \) of size \( N \times N \) with values drawn from a uniform random distribution on the interval \( [-1, 1] \) <a href="5">[5]</a>. This is the most important aspect of the ESN so in order to build it in an efficient manner we must first understand all of its components.</p>
<ul>
<li>The size \( N \) is of course the single most important one: the more challenging the task the bigger the size should be. Of course a bigger matrix will mean more computational time so the advice of Lukoševičius is to start small and then scale.</li>
<li>The sparsity of the reservoir. In most papers we see that each reservoir node is connected to a small number of other nodes, ranging from 5 to 12. The sparseness, beside theoretical implications, is also useful to speed up computations.</li>
<li>Spectral radius. After the generation of a random sparse reservoir matrix its spectral radius \( \rho (\textbf{W}) \) is computed and \( \textbf{W} \) is divided by it. This allows us to obtain a matrix with a unit spectral radius, that we can scale to a more suited value. Altough there are exceptions (when the inputs \( \textbf{u}(t) \) are nonzero for example), a spectral radius smaller than unity \( \rho (\textbf{W}) &lt; 1 \) ensures the echo state property. More generally this parameter should be selected to maximize the performance, keeping the unitary value as a useful reference point.</li>
</ul>
<p>After the construction of the input layer and the reservoir we can focus on harvesting the states. The update equations of the ESN are:</p>
<p>$$\textbf{x}(t+\Delta t) = (1-\alpha) \textbf{x}(t)+\alpha f( \textbf{W} \textbf{x}(t)+ \textbf{W}_{\text{in}} \textbf{u}(t))$$</p>
<p>$$\textbf{v}(t+\Delta t) = g( \textbf{W}_{\text{out}} \textbf{x}(t))$$</p>
<p>where</p>
<ul>
<li>\( \textbf{v}(t) \in R^{M} \) is the predicted output</li>
<li>\( \textbf{x}(t) \in R^{N} \) is the state vector</li>
<li>\( \textbf{W}_{\text{out}} \in R^{M \times N} \) is the output layer</li>
<li>\( f \) and \( g \) are two activation functions, most commonly the hyperbolic tangent and identity respectively</li>
<li>\( \alpha \) is the leaking rate</li>
</ul>
<p>The computation of \( \textbf{W}_{\text{out}} \) can be expressend in terms of solving a system of linear equations</p>
<p>$$\textbf{Y}^{\text{target}}=\textbf{W}_{\text{out}} \textbf{X}$$</p>
<p>where \( \textbf{X} \) is the states matrix, built using the single states vector \( \textbf{x}(t) \) as column for every \( t=1, &hellip;, T \), and \( \textbf{Y}^{\text{target}} \) is built in the same way only using \( \textbf{y}^{\text{target}}(t) \). The solution of choice for this problem is usually the <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov regularization</a>, also called ridge regression which has the following close form:</p>
<p>$$\textbf{W}_{\text{out}} = \textbf{Y}^{\text{target}} \textbf{X}^{\text{T}}(\textbf{X} \textbf{X}^{\text{T}} + \beta \textbf{I})^{-1}$$</p>
<p>where \( \textbf{I} \) is the identity matrix and \(\beta \) is a regularization coefficient.</p>
<p>In short this is the core theory behind the ESNs. In order to visualize how they work let&rsquo;s look at an example.</p>
<h2 id="lorentz-system-prediction">Lorentz system prediction</h2>
<p>This is a task already tackled in literature, so our intent is to try and replicate the results found in <a href="10">[10]</a>. This example can be followed in its entirety <a href="https://github.com/SciML/ReservoirComputing.jl/blob/master/examples/lorenz_example.jl">here</a>. In this section we will just give part of the code to illustrate the theory explained above, some important parts are not displayed.</p>
<p>Supposing we have already created the train data, consitued by 5000 timesteps of the chaotic Lorenz system, we are going to use the same parameters found in the paper:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">approx_res_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">300</span>
radius <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.2</span>
degree <span style="color:#f92672">=</span> <span style="color:#ae81ff">6</span>
activation <span style="color:#f92672">=</span> tanh
sigma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
nla_type <span style="color:#f92672">=</span> NLAT2()
extended_states <span style="color:#f92672">=</span> <span style="color:#66d9ef">false</span>
</code></pre></div><p>The ESN can easily be called in the following way:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia"><span style="color:#66d9ef">using</span> ReservoirComputing
esn <span style="color:#f92672">=</span> ESN(approx_res_size,
    train,
    degree,
    radius,
    activation, <span style="color:#75715e">#default = tanh</span>
    alpha, <span style="color:#75715e">#default = 1.0</span>
    sigma, <span style="color:#75715e">#default = 0.1</span>
    nla_type <span style="color:#75715e">#default = NLADefault(),</span>
    extended_states <span style="color:#75715e">#default = false</span>
    )
</code></pre></div><p>The training and the prediction, for 1250 timestps, are carried out as follows</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">W_out <span style="color:#f92672">=</span> ESNtrain(esn, beta)
output <span style="color:#f92672">=</span> ESNpredict(esn, predict_len, W_out)
</code></pre></div><p>In order to visualize the solution we can plot the individual trajectories</p>
<p><img src="https://user-images.githubusercontent.com/10376688/81470264-42f5c800-91ea-11ea-98a2-a8a8d7d96155.png" alt="lorenz_coord"></p>
<p>and the attractors</p>
<p><img src="https://user-images.githubusercontent.com/10376688/81470281-5a34b580-91ea-11ea-9eea-d2b266da19f4.png" alt="lorenz_attractor"></p>
<p>As expected the short term predictions are very good, and in the long term the behaviour of the system is mantained.</p>
<p>But what happens if the parameters are not ideal? In the paper is given an example where the spectral radius is bigger than the idel value and the results are compromised as a result. We can also show that if the value is less than one, as suggested in order to mantain the echo state property, the results are nowhere near optimal.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">radius <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.8</span>
</code></pre></div><p><img src="https://user-images.githubusercontent.com/10376688/83355574-91693300-a360-11ea-9794-cdbc9dc5388c.png" alt="lorenz_coord"></p>
<p>While incrementing the reservoir size is known to improve, up to a certain point, the results, a smaller one will almost surely worsen them.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">approx_res_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">60</span>
</code></pre></div><p><img src="https://user-images.githubusercontent.com/10376688/83355661-279d5900-a361-11ea-864c-fefc570eb6a1.png" alt="lorenz_coord"></p>
<p>As we can see then, the choice of the parameters is of the upmost importance in this model, as it is in most in the field of Machine Learning. There are ways of searching the optimal parameters in the state space such as grid search or random search, though experience with the model will give you the ability to know what to thinker with most of the times.</p>
<p>If you have any questions regarding the model, the package or you have found errors in my post don&rsquo;t hesitate to contact me!</p>
<h2 id="references">References</h2>
<p><!-- raw HTML omitted -->[1]<!-- raw HTML omitted -->
Jaeger, Herbert. &ldquo;The “echo state” approach to analysing and training recurrent neural networks-with an erratum note.&rdquo; Bonn, Germany: German National Research Center for Information Technology GMD Technical Report 148.34 (2001): 13.</p>
<p><!-- raw HTML omitted -->[2]<!-- raw HTML omitted -->
Maass W, Natschläger T, Markram H. Real-time computing without stable states: a new framework for neural computation based on perturbations. Neural Comput. 2002;14(11):2531‐2560.</p>
<p><!-- raw HTML omitted -->[3]<!-- raw HTML omitted -->
Konkoli Z.: Reservoir Computing. In: Meyers R. (eds) Encyclopedia of Complexity and Systems Science. Springer, Berlin, Heidelberg (2017)</p>
<p><!-- raw HTML omitted -->[4]<!-- raw HTML omitted -->
Schiller, Ulf D., and Jochen J. Steil. &ldquo;Analyzing the weight dynamics of recurrent learning algorithms.&rdquo; Neurocomputing 63 (2005): 5-23.</p>
<p><!-- raw HTML omitted -->[5]<!-- raw HTML omitted -->
Chattopadhyay, Ashesh, et al. &ldquo;Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods: Reservoir computing, ANN, and RNN-LSTM.&rdquo; arXiv preprint arXiv:1906.08829 (2019).</p>
<p><!-- raw HTML omitted -->[6]<!-- raw HTML omitted -->
Yildiz, Izzet B., Herbert Jaeger, and Stefan J. Kiebel. &ldquo;Re-visiting the echo state property.&rdquo; Neural networks 35 (2012): 1-9.</p>
<p><!-- raw HTML omitted -->[7]<!-- raw HTML omitted -->
Lukoševičius, Mantas. &ldquo;A practical guide to applying echo state networks.&rdquo; Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 659-686.</p>
<p><!-- raw HTML omitted -->[8]<!-- raw HTML omitted -->
Lukoševičius, Mantas, and Herbert Jaeger. &ldquo;Reservoir computing approaches to recurrent neural network training.&rdquo; Computer Science Review 3.3 (2009): 127-149.</p>
<p><!-- raw HTML omitted -->[9]<!-- raw HTML omitted -->
Lu, Zhixin, et al. &ldquo;Reservoir observers: Model-free inference of unmeasured variables in chaotic systems.&rdquo; Chaos: An Interdisciplinary Journal of Nonlinear Science 27.4 (2017): 041102.</p>
<p><!-- raw HTML omitted -->[10]<!-- raw HTML omitted -->
Pathak, Jaideep, et al. &ldquo;Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data.&rdquo; Chaos: An Interdisciplinary Journal of Nonlinear Science 27.12 (2017): 121102.</p>

</div>


  </main>

  <footer>
  <div>
    &copy; Martinuzzi Francesco 2020
    ·
    
    <a href="https://creativecommons.org/licenses/by-sa/4.0"
       target="_blank">CC BY-SA 4.0</a>
    
    
  </div>
</footer>

</body>

    <script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
            fontCache: 'global'
        }
    };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script> 


</html>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/js/all.min.js"
          integrity="sha256-MAgcygDRahs+F/Nk5Vz387whB4kSK9NXlDN3w58LLq0="
          crossorigin="anonymous"></script>

  

  
</body>
</html>
