<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A brief introduction to Reservoir Computing | Francesco Martinuzzi</title><meta name=keywords content><meta name=description content="This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package ReservoirComputing.jl as example tool. This package is a work in progress and it is currently the main project I am working on as part of the Google Summer of Code program. Future posts are going to further explain the various implementations and improvements to the code by means of comparisons with the literature and examples."><meta name=author content="Francesco Martinuzzi"><link rel=canonical href=https://martinuzzifrancesco.github.io/posts/a-brief-introduction-to-reservoir-computing/><link crossorigin=anonymous href=/assets/css/stylesheet.849fd8bd636f9bdcc8fd3087509c431a61faae78bd735e7ba75e6fd13ec64f83.css integrity="sha256-hJ/YvWNvm9zI/TCHUJxDGmH6rni9c157p15v0T7GT4M=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://martinuzzifrancesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://martinuzzifrancesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://martinuzzifrancesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://martinuzzifrancesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://martinuzzifrancesco.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="A brief introduction to Reservoir Computing"><meta property="og:description" content="This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package ReservoirComputing.jl as example tool. This package is a work in progress and it is currently the main project I am working on as part of the Google Summer of Code program. Future posts are going to further explain the various implementations and improvements to the code by means of comparisons with the literature and examples."><meta property="og:type" content="article"><meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/a-brief-introduction-to-reservoir-computing/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-05-26T22:17:09+02:00"><meta property="article:modified_time" content="2020-05-26T22:17:09+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A brief introduction to Reservoir Computing"><meta name=twitter:description content="This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package ReservoirComputing.jl as example tool. This package is a work in progress and it is currently the main project I am working on as part of the Google Summer of Code program. Future posts are going to further explain the various implementations and improvements to the code by means of comparisons with the literature and examples."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://martinuzzifrancesco.github.io/posts/"},{"@type":"ListItem","position":2,"name":"A brief introduction to Reservoir Computing","item":"https://martinuzzifrancesco.github.io/posts/a-brief-introduction-to-reservoir-computing/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A brief introduction to Reservoir Computing","name":"A brief introduction to Reservoir Computing","description":"This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package ReservoirComputing.jl as example tool. This package is a work in progress and it is currently the main project I am working on as part of the Google Summer of Code program. Future posts are going to further explain the various implementations and improvements to the code by means of comparisons with the literature and examples.","keywords":[],"articleBody":"This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package ReservoirComputing.jl as example tool. This package is a work in progress and it is currently the main project I am working on as part of the Google Summer of Code program. Future posts are going to further explain the various implementations and improvements to the code by means of comparisons with the literature and examples.\nWhat is Reservoir Computing? Reservoir Computing is an umbrella term used to identify a general framework of computation derived from Recurrent Neural Networks (RNN), indipendently developed by Jaeger [1] and Maass et al. [2]. These papers introduced the concepts of Echo State Networks (ESN) and Liquid State Machines (LSM) respectively. Further improvements over these two models constitute what is now called the field of Reservoir Computing. The main idea lies in leveraging a fixed non-linear system, of higher dimension than the input, onto which to input signal is mapped. After this mapping is only necessary to use a simple readout layer to harvest the state of the reservoir and to train it to the desired output. In principle, given a complex enough system, this architecture should be capable of any computation [3]. The intuition was born from the fact that in training RNNs most of the times the weights showing most change were the ones in the last layer [4]. In the next section we will also see that ESNs actually use a fixed random RNN as the reservoir. Given the static nature of this implementation usually ESNs can yield faster results and in some cases even better, in particular when dealing with chaotic time series predictions [5].\nBut not every complex system is suited to be a good reservoir. A good reservoir is one that is able to separate inputs; different external inputs should drive the system to different regions of the configuration space [3]. This is called the separability condition. Furthermore an important property for the reservoirs of ESNs is the Echo State property which states that inputs to the reservoir echo in the system forever, or util they dissipate. A more formal definition of this property can be found in [6].\nIn order to better show the inner workings of models of this family I am going to explain in mathematical details the ESN, a model that is already implemented in the package, so it will be useful for making examples.\nEcho State Networks Theoretical Background This intends to be a quick overview of the theory behind the ESN to get the reader acquainted with the concepts and workings of this particular model, and it is by no means comprehensive. For in depth reviews and explanations please refer to [7] and [8]. All of the information laid out in this section is adapted from these two sources, unless stated otherwise.\nLet us suppose we have an input signal \\( \\textbf{u}(t) \\in R^M \\) where \\( t = 1, …, T \\) is the discrete time and \\( T \\) the number of data points in the training set. In order to project this input onto the reservoir we will need an input to reservoir coupler, identified by the matrix \\( \\textbf{W}_{\\text{in}} \\in R^{N \\times M} \\). Usually this matrix is built in the same way the reservior is, and at the same time. For the implementation used in ReservoirComputing.jl we have followed the same construction proposed in [9] where the i-th of the \\( M \\) input signals is connected to \\( N/M \\) reservoir nodes with connection weights in the i-th column of \\( \\textbf{W}_{\\text{in}} \\). The non-zero elements are chosen randomly from a uniform distribution and then scaled in the range \\( [-\\sigma , \\sigma ] \\).\nThe reservoir is constitued by \\( N \\) neurons connected in a Erdős–Rényi graph configuration and it is represented by an adjacency matrix \\( \\textbf{W} \\) of size \\( N \\times N \\) with values drawn from a uniform random distribution on the interval \\( [-1, 1] \\) [5]. This is the most important aspect of the ESN, so in order to build one in an efficient manner we must first understand all of its components.\n The size \\( N \\) is of course the single most important one: the more challenging the task, the bigger the size should be. Of course a bigger matrix will mean more computational time so the advice of Lukoševičius is to start small and then scale. The sparsity of the reservoir. In most papers we see that each reservoir node is connected to a small number of other nodes, ranging from 5 to 12. The sparseness, beside theoretical implications, is also useful to speed up computations. Spectral radius. After the generation of a random sparse reservoir matrix, its spectral radius \\( \\rho (\\textbf{W}) \\) is computed and \\( \\textbf{W} \\) is divided by it. This allows us to obtain a matrix with a unit spectral radius, that can be scaled to a more suited value. Altough there are exceptions (when the inputs \\( \\textbf{u}(t) \\) are non-zero for example), a spectral radius smaller than unity \\( \\rho (\\textbf{W})  After the construction of the input layer and the reservoir we can focus on harvesting the states. The update equations of the ESN are:\n$$\\textbf{x}(t+\\Delta t) = (1-\\alpha) \\textbf{x}(t)+\\alpha f( \\textbf{W} \\textbf{x}(t)+ \\textbf{W}_{\\text{in}} \\textbf{u}(t))$$\n$$\\textbf{v}(t+\\Delta t) = g( \\textbf{W}_{\\text{out}} \\textbf{x}(t))$$\nwhere\n \\( \\textbf{v}(t) \\in R^{M} \\) is the predicted output \\( \\textbf{x}(t) \\in R^{N} \\) is the state vector \\( \\textbf{W}_{\\text{out}} \\in R^{M \\times N} \\) is the output layer \\( f \\) and \\( g \\) are two activation functions, most commonly the hyperbolic tangent and identity respectively \\( \\alpha \\) is the leaking rate  The computation of \\( \\textbf{W}_{\\text{out}} \\) can be expressed in terms of solving a system of linear equations\n$$\\textbf{Y}^{\\text{target}}=\\textbf{W}_{\\text{out}} \\textbf{X}$$\nwhere \\( \\textbf{X} \\) is the states matrix, built using the single states vector \\( \\textbf{x}(t) \\) as column for every \\( t=1, …, T \\), and \\( \\textbf{Y}^{\\text{target}} \\) is built in the same way only using \\( \\textbf{y}^{\\text{target}}(t) \\). The chosen solution for this problem is usually the Tikhonov regularization, also called ridge regression which has the following close form:\n$$\\textbf{W}_{\\text{out}} = \\textbf{Y}^{\\text{target}} \\textbf{X}^{\\text{T}}(\\textbf{X} \\textbf{X}^{\\text{T}} + \\beta \\textbf{I})^{-1}$$\nwhere \\( \\textbf{I} \\) is the identity matrix and \\(\\beta \\) is a regularization coefficient.\nAfter the training of the ESN, the prediction phase uses the same update equations showed above, but the input \\( \\textbf{u}(t) \\) is represented by the computed output \\( \\textbf{v}(t-\\Delta t) \\) of the preceding step.\nIn short this is the core theory behind the ESNs. In order to visualize how they work let’s look at an example.\nLorenz system prediction This is a task already tackled in literature, so our intent is to try and replicate the results found in [10]. This example can be followed in its entirety here. In this section we will just give part of the code to illustrate the theory explained above, so some important parts are not displayed.\nSupposing we have already created the train data, constituted by 5000 timesteps of the chaotic Lorenz system, we are going to use the same parameters found in the paper:\nusing ReservoirComputing approx_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 0.0 alpha = 1.0 nla_type = NLAT2() extended_states = false The ESN can easily be called in the following way:\nesn = ESN(approx_res_size,  train,  degree,  radius,  activation, #default = tanh  alpha, #default = 1.0  sigma, #default = 0.1  nla_type #default = NLADefault(),  extended_states #default = false  ) The training and the prediction, for 1250 timestps, are carried out as follows\nW_out = ESNtrain(esn, beta) output = ESNpredict(esn, predict_len, W_out) In order to visualize the solution we can plot the individual trajectories\nand the attractors\nAs expected the short term predictions are very good, and in the long term the behaviour of the system is mantained.\nBut what happens if the parameters are not ideal? In the paper is given an example where the spectral radius is bigger than the ideal value and the predictions are compromised as a result. We can also show that if the value is less than one, as suggested in order to mantain the echo state property, the results are nowhere near optimal.\nradius = 0.8 While incrementing the reservoir size is known to improve the results, up to a certain point, a smaller one will almost surely worsen them.\napprox_res_size = 60 As we can see, the choice of the parameters is of the upmost importance in this model, as it is in most models in the field of Machine Learning. There are ways of searching the optimal parameters in the state space such as grid search or random search, though experience with the model will give you the ability to know what to thinker with most of the times.\nIf you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!\nReferences [1]Jaeger, Herbert. “The “echo state” approach to analysing and training recurrent neural networks-with an erratum note.” Bonn, Germany: German National Research Center for Information Technology GMD Technical Report 148.34 (2001): 13.\n[2]Maass W, Natschläger T, Markram H. Real-time computing without stable states: a new framework for neural computation based on perturbations. Neural Comput. 2002;14(11):2531‐2560.\n[3]Konkoli Z.: Reservoir Computing. In: Meyers R. (eds) Encyclopedia of Complexity and Systems Science. Springer, Berlin, Heidelberg (2017)\n[4]Schiller, Ulf D., and Jochen J. Steil. “Analyzing the weight dynamics of recurrent learning algorithms.” Neurocomputing 63 (2005): 5-23.\n[5]Chattopadhyay, Ashesh, et al. “Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods: Reservoir computing, ANN, and RNN-LSTM.” arXiv preprint arXiv:1906.08829 (2019).\n[6]Yildiz, Izzet B., Herbert Jaeger, and Stefan J. Kiebel. “Re-visiting the echo state property.” Neural networks 35 (2012): 1-9.\n[7]Lukoševičius, Mantas. “A practical guide to applying echo state networks.” Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 659-686.\n[8]Lukoševičius, Mantas, and Herbert Jaeger. “Reservoir computing approaches to recurrent neural network training.” Computer Science Review 3.3 (2009): 127-149.\n[9]Lu, Zhixin, et al. “Reservoir observers: Model-free inference of unmeasured variables in chaotic systems.” Chaos: An Interdisciplinary Journal of Nonlinear Science 27.4 (2017): 041102.\n[10]Pathak, Jaideep, et al. “Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data.” Chaos: An Interdisciplinary Journal of Nonlinear Science 27.12 (2017): 121102.\n","wordCount":"1774","inLanguage":"en","datePublished":"2020-05-26T22:17:09+02:00","dateModified":"2020-05-26T22:17:09+02:00","author":{"@type":"Person","name":"Francesco Martinuzzi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://martinuzzifrancesco.github.io/posts/a-brief-introduction-to-reservoir-computing/"},"publisher":{"@type":"Organization","name":"Francesco Martinuzzi","logo":{"@type":"ImageObject","url":"https://martinuzzifrancesco.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://martinuzzifrancesco.github.io/ accesskey=h title="Francesco Martinuzzi (Alt + H)">Francesco Martinuzzi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://martinuzzifrancesco.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://martinuzzifrancesco.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://martinuzzifrancesco.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>A brief introduction to Reservoir Computing</h1><div class=post-meta><span title='2020-05-26 22:17:09 +0200 CEST'>May 26, 2020</span>&nbsp;·&nbsp;Francesco Martinuzzi</div></header><div class=post-content><p>This post is meant to work as an high level introduction to the concept of Reservoir Computing, using the Julia package <a href=https://github.com/SciML/ReservoirComputing.jl>ReservoirComputing.jl</a> as example tool. This package is a work in progress and it is currently the main project I am working on as part of the <a href=https://summerofcode.withgoogle.com/projects/#5374375945043968>Google Summer of Code</a> program. Future posts are going to further explain the various implementations and improvements to the code by means of comparisons with the literature and examples.</p><h1 id=what-is-reservoir-computing>What is Reservoir Computing?<a hidden class=anchor aria-hidden=true href=#what-is-reservoir-computing>#</a></h1><p>Reservoir Computing is an umbrella term used to identify a general framework of computation derived from Recurrent Neural Networks (RNN), indipendently developed by Jaeger <a href=#1>[1]</a> and Maass et al. <a href=#2>[2]</a>. These papers introduced the concepts of Echo State Networks (ESN) and Liquid State Machines (LSM) respectively. Further improvements over these two models constitute what is now called the field of Reservoir Computing. The main idea lies in leveraging a fixed non-linear system, of higher dimension than the input, onto which to input signal is mapped. After this mapping is only necessary to use a simple readout layer to harvest the state of the reservoir and to train it to the desired output. In principle, given a complex enough system, this architecture should be capable of any computation <a href=#3>[3]</a>. The intuition was born from the fact that in training RNNs most of the times the weights showing most change were the ones in the last layer <a href=#4>[4]</a>. In the next section we will also see that ESNs actually use a fixed random RNN as the reservoir. Given the static nature of this implementation usually ESNs can yield faster results and in some cases even better, in particular when dealing with chaotic time series predictions <a href=#5>[5]</a>.</p><p>But not every complex system is suited to be a good reservoir. A good reservoir is one that is able to separate inputs; different external inputs should drive the system to different regions of the configuration space <a href=#3>[3]</a>. This is called the <em>separability condition</em>. Furthermore an important property for the reservoirs of ESNs is the <em>Echo State property</em> which states that inputs to the reservoir echo in the system forever, or util they dissipate. A more formal definition of this property can be found in <a href=#6>[6]</a>.</p><p>In order to better show the inner workings of models of this family I am going to explain in mathematical details the ESN, a model that is already implemented in the package, so it will be useful for making examples.</p><h1 id=echo-state-networks>Echo State Networks<a hidden class=anchor aria-hidden=true href=#echo-state-networks>#</a></h1><h2 id=theoretical-background>Theoretical Background<a hidden class=anchor aria-hidden=true href=#theoretical-background>#</a></h2><p>This intends to be a quick overview of the theory behind the ESN to get the reader acquainted with the concepts and workings of this particular model, and it is by no means comprehensive. For in depth reviews and explanations please refer to <a href=#7>[7]</a> and <a href=#8>[8]</a>. All of the information laid out in this section is adapted from these two sources, unless stated otherwise.</p><p>Let us suppose we have an input signal \( \textbf{u}(t) \in R^M \) where \( t = 1, &mldr;, T \) is the discrete time and \( T \) the number of data points in the training set. In order to project this input onto the reservoir we will need an input to reservoir coupler, identified by the matrix \( \textbf{W}_{\text{in}} \in R^{N \times M} \). Usually this matrix is built in the same way the reservior is, and at the same time. For the implementation used in ReservoirComputing.jl we have followed the same construction proposed in <a href=#9>[9]</a> where the i-th of the \( M \) input signals is connected to \( N/M \) reservoir nodes with connection weights in the i-th column of \( \textbf{W}_{\text{in}} \). The non-zero elements are chosen randomly from a uniform distribution and then scaled in the range \( [-\sigma , \sigma ] \).</p><p>The reservoir is constitued by \( N \) neurons connected in a Erdős–Rényi graph configuration and it is represented by an adjacency matrix \( \textbf{W} \) of size \( N \times N \) with values drawn from a uniform random distribution on the interval \( [-1, 1] \) <a href=5>[5]</a>. This is the most important aspect of the ESN, so in order to build one in an efficient manner we must first understand all of its components.</p><ul><li>The size \( N \) is of course the single most important one: the more challenging the task, the bigger the size should be. Of course a bigger matrix will mean more computational time so the advice of Lukoševičius is to start small and then scale.</li><li>The sparsity of the reservoir. In most papers we see that each reservoir node is connected to a small number of other nodes, ranging from 5 to 12. The sparseness, beside theoretical implications, is also useful to speed up computations.</li><li>Spectral radius. After the generation of a random sparse reservoir matrix, its spectral radius \( \rho (\textbf{W}) \) is computed and \( \textbf{W} \) is divided by it. This allows us to obtain a matrix with a unit spectral radius, that can be scaled to a more suited value. Altough there are exceptions (when the inputs \( \textbf{u}(t) \) are non-zero for example), a spectral radius smaller than unity \( \rho (\textbf{W}) &lt; 1 \) ensures the echo state property. More generally this parameter should be selected to maximize the performance, keeping the unitary value as a useful reference point.</li></ul><p>After the construction of the input layer and the reservoir we can focus on harvesting the states. The update equations of the ESN are:</p><p>$$\textbf{x}(t+\Delta t) = (1-\alpha) \textbf{x}(t)+\alpha f( \textbf{W} \textbf{x}(t)+ \textbf{W}_{\text{in}} \textbf{u}(t))$$</p><p>$$\textbf{v}(t+\Delta t) = g( \textbf{W}_{\text{out}} \textbf{x}(t))$$</p><p>where</p><ul><li>\( \textbf{v}(t) \in R^{M} \) is the predicted output</li><li>\( \textbf{x}(t) \in R^{N} \) is the state vector</li><li>\( \textbf{W}_{\text{out}} \in R^{M \times N} \) is the output layer</li><li>\( f \) and \( g \) are two activation functions, most commonly the hyperbolic tangent and identity respectively</li><li>\( \alpha \) is the leaking rate</li></ul><p>The computation of \( \textbf{W}_{\text{out}} \) can be expressed in terms of solving a system of linear equations</p><p>$$\textbf{Y}^{\text{target}}=\textbf{W}_{\text{out}} \textbf{X}$$</p><p>where \( \textbf{X} \) is the states matrix, built using the single states vector \( \textbf{x}(t) \) as column for every \( t=1, &mldr;, T \), and \( \textbf{Y}^{\text{target}} \) is built in the same way only using \( \textbf{y}^{\text{target}}(t) \). The chosen solution for this problem is usually the <a href=https://en.wikipedia.org/wiki/Tikhonov_regularization>Tikhonov regularization</a>, also called ridge regression which has the following close form:</p><p>$$\textbf{W}_{\text{out}} = \textbf{Y}^{\text{target}} \textbf{X}^{\text{T}}(\textbf{X} \textbf{X}^{\text{T}} + \beta \textbf{I})^{-1}$$</p><p>where \( \textbf{I} \) is the identity matrix and \(\beta \) is a regularization coefficient.</p><p>After the training of the ESN, the prediction phase uses the same update equations showed above, but the input \( \textbf{u}(t) \) is represented by the computed output \( \textbf{v}(t-\Delta t) \) of the preceding step.</p><p>In short this is the core theory behind the ESNs. In order to visualize how they work let&rsquo;s look at an example.</p><h2 id=lorenz-system-prediction>Lorenz system prediction<a hidden class=anchor aria-hidden=true href=#lorenz-system-prediction>#</a></h2><p>This is a task already tackled in literature, so our intent is to try and replicate the results found in <a href=10>[10]</a>. This example can be followed in its entirety <a href=https://github.com/SciML/ReservoirComputing.jl/blob/master/examples/lorenz_example.jl>here</a>. In this section we will just give part of the code to illustrate the theory explained above, so some important parts are not displayed.</p><p>Supposing we have already created the train data, constituted by 5000 timesteps of the chaotic Lorenz system, we are going to use the same parameters found in the paper:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> ReservoirComputing
</span></span><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>degree <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLAT2()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span></code></pre></div><p>The ESN can easily be called in the following way:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(approx_res_size,
</span></span><span style=display:flex><span>    train,
</span></span><span style=display:flex><span>    degree,
</span></span><span style=display:flex><span>    radius,
</span></span><span style=display:flex><span>    activation, <span style=color:#75715e>#default = tanh</span>
</span></span><span style=display:flex><span>    alpha, <span style=color:#75715e>#default = 1.0</span>
</span></span><span style=display:flex><span>    sigma, <span style=color:#75715e>#default = 0.1</span>
</span></span><span style=display:flex><span>    nla_type <span style=color:#75715e>#default = NLADefault(),</span>
</span></span><span style=display:flex><span>    extended_states <span style=color:#75715e>#default = false</span>
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>The training and the prediction, for 1250 timestps, are carried out as follows</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>W_out <span style=color:#f92672>=</span> ESNtrain(esn, beta)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> ESNpredict(esn, predict_len, W_out)
</span></span></code></pre></div><p>In order to visualize the solution we can plot the individual trajectories</p><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/81470264-42f5c800-91ea-11ea-98a2-a8a8d7d96155.png alt=lorenz_coord></p><p>and the attractors</p><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/81470281-5a34b580-91ea-11ea-9eea-d2b266da19f4.png alt=lorenz_attractor></p><p>As expected the short term predictions are very good, and in the long term the behaviour of the system is mantained.</p><p>But what happens if the parameters are not ideal? In the paper is given an example where the spectral radius is bigger than the ideal value and the predictions are compromised as a result. We can also show that if the value is less than one, as suggested in order to mantain the echo state property, the results are nowhere near optimal.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.8</span>
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/83355574-91693300-a360-11ea-9794-cdbc9dc5388c.png alt=lorenz_coord></p><p>While incrementing the reservoir size is known to improve the results, up to a certain point, a smaller one will almost surely worsen them.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>60</span>
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/83355661-279d5900-a361-11ea-864c-fefc570eb6a1.png alt=lorenz_coord></p><p>As we can see, the choice of the parameters is of the upmost importance in this model, as it is in most models in the field of Machine Learning. There are ways of searching the optimal parameters in the state space such as grid search or random search, though experience with the model will give you the ability to know what to thinker with most of the times.</p><p>If you have any questions regarding the model, the package or you have found errors in my post, please don&rsquo;t hesitate to contact me!</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1]</p><p>Jaeger, Herbert. &ldquo;The “echo state” approach to analysing and training recurrent neural networks-with an erratum note.&rdquo; Bonn, Germany: German National Research Center for Information Technology GMD Technical Report 148.34 (2001): 13.</p><p>[2]
Maass W, Natschläger T, Markram H. Real-time computing without stable states: a new framework for neural computation based on perturbations. Neural Comput. 2002;14(11):2531‐2560.</p><p>[3]
Konkoli Z.: Reservoir Computing. In: Meyers R. (eds) Encyclopedia of Complexity and Systems Science. Springer, Berlin, Heidelberg (2017)</p><p>[4]
Schiller, Ulf D., and Jochen J. Steil. &ldquo;Analyzing the weight dynamics of recurrent learning algorithms.&rdquo; Neurocomputing 63 (2005): 5-23.</p><p>[5]
Chattopadhyay, Ashesh, et al. &ldquo;Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods: Reservoir computing, ANN, and RNN-LSTM.&rdquo; arXiv preprint arXiv:1906.08829 (2019).</p><p>[6]
Yildiz, Izzet B., Herbert Jaeger, and Stefan J. Kiebel. &ldquo;Re-visiting the echo state property.&rdquo; Neural networks 35 (2012): 1-9.</p><p>[7]
Lukoševičius, Mantas. &ldquo;A practical guide to applying echo state networks.&rdquo; Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 659-686.</p><p>[8]
Lukoševičius, Mantas, and Herbert Jaeger. &ldquo;Reservoir computing approaches to recurrent neural network training.&rdquo; Computer Science Review 3.3 (2009): 127-149.</p><p>[9]
Lu, Zhixin, et al. &ldquo;Reservoir observers: Model-free inference of unmeasured variables in chaotic systems.&rdquo; Chaos: An Interdisciplinary Journal of Nonlinear Science 27.4 (2017): 041102.</p><p>[10]
Pathak, Jaideep, et al. &ldquo;Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data.&rdquo; Chaos: An Interdisciplinary Journal of Nonlinear Science 27.12 (2017): 121102.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://martinuzzifrancesco.github.io/>Francesco Martinuzzi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>