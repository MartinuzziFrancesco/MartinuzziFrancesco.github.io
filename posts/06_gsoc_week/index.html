<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GSoC week 6: minimum complexity echo state network | Francesco Martinuzzi</title><meta name=keywords content><meta name=description content="Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning. And even when the optimal parameters are found, the prediction is run-dependent and can show different results with different generations of the reservoir. Is this the only way possible to contruct an Echo State Network (ESN)? Is there a deterministic way to build a ESN? These are the question posed in [1], and the following post is an illustration of the implementation in ReservoirComputing."><meta name=author content="Francesco Martinuzzi"><link rel=canonical href=https://martinuzzifrancesco.github.io/posts/06_gsoc_week/><link crossorigin=anonymous href=/assets/css/stylesheet.849fd8bd636f9bdcc8fd3087509c431a61faae78bd735e7ba75e6fd13ec64f83.css integrity="sha256-hJ/YvWNvm9zI/TCHUJxDGmH6rni9c157p15v0T7GT4M=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://martinuzzifrancesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://martinuzzifrancesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://martinuzzifrancesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://martinuzzifrancesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://martinuzzifrancesco.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="GSoC week 6: minimum complexity echo state network"><meta property="og:description" content="Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning. And even when the optimal parameters are found, the prediction is run-dependent and can show different results with different generations of the reservoir. Is this the only way possible to contruct an Echo State Network (ESN)? Is there a deterministic way to build a ESN? These are the question posed in [1], and the following post is an illustration of the implementation in ReservoirComputing."><meta property="og:type" content="article"><meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/06_gsoc_week/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-07-12T14:37:22+02:00"><meta property="article:modified_time" content="2020-07-12T14:37:22+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GSoC week 6: minimum complexity echo state network"><meta name=twitter:description content="Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning. And even when the optimal parameters are found, the prediction is run-dependent and can show different results with different generations of the reservoir. Is this the only way possible to contruct an Echo State Network (ESN)? Is there a deterministic way to build a ESN? These are the question posed in [1], and the following post is an illustration of the implementation in ReservoirComputing."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://martinuzzifrancesco.github.io/posts/"},{"@type":"ListItem","position":2,"name":"GSoC week 6: minimum complexity echo state network","item":"https://martinuzzifrancesco.github.io/posts/06_gsoc_week/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GSoC week 6: minimum complexity echo state network","name":"GSoC week 6: minimum complexity echo state network","description":"Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning. And even when the optimal parameters are found, the prediction is run-dependent and can show different results with different generations of the reservoir. Is this the only way possible to contruct an Echo State Network (ESN)? Is there a deterministic way to build a ESN? These are the question posed in [1], and the following post is an illustration of the implementation in ReservoirComputing.","keywords":[],"articleBody":"Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning. And even when the optimal parameters are found, the prediction is run-dependent and can show different results with different generations of the reservoir. Is this the only way possible to contruct an Echo State Network (ESN)? Is there a deterministic way to build a ESN? These are the question posed in [1], and the following post is an illustration of the implementation in ReservoirComputing.jl of their construction of a deterministic input layer and three reservoirs. As always we will quickly lay out the theory, then an example will be given.\nMinimum complexity reservoir and input layer The usual construction of a reservoir implies the creation of a random sparse matrix, with given sparsity and dimension, and following rescaling of the values in order to have set the spectral radius to be under a determined value, usually one, in order to ensure the Echo State Property (ESP) [2]. As already stated in the work done in the 4th week, this construction, although efficient, could have some downsides. The particular problem we want to solve with the current implementation is the one given by the randomness of the process: both the reservoir and the input layer construction are initially generated as random and later rescaled. The paper we are following for a possible solution [1] introduces three different constructions for a deterministic reservoir:\nDelay Line Reservoir (DLR): is composed of units organized in a line. The elements of the lower subdiagonal of the reservoir matrix have non-zero values, and all are the same. DLR with backward connections (DLRB): based on the DLR each reservoir unit is also connected to the preceding neuron. This is obtained setting as non-zero the elements of both the upper and lower subdiagonal, with two different values. Simple Cycle Reservoir (SCR): is composed by units organized in a cycle. The non-zero elements of the reservoir are the lower subdiagonal and the upper right corner, all set to the same weight. In addition to these reservoirs, also a contruction for the input layer is given: all input connections have the same absolute weight and the sign of each value is determined randomly by a draw from a Bernoulli distribution of mean 1/2. In the paper is stated that any other imposition of sign over the input weight deteriorates the results, so a little randomness is manteined even in this construction, but of course is still far from the original implementation.\nImplementation in ReservoirComputing The implementation of the construction of reservoir and input layer as described in the paper is straightforward: following the instructions we created three different functions for the reservoir named DLR(), DLRB() and SCR() that take as input\nres_size the size of the reservoir weight the value for the weights fb_weight the value for the feedback weights, only needed for the DLRB() function. The result of each function is a reservoir matrix with the requested construction. In addition we also added a min_complex_input function, taking as input\nres_size the size of the reservoir in_size the size of the input array weight the value of the weights and giving as output the minimum complexity input layer.\nExample For this example we are goind to use the Henon map, defined as $$x_{x+1} = 1 - ax_n^2 + y_n$$ $$ y_{n+1} = bx_n $$\nThe attractor depends on the two values \\( a, b \\) and shows chaotic behaviour for the classical values of \\( a=1.4 \\) and \\( b=0.3 \\).\nTo obtain a dataset for the Henon map this time we will use the DynamicalSystems package. Before starting the work we will need to download all the necessary utilies and import them:\nusing Pkg Pkg.add(\"ReservoirComputing\") Pkg.add(\"Plots\") Pkg.add(\"DynamicalSystems\") Pkg.add(\"Statistics\") Pkg.add(\"LinearAlgebra\") Pkg.add(\"Random\") using ReservoirComputing using Plots using DynamicalSystems using Statistics using LinearAlgebra using Random Now we can generate the Henon map, which will be shifted by -0.5 and scaled by 2, in order to have consistency with the paper. At the same time we are going to wash out any initial transient and construct the training, train, and testing, test, datasets, following the values given by the paper:\nds = Systems.henon() traj = trajectory(ds, 7000) data = Matrix(traj)' data = (data .-0.5) .* 2 shift = 200 train_len = 2000 predict_len = 3000 train = data[:, shift:shift+train_len-1] test = data[:, shift+train_len:shift+train_len+predict_len-1] One step ahead prediction Now we can set the parameters for the construction of the ESN, for which we followed closely the ones given in the paper, outside for the ridge regression value. Note that since some values are corresponding to our default (activation function, alpha and non linear algorithm) we will omit them for clarity.\napprox_res_size = 100 radius = 0.3 sparsity = 0.5 sigma = 1.0 beta = 1*10^(-1) extended_states = true input_weight = 0.95 r= 0.95 b = 0.05 We can now build both the standard ESN and three other ESNs based on the novel reservoir implementation. We are going to need the four of them for a comparison of the results:\nRandom.seed!(17) #fixed seed for reproducibility @time W = init_reservoir_givensp(approx_res_size, radius, sparsity) W_in = init_dense_input_layer(approx_res_size, size(train, 1), sigma) esn = ESN(W, train, W_in, extended_states = extended_states) Winmc = min_complex_input(size(train, 1), approx_res_size, input_weight) @time Wscr = SCR(approx_res_size, r) esnscr = ESN(Wscr, train, Winmc, extended_states = extended_states) @time Wdlrb = DLRB(approx_res_size, r, b) esndlrb = ESN(Wdlrb, train, Winmc, extended_states = extended_states) @time Wdlr = DLR(approx_res_size, r) esndlr = ESN(Wdlr, train, Winmc, extended_states = extended_states) 0.012062 seconds (33 allocations: 359.922 KiB) 0.000020 seconds (6 allocations: 78.359 KiB) 0.000019 seconds (6 allocations: 78.359 KiB) 0.000019 seconds (6 allocations: 78.359 KiB) In order to test the accuracy of the predictions given by different architectures we are going to use the Normalized Mean Square Error (NMSE), defined as $$NMSE = \\frac{\u003c||\\hat{y}(t)-y(t)||^2\u003e}{\u003c||y(t)-||^2\u003e}$$ where \\( \\hat{y}(t) \\) is the readout output, \\( y(t) \\) is the target output, \\( \u003c\\cdot\u003e \\) indicates the empirical mean and \\( ||\\cdot|| \\) is the Euclidean norm. A simple NMSE function is created:\nfunction NMSE(target, output) num = 0.0 den = 0.0 sums = [] for i=1:size(target, 1) append!(sums, sum(target[i,:])) end for i=1:size(target, 2) num += norm(output[:,i]-target[:,i])^2.0 den += norm(target[:,i]-sums./size(target, 2))^2.0 end nmse = (num/size(target, 2))/(den/size(target, 2)) return nmse end Now we can iterate and test the output of all the different implementations in a one step ahead prediction task:\nesns = [esn, esndlr, esndlrb, esnscr] for i in esns W_out = ESNtrain(i, beta) output = ESNpredict_h_steps(i, predict_len, 1, test, W_out) println(NMSE(test, output)) end 0.000766235182367319 0.0013015853534120024 0.0011355988458350088 0.001843450482139491 The standard ESN shows the best results, but the NMSE given by the minimum complexity ESNs are actually not bad. The results are better than those presented in the paper for all the architectures so they are not directly comparable, but the best performing ESN between the minimum complexity ones seems to be the DLRB-based, something that is also true in the paper.\nAttractor reconstruction Now we want to venture into something that is not done in the paper: we want to see if this deterministic implementation of reservoirs and input layers are capable of reconstructing the Henon attractor. We will use the ESNs already built and we will predict the system for predict_len steps to see if the behaviour is manteined. We will do so only through an eye test, but it should suffice to have a general idea of the capabilities of these reservoirs.\nTo start we will plot the actual data, in order to have something to compare the resuls to:\nscatter(test[1,:], test[2,:], label=\"actual\") Now let’s see if the standard ESN is able to predict correctly this attractor\nwout = ESNtrain(esn, beta) output = ESNpredict(esn, predict_len, wout) scatter(output[1,:], output[2,:], label=\"ESN\") Not bad, but we already know the capabilities of the ESN. We are here to test the minimum complexity construction, so let us start with DLR\nwout = ESNtrain(esndlr, beta) output = ESNpredict(esndlr, predict_len, wout) scatter(output[1,:], output[2,:], label=\"ESN-DLR\") The predictions are not as clear cut as we would like, but the behaviour is manteined nevertheless. Actually impressive considering the simple construction of the reservoir. Trying the two other constructions gives the following:\nwout = ESNtrain(esndlrb, beta) output = ESNpredict(esndlrb, predict_len, wout) scatter(output[1,:], output[2,:], label=\"ESN-DLRB\") wout = ESNtrain(esnscr, beta) output = ESNpredict(esnscr, predict_len, wout) scatter(output[1,:], output[2,:], label=\"ESN-SCR\") The results are somewhat similar between each other, and a deeper quantitative analysis is needed to determine the best performing construction, but this was not the aim of this post. We wanted to see if these basic implementations of reservoirs and input layers were capable not only of maintaining a short term prediction capability, but also if they were still able to mimic the behaviour of a chaotic attractor in the long term and it seems that both of these statements are proven to be correct. This seminal paper not only sheds light on the still inexplored possibilities of ESN reservoir constructions, but also shows that very little complexity is needed for this model to obtain very good results in a short amount of time.\nAs always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!\nCiting Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:\n@article{JMLR:v23:22-0611, author = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}, title = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}, journal = {Journal of Machine Learning Research}, year = {2022}, volume = {23}, number = {288}, pages = {1--8}, url = {http://jmlr.org/papers/v23/22-0611.html} } Documentation [1] Rodan, Ali, and Peter Tino. “Minimum complexity echo state network.” IEEE transactions on neural networks 22.1 (2010): 131-144.\n[2] Yildiz, Izzet B., Herbert Jaeger, and Stefan J. Kiebel. “Re-visiting the echo state property.” Neural networks 35 (2012): 1-9.\n","wordCount":"1653","inLanguage":"en","datePublished":"2020-07-12T14:37:22+02:00","dateModified":"2020-07-12T14:37:22+02:00","author":{"@type":"Person","name":"Francesco Martinuzzi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://martinuzzifrancesco.github.io/posts/06_gsoc_week/"},"publisher":{"@type":"Organization","name":"Francesco Martinuzzi","logo":{"@type":"ImageObject","url":"https://martinuzzifrancesco.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://martinuzzifrancesco.github.io/ accesskey=h title="Francesco Martinuzzi (Alt + H)">Francesco Martinuzzi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://martinuzzifrancesco.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://martinuzzifrancesco.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://martinuzzifrancesco.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>GSoC week 6: minimum complexity echo state network</h1><div class=post-meta><span title='2020-07-12 14:37:22 +0200 CEST'>July 12, 2020</span>&nbsp;·&nbsp;Francesco Martinuzzi</div></header><div class=post-content><p>Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning. And even when the optimal parameters are found, the prediction is run-dependent and can show different results with different generations of the reservoir. Is this the only way possible to contruct an Echo State Network (ESN)? Is there a deterministic way to build a ESN? These are the question posed in <a href=#1>[1]</a>, and the following post is an illustration of the implementation in ReservoirComputing.jl of their construction of a deterministic input layer and three reservoirs. As always we will quickly lay out the theory, then an example will be given.</p><h1 id=minimum-complexity-reservoir-and-input-layer>Minimum complexity reservoir and input layer<a hidden class=anchor aria-hidden=true href=#minimum-complexity-reservoir-and-input-layer>#</a></h1><p>The usual construction of a reservoir implies the creation of a random sparse matrix, with given sparsity and dimension, and following rescaling of the values in order to have set the spectral radius to be under a determined value, usually one, in order to ensure the Echo State Property (ESP) <a href=#2>[2]</a>. As already stated in the work done in the <a href=https://martinuzzifrancesco.github.io/posts/04_gsoc_week/>4th week</a>, this construction, although efficient, could have some downsides. The particular problem we want to solve with the current implementation is the one given by the randomness of the process: both the reservoir and the input layer construction are initially generated as random and later rescaled. The paper we are following for a possible solution <a href=#1>[1]</a> introduces three different constructions for a deterministic reservoir:</p><ul><li><strong>Delay Line Reservoir (DLR)</strong>: is composed of units organized in a line. The elements of the lower subdiagonal of the reservoir matrix have non-zero values, and all are the same.</li><li><strong>DLR with backward connections (DLRB)</strong>: based on the DLR each reservoir unit is also connected to the preceding neuron. This is obtained setting as non-zero the elements of both the upper and lower subdiagonal, with two different values.</li><li><strong>Simple Cycle Reservoir (SCR)</strong>: is composed by units organized in a cycle. The non-zero elements of the reservoir are the lower subdiagonal and the upper right corner, all set to the same weight.</li></ul><p>In addition to these reservoirs, also a contruction for the input layer is given: all input connections have the same absolute weight and the sign of each value is determined randomly by a draw from a Bernoulli distribution of mean 1/2. In the paper is stated that any other imposition of sign over the input weight deteriorates the results, so a little randomness is manteined even in this construction, but of course is still far from the original implementation.</p><h2 id=implementation-in-reservoircomputing>Implementation in ReservoirComputing<a hidden class=anchor aria-hidden=true href=#implementation-in-reservoircomputing>#</a></h2><p>The implementation of the construction of reservoir and input layer as described in the paper is straightforward: following the instructions we created three different functions for the reservoir named <code>DLR()</code>, <code>DLRB()</code> and <code>SCR()</code> that take as input</p><ul><li><code>res_size</code> the size of the reservoir</li><li><code>weight</code> the value for the weights</li><li><code>fb_weight</code> the value for the feedback weights, only needed for the <code>DLRB()</code> function.</li></ul><p>The result of each function is a reservoir matrix with the requested construction. In addition we also added a <code>min_complex_input</code> function, taking as input</p><ul><li><code>res_size</code> the size of the reservoir</li><li><code>in_size</code> the size of the input array</li><li><code>weight</code> the value of the weights</li></ul><p>and giving as output the minimum complexity input layer.</p><h1 id=example>Example<a hidden class=anchor aria-hidden=true href=#example>#</a></h1><p>For this example we are goind to use the <a href=https://en.wikipedia.org/wiki/H%C3%A9non_map>Henon map</a>, defined as
$$x_{x+1} = 1 - ax_n^2 + y_n$$
$$ y_{n+1} = bx_n $$</p><p>The attractor depends on the two values \( a, b \) and shows chaotic behaviour for the classical values of \( a=1.4 \) and \( b=0.3 \).</p><p>To obtain a dataset for the Henon map this time we will use the <a href=https://juliadynamics.github.io/DynamicalSystems.jl/latest/>DynamicalSystems</a> package. Before starting the work we will need to download all the necessary utilies and import them:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> Pkg
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;ReservoirComputing&#34;</span>)
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;Plots&#34;</span>)
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;DynamicalSystems&#34;</span>)
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;Statistics&#34;</span>)
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;LinearAlgebra&#34;</span>)
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;Random&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> ReservoirComputing
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> Plots
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> DynamicalSystems
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> Statistics
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> LinearAlgebra
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> Random
</span></span></code></pre></div><p>Now we can generate the Henon map, which will be shifted by -0.5 and scaled by 2, in order to have consistency with the paper. At the same time we are going to wash out any initial transient and construct the training, <code>train</code>, and testing, <code>test</code>, datasets, following the values given by the paper:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>ds <span style=color:#f92672>=</span> Systems<span style=color:#f92672>.</span>henon()
</span></span><span style=display:flex><span>traj <span style=color:#f92672>=</span> trajectory(ds, <span style=color:#ae81ff>7000</span>)
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> <span style=color:#66d9ef>Matrix</span>(traj)<span style=color:#f92672>&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> (data <span style=color:#f92672>.-</span><span style=color:#ae81ff>0.5</span>) <span style=color:#f92672>.*</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>shift <span style=color:#f92672>=</span> <span style=color:#ae81ff>200</span>
</span></span><span style=display:flex><span>train_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>2000</span>
</span></span><span style=display:flex><span>predict_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>3000</span>
</span></span><span style=display:flex><span>train <span style=color:#f92672>=</span> data[<span style=color:#f92672>:</span>, shift<span style=color:#f92672>:</span>shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>test <span style=color:#f92672>=</span> data[<span style=color:#f92672>:</span>, shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>:</span>shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>+</span>predict_len<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span></code></pre></div><h2 id=one-step-ahead-prediction>One step ahead prediction<a hidden class=anchor aria-hidden=true href=#one-step-ahead-prediction>#</a></h2><p>Now we can set the parameters for the construction of the ESN, for which we followed closely the ones given in the paper, outside for the ridge regression value. Note that since some values are corresponding to our default (activation function, alpha and non linear algorithm) we will omit them for clarity.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.3</span>
</span></span><span style=display:flex><span>sparsity <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>*</span><span style=color:#ae81ff>10</span><span style=color:#f92672>^</span>(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>input_weight <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.95</span>
</span></span><span style=display:flex><span>r<span style=color:#f92672>=</span> <span style=color:#ae81ff>0.95</span>
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.05</span>
</span></span></code></pre></div><p>We can now build both the standard ESN and three other ESNs based on the novel reservoir implementation. We are going to need the four of them for a comparison of the results:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>Random<span style=color:#f92672>.</span>seed!(<span style=color:#ae81ff>17</span>) <span style=color:#75715e>#fixed seed for reproducibility</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> W <span style=color:#f92672>=</span> init_reservoir_givensp(approx_res_size, radius, sparsity)
</span></span><span style=display:flex><span>W_in <span style=color:#f92672>=</span> init_dense_input_layer(approx_res_size, size(train, <span style=color:#ae81ff>1</span>), sigma)
</span></span><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(W, train, W_in, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Winmc <span style=color:#f92672>=</span> min_complex_input(size(train, <span style=color:#ae81ff>1</span>), approx_res_size, input_weight)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> Wscr <span style=color:#f92672>=</span> SCR(approx_res_size, r)
</span></span><span style=display:flex><span>esnscr <span style=color:#f92672>=</span> ESN(Wscr, train, Winmc, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> Wdlrb <span style=color:#f92672>=</span> DLRB(approx_res_size, r, b)
</span></span><span style=display:flex><span>esndlrb <span style=color:#f92672>=</span> ESN(Wdlrb, train, Winmc, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> Wdlr <span style=color:#f92672>=</span> DLR(approx_res_size, r)
</span></span><span style=display:flex><span>esndlr <span style=color:#f92672>=</span> ESN(Wdlr, train, Winmc, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span></code></pre></div><pre tabindex=0><code>0.012062 seconds (33 allocations: 359.922 KiB)
0.000020 seconds (6 allocations: 78.359 KiB)
0.000019 seconds (6 allocations: 78.359 KiB)
0.000019 seconds (6 allocations: 78.359 KiB)
</code></pre><p>In order to test the accuracy of the predictions given by different architectures we are going to use the Normalized Mean Square Error (NMSE), defined as
$$NMSE = \frac{&lt;||\hat{y}(t)-y(t)||^2>}{&lt;||y(t)-&lt;y(t)>||^2>}$$
where \( \hat{y}(t) \) is the readout output, \( y(t) \) is the target output, \( &lt;\cdot> \) indicates the empirical mean and \( ||\cdot|| \) is the Euclidean norm. A simple <code>NMSE</code> function is created:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>function</span> NMSE(target, output)
</span></span><span style=display:flex><span>    num <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>    den <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>    sums <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(target, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        append!(sums, sum(target[i,<span style=color:#f92672>:</span>]))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(target, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        num <span style=color:#f92672>+=</span> norm(output[<span style=color:#f92672>:</span>,i]<span style=color:#f92672>-</span>target[<span style=color:#f92672>:</span>,i])<span style=color:#f92672>^</span><span style=color:#ae81ff>2.0</span>
</span></span><span style=display:flex><span>        den <span style=color:#f92672>+=</span> norm(target[<span style=color:#f92672>:</span>,i]<span style=color:#f92672>-</span>sums<span style=color:#f92672>./</span>size(target, <span style=color:#ae81ff>2</span>))<span style=color:#f92672>^</span><span style=color:#ae81ff>2.0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    nmse <span style=color:#f92672>=</span> (num<span style=color:#f92672>/</span>size(target, <span style=color:#ae81ff>2</span>))<span style=color:#f92672>/</span>(den<span style=color:#f92672>/</span>size(target, <span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> nmse
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><p>Now we can iterate and test the output of all the different implementations in a one step ahead prediction task:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>esns <span style=color:#f92672>=</span> [esn, esndlr, esndlrb, esnscr]
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#66d9ef>in</span> esns
</span></span><span style=display:flex><span>    W_out <span style=color:#f92672>=</span> ESNtrain(i, beta)
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> ESNpredict_h_steps(i, predict_len, <span style=color:#ae81ff>1</span>, test, W_out)
</span></span><span style=display:flex><span>    println(NMSE(test, output))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><pre tabindex=0><code>0.000766235182367319
0.0013015853534120024
0.0011355988458350088
0.001843450482139491
</code></pre><p>The standard ESN shows the best results, but the NMSE given by the minimum complexity ESNs are actually not bad. The results are better than those presented in the paper for all the architectures so they are not directly comparable, but the best performing ESN between the minimum complexity ones seems to be the DLRB-based, something that is also true in the paper.</p><h2 id=attractor-reconstruction>Attractor reconstruction<a hidden class=anchor aria-hidden=true href=#attractor-reconstruction>#</a></h2><p>Now we want to venture into something that is not done in the paper: we want to see if this deterministic implementation of reservoirs and input layers are capable of reconstructing the Henon attractor. We will use the ESNs already built and we will predict the system for <code>predict_len</code> steps to see if the behaviour is manteined. We will do so only through an eye test, but it should suffice to have a general idea of the capabilities of these reservoirs.</p><p>To start we will plot the actual data, in order to have something to compare the resuls to:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>scatter(test[<span style=color:#ae81ff>1</span>,<span style=color:#f92672>:</span>], test[<span style=color:#ae81ff>2</span>,<span style=color:#f92672>:</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;actual&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/87250878-4dda0c80-c468-11ea-8b38-d7071f051363.png alt=actual></p><p>Now let&rsquo;s see if the standard ESN is able to predict correctly this attractor</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>wout <span style=color:#f92672>=</span> ESNtrain(esn, beta)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> ESNpredict(esn, predict_len, wout)
</span></span><span style=display:flex><span>scatter(output[<span style=color:#ae81ff>1</span>,<span style=color:#f92672>:</span>], output[<span style=color:#ae81ff>2</span>,<span style=color:#f92672>:</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ESN&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/87250933-8974d680-c468-11ea-9006-425439668774.png alt=ESN></p><p>Not bad, but we already know the capabilities of the ESN. We are here to test the minimum complexity construction, so let us start with DLR</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>wout <span style=color:#f92672>=</span> ESNtrain(esndlr, beta)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> ESNpredict(esndlr, predict_len, wout)
</span></span><span style=display:flex><span>scatter(output[<span style=color:#ae81ff>1</span>,<span style=color:#f92672>:</span>], output[<span style=color:#ae81ff>2</span>,<span style=color:#f92672>:</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ESN-DLR&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/87250941-9396d500-c468-11ea-910b-94ec2f5e5956.png alt=ESN-DLR></p><p>The predictions are not as clear cut as we would like, but the behaviour is manteined nevertheless. Actually impressive considering the simple construction of the reservoir. Trying the two other constructions gives the following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>wout <span style=color:#f92672>=</span> ESNtrain(esndlrb, beta)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> ESNpredict(esndlrb, predict_len, wout)
</span></span><span style=display:flex><span>scatter(output[<span style=color:#ae81ff>1</span>,<span style=color:#f92672>:</span>], output[<span style=color:#ae81ff>2</span>,<span style=color:#f92672>:</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ESN-DLRB&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/87250958-9f829700-c468-11ea-8721-194a1d1f3025.png alt=ESN-DLRB></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>wout <span style=color:#f92672>=</span> ESNtrain(esnscr, beta)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> ESNpredict(esnscr, predict_len, wout)
</span></span><span style=display:flex><span>scatter(output[<span style=color:#ae81ff>1</span>,<span style=color:#f92672>:</span>], output[<span style=color:#ae81ff>2</span>,<span style=color:#f92672>:</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ESN-SCR&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/87250962-a6a9a500-c468-11ea-962a-21ad28695afd.png alt=ESN-SCR></p><p>The results are somewhat similar between each other, and a deeper quantitative analysis is needed to determine the best performing construction, but this was not the aim of this post. We wanted to see if these basic implementations of reservoirs and input layers were capable not only of maintaining a short term prediction capability, but also if they were still able to mimic the behaviour of a chaotic attractor in the long term and it seems that both of these statements are proven to be correct. This seminal paper not only sheds light on the still inexplored possibilities of ESN reservoir constructions, but also shows that very little complexity is needed for this model to obtain very good results in a short amount of time.</p><p>As always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!</p><h2 id=citing>Citing<a hidden class=anchor aria-hidden=true href=#citing>#</a></h2><p>Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bibtex data-lang=bibtex><span style=display:flex><span><span style=color:#a6e22e>@article</span>{JMLR:v23:22-0611,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>author</span>  = <span style=color:#e6db74>{Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>title</span>   = <span style=color:#e6db74>{ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>journal</span> = <span style=color:#e6db74>{Journal of Machine Learning Research}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>year</span>    = <span style=color:#e6db74>{2022}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>volume</span>  = <span style=color:#e6db74>{23}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>number</span>  = <span style=color:#e6db74>{288}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>pages</span>   = <span style=color:#e6db74>{1--8}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>url</span>     = <span style=color:#e6db74>{http://jmlr.org/papers/v23/22-0611.html}</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=documentation>Documentation<a hidden class=anchor aria-hidden=true href=#documentation>#</a></h2><p>[1]
Rodan, Ali, and Peter Tino. &ldquo;Minimum complexity echo state network.&rdquo; IEEE transactions on neural networks 22.1 (2010): 131-144.</p><p>[2]
Yildiz, Izzet B., Herbert Jaeger, and Stefan J. Kiebel. &ldquo;Re-visiting the echo state property.&rdquo; Neural networks 35 (2012): 1-9.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://martinuzzifrancesco.github.io/>Francesco Martinuzzi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>