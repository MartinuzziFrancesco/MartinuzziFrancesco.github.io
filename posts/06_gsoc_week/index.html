<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.74.3" />

  <title>GSoC week 6: minimum complexity echo state network &middot; Francesco Martinuzzi</title>

  <meta name="description" content="" />

  

<meta itemprop="name" content="GSoC week 6: minimum complexity echo state network">
<meta itemprop="description" content="Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning.">
<meta itemprop="datePublished" content="2020-07-12T14:37:22+02:00" />
<meta itemprop="dateModified" content="2020-07-12T14:37:22+02:00" />
<meta itemprop="wordCount" content="1570">
<meta itemprop="image" content="https://martinuzzifrancesco.github.io/images/"/>



<meta itemprop="keywords" content="" />


<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://martinuzzifrancesco.github.io/images/"/>

<meta name="twitter:title" content="GSoC week 6: minimum complexity echo state network"/>
<meta name="twitter:description" content="Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning."/>


<meta property="og:title" content="GSoC week 6: minimum complexity echo state network" />
<meta property="og:description" content="Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/06_gsoc_week/" />
<meta property="og:image" content="https://martinuzzifrancesco.github.io/images/"/>
<meta property="article:published_time" content="2020-07-12T14:37:22+02:00" />
<meta property="article:modified_time" content="2020-07-12T14:37:22+02:00" /><meta property="og:site_name" content="Francesco Martinuzzi" />



  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/print.css"
        media="print">

  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/poole.css">

  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/hyde.css">

  
<style type="text/css">
  .sidebar {
    background-color: #536060;
  }

  .read-more-link a {
    border-color: #536060;
  }

  .pagination li a {
    color: #536060;
    border: 1px solid #536060;
  }

  .pagination li.active a {
    background-color: #536060;
  }

  .pagination li a:hover {
    background-color: #536060;
    opacity: 0.75;
  }

  footer a,
  .content a,
  .related-posts li a:hover {
    color: #536060;
  }
</style>



  

  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700&display=swap">

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css"
        integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk="
        crossorigin="anonymous" />

  <link rel="apple-touch-icon-precomposed"
        sizes="144x144"
        href="/apple-touch-icon-144-precomposed.png">

  <link rel="shortcut icon" href="/favicon.png">

  
  </head>
<body>
  <aside class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      
        
        <div class="author-image">
          <img src="https://martinuzzifrancesco.github.io/images/prova.jpg" class="img-circle img-headshot center" alt="Profile Picture">
        </div>
        
      

      <h1>Francesco Martinuzzi</h1>

      
      <p class="lead">Physicist learning how to make machines learn</p>
      
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li>
          <a href="https://martinuzzifrancesco.github.io/">Home</a>
        </li>
        <li>
          <a href="/posts/"> Posts </a>
        </li><li>
          <a href="/about/"> About </a>
        </li><li>
          <a href="/contact/"> Contact </a>
        </li>
      </ul>
    </nav>

    <section class="social-icons">
      
      <a href="https://github.com/MartinuzziFrancesco" rel="me" title="GitHub">
        <i class="fab fa-github" aria-hidden="true"></i>
      </a>
      
      <a href="https://twitter.com/MartinuzziFra" rel="me" title="Twitter">
        <i class="fab fa-twitter" aria-hidden="true"></i>
      </a>
      
    </section>
  </div>
</aside>


  <main class="content container">
  <div class="post">
  <h1>GSoC week 6: minimum complexity echo state network</h1>

  <div class="post-date">
    <time datetime="2020-07-12T14:37:22&#43;0200">Jul 12, 2020</time> Â· 8 min read
  </div>

  <p>Up until now we used reservoir generated mainly through a random process, and this approach requires a lot of fine parameter tuning. And even when the optimal parameters are found, the prediction is run-dependent and can show different results with different generations of the reservoir. Is this the only way possible to contruct an Echo State Network (ESN)? Is there a deterministic way to build a ESN? These are the question posed in <a href="#1">[1]</a>, and the following post is an illustration of the implementation in ReservoirComputing.jl of their construction of a deterministic input layer and three reservoirs. As always we will quickly lay out the theory, then an example will be given.</p>
<h1 id="minimum-complexity-reservoir-and-input-layer">Minimum complexity reservoir and input layer</h1>
<p>The usual construction of a reservoir implies the creation of a random sparse matrix, with given sparsity and dimension, and following rescaling of the values in order to have set the spectral radius to be under a determined value, usually one, in order to ensure the Echo State Property (ESP) <a href="#2">[2]</a>. As already stated in the work done in the <a href="https://martinuzzifrancesco.github.io/posts/04_gsoc_week/">4th week</a>, this construction, although efficient, could have some downsides. The particular problem we want to solve with the current implementation is the one given by the randomness of the process: both the reservoir and the input layer construction are initially generated as random and later rescaled. The paper we are following for a possible solution <a href="#1">[1]</a> introduces three different constructions for a deterministic reservoir:</p>
<ul>
<li><strong>Delay Line Reservoir (DLR)</strong>: is composed of units organized in a line. The elements of the lower subdiagonal of the reservoir matrix have non-zero values, and all are the same.</li>
<li><strong>DLR with backward connections (DLRB)</strong>: based on the DLR each reservoir unit is also connected to the preceding neuron. This is obtained setting as non-zero the elements of both the upper and lower subdiagonal, with two different values.</li>
<li><strong>Simple Cycle Reservoir (SCR)</strong>: is composed by units organized in a cycle. The non-zero elements of the reservoir are the lower subdiagonal and the upper right corner, all set to the same weight.</li>
</ul>
<p>In addition to these reservoirs, also a contruction for the input layer is given: all input connections have the same absolute weight and the sign of each value is determined randomly by a draw from a Bernoulli distribution of mean 1/2. In the paper is stated that any other imposition of sign over the input weight deteriorates the results, so a little randomness is manteined even in this construction, but of course is still far from the original implementation.</p>
<h2 id="implementation-in-reservoircomputing">Implementation in ReservoirComputing</h2>
<p>The implementation of the construction of reservoir and input layer as described in the paper is straightforward: following the instructions we created three different functions for the reservoir named <code>DLR()</code>, <code>DLRB()</code> and <code>SCR()</code> that take as input</p>
<ul>
<li><code>res_size</code> the size of the reservoir</li>
<li><code>weight</code> the value for the weights</li>
<li><code>fb_weight</code> the value for the feedback weights, only needed for the <code>DLRB()</code> function.</li>
</ul>
<p>The result of each function is a reservoir matrix with the requested construction. In addition we also added a <code>min_complex_input</code> function, taking as input</p>
<ul>
<li><code>res_size</code> the size of the reservoir</li>
<li><code>in_size</code> the size of the input array</li>
<li><code>weight</code> the value of the weights</li>
</ul>
<p>and giving as output the minimum complexity input layer.</p>
<h1 id="example">Example</h1>
<p>For this example we are goind to use the <a href="https://en.wikipedia.org/wiki/H%C3%A9non_map">Henon map</a>, defined as
$$x_{x+1} = 1 - ax_n^2 + y_n$$
$$ y_{n+1} = bx_n $$</p>
<p>The attractor depends on the two values \( a, b \) and shows chaotic behaviour for the classical values of \( a=1.4 \) and \( b=0.3 \).</p>
<p>To obtain a dataset for the Henon map this time we will use the <a href="https://juliadynamics.github.io/DynamicalSystems.jl/latest/">DynamicalSystems</a> package. Before starting the work we will need to download all the necessary utilies and import them:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia"><span style="color:#66d9ef">using</span> Pkg
Pkg<span style="color:#f92672">.</span>add(<span style="color:#e6db74">&#34;ReservoirComputing&#34;</span>)
Pkg<span style="color:#f92672">.</span>add(<span style="color:#e6db74">&#34;Plots&#34;</span>)
Pkg<span style="color:#f92672">.</span>add(<span style="color:#e6db74">&#34;DynamicalSystems&#34;</span>)
Pkg<span style="color:#f92672">.</span>add(<span style="color:#e6db74">&#34;Statistics&#34;</span>)
Pkg<span style="color:#f92672">.</span>add(<span style="color:#e6db74">&#34;LinearAlgebra&#34;</span>)
Pkg<span style="color:#f92672">.</span>add(<span style="color:#e6db74">&#34;Random&#34;</span>)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia"><span style="color:#66d9ef">using</span> ReservoirComputing
<span style="color:#66d9ef">using</span> Plots
<span style="color:#66d9ef">using</span> DynamicalSystems
<span style="color:#66d9ef">using</span> Statistics
<span style="color:#66d9ef">using</span> LinearAlgebra
<span style="color:#66d9ef">using</span> Random
</code></pre></div><p>Now we can generate the Henon map, which will be shifted by -0.5 and scaled by 2, in order to have consistency with the paper. At the same time we are going to wash out any initial transient and construct the training,  <code>train</code>, and testing, <code>test</code>, datasets, following the values given by the paper:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">ds <span style="color:#f92672">=</span> Systems<span style="color:#f92672">.</span>henon()
traj <span style="color:#f92672">=</span> trajectory(ds, <span style="color:#ae81ff">7000</span>)
data <span style="color:#f92672">=</span> <span style="color:#66d9ef">Matrix</span>(traj)<span style="color:#f92672">&#39;</span>

data <span style="color:#f92672">=</span> (data <span style="color:#f92672">.-</span><span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">.*</span> <span style="color:#ae81ff">2</span>
shift <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>
train_len <span style="color:#f92672">=</span> <span style="color:#ae81ff">2000</span>
predict_len <span style="color:#f92672">=</span> <span style="color:#ae81ff">3000</span>
train <span style="color:#f92672">=</span> data[<span style="color:#f92672">:</span>, shift<span style="color:#f92672">:</span>shift<span style="color:#f92672">+</span>train_len<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
test <span style="color:#f92672">=</span> data[<span style="color:#f92672">:</span>, shift<span style="color:#f92672">+</span>train_len<span style="color:#f92672">:</span>shift<span style="color:#f92672">+</span>train_len<span style="color:#f92672">+</span>predict_len<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</code></pre></div><h2 id="one-step-ahead-prediction">One step ahead prediction</h2>
<p>Now we can set the parameters for the construction of the ESN, for which we followed closely the ones given in the paper, outside for the ridge regression value. Note that since some values are corresponding to our default (activation function, alpha and non linear algorithm) we will omit them for clarity.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">approx_res_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
radius <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.3</span>
sparsity <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>
sigma <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">*</span><span style="color:#ae81ff">10</span><span style="color:#f92672">^</span>(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
extended_states <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span>

input_weight <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.95</span>
r<span style="color:#f92672">=</span> <span style="color:#ae81ff">0.95</span>
b <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.05</span>
</code></pre></div><p>We can now build both the standard ESN and three other ESNs based on the novel reservoir implementation. We are going to need the four of them for a comparison of the results:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">Random<span style="color:#f92672">.</span>seed!(<span style="color:#ae81ff">17</span>) <span style="color:#75715e">#fixed seed for reproducibility</span>
<span style="color:#a6e22e">@time</span> W <span style="color:#f92672">=</span> init_reservoir_givensp(approx_res_size, radius, sparsity)
W_in <span style="color:#f92672">=</span> init_dense_input_layer(approx_res_size, size(train, <span style="color:#ae81ff">1</span>), sigma)
esn <span style="color:#f92672">=</span> ESN(W, train, W_in, extended_states <span style="color:#f92672">=</span> extended_states)

Winmc <span style="color:#f92672">=</span> min_complex_input(size(train, <span style="color:#ae81ff">1</span>), approx_res_size, input_weight)

<span style="color:#a6e22e">@time</span> Wscr <span style="color:#f92672">=</span> SCR(approx_res_size, r)
esnscr <span style="color:#f92672">=</span> ESN(Wscr, train, Winmc, extended_states <span style="color:#f92672">=</span> extended_states)

<span style="color:#a6e22e">@time</span> Wdlrb <span style="color:#f92672">=</span> DLRB(approx_res_size, r, b)
esndlrb <span style="color:#f92672">=</span> ESN(Wdlrb, train, Winmc, extended_states <span style="color:#f92672">=</span> extended_states)

<span style="color:#a6e22e">@time</span> Wdlr <span style="color:#f92672">=</span> DLR(approx_res_size, r)
esndlr <span style="color:#f92672">=</span> ESN(Wdlr, train, Winmc, extended_states <span style="color:#f92672">=</span> extended_states)
</code></pre></div><pre><code>0.012062 seconds (33 allocations: 359.922 KiB)
0.000020 seconds (6 allocations: 78.359 KiB)
0.000019 seconds (6 allocations: 78.359 KiB)
0.000019 seconds (6 allocations: 78.359 KiB)
</code></pre><p>In order to test the accuracy of the predictions given by different architectures we are going to use the Normalized Mean Square Error (NMSE), defined as
$$NMSE = \frac{&lt;||\hat{y}(t)-y(t)||^2&gt;}{&lt;||y(t)-&lt;y(t)&gt;||^2&gt;}$$
where \( \hat{y}(t) \) is the readout output, \( y(t) \) is the target output, \( &lt;\cdot&gt; \) indicates the empirical mean and \( ||\cdot|| \) is the Euclidean norm. A simple <code>NMSE</code> function is created:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia"><span style="color:#66d9ef">function</span> NMSE(target, output)
    num <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
    den <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
    sums <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> i<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>size(target, <span style="color:#ae81ff">1</span>)
        append!(sums, sum(target[i,<span style="color:#f92672">:</span>]))
    <span style="color:#66d9ef">end</span>
    <span style="color:#66d9ef">for</span> i<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>size(target, <span style="color:#ae81ff">2</span>)
        num <span style="color:#f92672">+=</span> norm(output[<span style="color:#f92672">:</span>,i]<span style="color:#f92672">-</span>target[<span style="color:#f92672">:</span>,i])<span style="color:#f92672">^</span><span style="color:#ae81ff">2.0</span>
        den <span style="color:#f92672">+=</span> norm(target[<span style="color:#f92672">:</span>,i]<span style="color:#f92672">-</span>sums<span style="color:#f92672">./</span>size(target, <span style="color:#ae81ff">2</span>))<span style="color:#f92672">^</span><span style="color:#ae81ff">2.0</span>
    <span style="color:#66d9ef">end</span>
    nmse <span style="color:#f92672">=</span> (num<span style="color:#f92672">/</span>size(target, <span style="color:#ae81ff">2</span>))<span style="color:#f92672">/</span>(den<span style="color:#f92672">/</span>size(target, <span style="color:#ae81ff">2</span>))
    <span style="color:#66d9ef">return</span> nmse
<span style="color:#66d9ef">end</span>
</code></pre></div><p>Now we can iterate and test the output of all the different implementations in a one step ahead prediction task:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">esns <span style="color:#f92672">=</span> [esn, esndlr, esndlrb, esnscr]
<span style="color:#66d9ef">for</span> i <span style="color:#66d9ef">in</span> esns
    W_out <span style="color:#f92672">=</span> ESNtrain(i, beta)
    output <span style="color:#f92672">=</span> ESNpredict_h_steps(i, predict_len, <span style="color:#ae81ff">1</span>, test, W_out)
    println(NMSE(test, output))
<span style="color:#66d9ef">end</span>
</code></pre></div><pre><code>0.000766235182367319
0.0013015853534120024
0.0011355988458350088
0.001843450482139491
</code></pre><p>The standard ESN shows the best results, but the NMSE given by the minimum complexity ESNs are actually not bad. The results are better than those presented in the paper for all the architectures so they are not directly comparable, but the best performing ESN between the minimum complexity ones seems to be the DLRB-based, something that is also true in the paper.</p>
<h2 id="attractor-reconstruction">Attractor reconstruction</h2>
<p>Now we want to venture into something that is not done in the paper: we want to see if this deterministic implementation of reservoirs and input layers are capable of reconstructing the Henon attractor. We will use the ESNs already built and we will predict the system for <code>predict_len</code> steps to see if the behaviour is manteined. We will do so only through an eye test, but it should suffice to have a general idea of the capabilities of these reservoirs.</p>
<p>To start we will plot the actual data, in order to have something to compare the resuls to:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">scatter(test[<span style="color:#ae81ff">1</span>,<span style="color:#f92672">:</span>], test[<span style="color:#ae81ff">2</span>,<span style="color:#f92672">:</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;actual&#34;</span>)
</code></pre></div><p><img src="https://user-images.githubusercontent.com/10376688/87250878-4dda0c80-c468-11ea-8b38-d7071f051363.png" alt="actual"></p>
<p>Now let&rsquo;s see if the standard ESN is able to predict correctly this attractor</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">wout <span style="color:#f92672">=</span> ESNtrain(esn, beta)
output <span style="color:#f92672">=</span> ESNpredict(esn, predict_len, wout)
scatter(output[<span style="color:#ae81ff">1</span>,<span style="color:#f92672">:</span>], output[<span style="color:#ae81ff">2</span>,<span style="color:#f92672">:</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ESN&#34;</span>)
</code></pre></div><p><img src="https://user-images.githubusercontent.com/10376688/87250933-8974d680-c468-11ea-9006-425439668774.png" alt="ESN"></p>
<p>Not bad, but we already know the capabilities of the ESN. We are here to test the minimum complexity construction, so let us start with DLR</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">wout <span style="color:#f92672">=</span> ESNtrain(esndlr, beta)
output <span style="color:#f92672">=</span> ESNpredict(esndlr, predict_len, wout)
scatter(output[<span style="color:#ae81ff">1</span>,<span style="color:#f92672">:</span>], output[<span style="color:#ae81ff">2</span>,<span style="color:#f92672">:</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ESN-DLR&#34;</span>)
</code></pre></div><p><img src="https://user-images.githubusercontent.com/10376688/87250941-9396d500-c468-11ea-910b-94ec2f5e5956.png" alt="ESN-DLR"></p>
<p>The predictions are not as clear cut as we would like, but the behaviour is manteined nevertheless. Actually impressive considering the simple construction of the reservoir. Trying the two other constructions gives the following:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">wout <span style="color:#f92672">=</span> ESNtrain(esndlrb, beta)
output <span style="color:#f92672">=</span> ESNpredict(esndlrb, predict_len, wout)
scatter(output[<span style="color:#ae81ff">1</span>,<span style="color:#f92672">:</span>], output[<span style="color:#ae81ff">2</span>,<span style="color:#f92672">:</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ESN-DLRB&#34;</span>)
</code></pre></div><p><img src="https://user-images.githubusercontent.com/10376688/87250958-9f829700-c468-11ea-8721-194a1d1f3025.png" alt="ESN-DLRB"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">wout <span style="color:#f92672">=</span> ESNtrain(esnscr, beta)
output <span style="color:#f92672">=</span> ESNpredict(esnscr, predict_len, wout)
scatter(output[<span style="color:#ae81ff">1</span>,<span style="color:#f92672">:</span>], output[<span style="color:#ae81ff">2</span>,<span style="color:#f92672">:</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ESN-SCR&#34;</span>)
</code></pre></div><p><img src="https://user-images.githubusercontent.com/10376688/87250962-a6a9a500-c468-11ea-962a-21ad28695afd.png" alt="ESN-SCR"></p>
<p>The results are somewhat similar between each other, and a deeper quantitative analysis is needed to determine the best performing construction, but this was not the aim of this post. We wanted to see if these basic implementations of reservoirs and input layers were capable not only of maintaining a short term prediction capability, but also if they were still able to mimic the behaviour of a chaotic attractor in the long term and it seems that both of these statements are proven to be correct. This seminal paper not only sheds light on the still inexplored possibilities of ESN reservoir constructions, but also shows that very little complexity is needed for this model to obtain very good results in a short amount of time.</p>
<p>As always, if you have any questions regarding the model, the package or you have found errors in my post, please donât hesitate to contact me!</p>
<h2 id="documentation">Documentation</h2>
<p><!-- raw HTML omitted -->[1]<!-- raw HTML omitted -->
Rodan, Ali, and Peter Tino. &ldquo;Minimum complexity echo state network.&rdquo; IEEE transactions on neural networks 22.1 (2010): 131-144.</p>
<p><!-- raw HTML omitted -->[2]<!-- raw HTML omitted -->
Yildiz, Izzet B., Herbert Jaeger, and Stefan J. Kiebel. &ldquo;Re-visiting the echo state property.&rdquo; Neural networks 35 (2012): 1-9.</p>

</div>


  </main>

  <footer>
  <div>
    &copy; Martinuzzi Francesco 2020
    Â·
    
    <a href="https://creativecommons.org/licenses/by-sa/4.0"
       target="_blank">CC BY-SA 4.0</a>
    
    
  </div>
</footer>

</body>

    <script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
            fontCache: 'global'
        }
    };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script> 


</html>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/js/all.min.js"
          integrity="sha256-MAgcygDRahs+F/Nk5Vz387whB4kSK9NXlDN3w58LLq0="
          crossorigin="anonymous"></script>

  

  
</body>
</html>
