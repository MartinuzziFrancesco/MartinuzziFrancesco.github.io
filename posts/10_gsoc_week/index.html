<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GSoC week 10: Reservoir Memory Machines | Francesco Martinuzzi</title><meta name=keywords content><meta name=description content="For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year [1]. This was one of the hardest, and longest, implementations to date and I believe there is still some work to be done. In this post we will briefly touch on the theory behind the model, and after an example of their usage will be presented."><meta name=author content="Francesco Martinuzzi"><link rel=canonical href=https://martinuzzifrancesco.github.io/posts/10_gsoc_week/><link crossorigin=anonymous href=/assets/css/stylesheet.849fd8bd636f9bdcc8fd3087509c431a61faae78bd735e7ba75e6fd13ec64f83.css integrity="sha256-hJ/YvWNvm9zI/TCHUJxDGmH6rni9c157p15v0T7GT4M=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://martinuzzifrancesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://martinuzzifrancesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://martinuzzifrancesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://martinuzzifrancesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://martinuzzifrancesco.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="GSoC week 10: Reservoir Memory Machines"><meta property="og:description" content="For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year [1]. This was one of the hardest, and longest, implementations to date and I believe there is still some work to be done. In this post we will briefly touch on the theory behind the model, and after an example of their usage will be presented."><meta property="og:type" content="article"><meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/10_gsoc_week/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-08-13T14:41:28+02:00"><meta property="article:modified_time" content="2020-08-13T14:41:28+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GSoC week 10: Reservoir Memory Machines"><meta name=twitter:description content="For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year [1]. This was one of the hardest, and longest, implementations to date and I believe there is still some work to be done. In this post we will briefly touch on the theory behind the model, and after an example of their usage will be presented."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://martinuzzifrancesco.github.io/posts/"},{"@type":"ListItem","position":2,"name":"GSoC week 10: Reservoir Memory Machines","item":"https://martinuzzifrancesco.github.io/posts/10_gsoc_week/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GSoC week 10: Reservoir Memory Machines","name":"GSoC week 10: Reservoir Memory Machines","description":"For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year [1]. This was one of the hardest, and longest, implementations to date and I believe there is still some work to be done. In this post we will briefly touch on the theory behind the model, and after an example of their usage will be presented.","keywords":[],"articleBody":"For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year [1]. This was one of the hardest, and longest, implementations to date and I believe there is still some work to be done. In this post we will briefly touch on the theory behind the model, and after an example of their usage will be presented.\nTheoretical Background Born as an alternative to the Neural Turing Machine [2] the RMM is an extension of the Echo State Network model, with the addition of an actual memory \\( \\textbf{M}_t \\in \\mathbb{R}^{K \\times n} \\), a write head and a read head. The dynamics of the RMM are the following:\nIn the first step the previous memory state is copied \\( \\textbf{M}_t = \\textbf{M}_{t-1} \\), with the initial memory step being initialized to zero. The write head is then controlled by the value \\( c_t^w = \\textbf{u}^w \\textbf{x}^t + \\textbf{v}^r \\textbf{h}^t \\) where \\( \\textbf{u}^w, \\textbf{v}^r \\) are learnable parameters and \\( \\textbf{x}^t, \\textbf{h}^t \\) are the input vector and state vector at time t respectively. If \\( c_t^w \u003e 0 \\) then the input is written to memory, \\( \\textbf{m}_{t, k} = \\textbf{x}^t \\) and \\( k_t = k_{t-1}+1 \\). \\( k \\) is resetted to 1 if it exceeds the memory size \\( K \\). In the other case the memory and \\( k \\) are left as they are. Each time step the read head is controlled in a similar way using the vector \\( \\textbf{c}_t^r = \\textbf{U}^r \\textbf{x}^t + \\textbf{V}^r \\textbf{h}^t \\) where \\( \\textbf{U}^r, \\textbf{V}^r \\) are learnable parameters. If \\( c^r_{t, 2} = max{c^r_{t, 1}, c^r_{t, 2}, c^r_{t, 3}} \\) \\( l_t = l_{t-1}+1 \\), otherwise \\( l_t = 1 \\). After that the memory read at time \\( t \\) is set as the \\( l\\)th row of \\( \\textbf{M}_t \\), \\( \\textbf{r}^t = \\textbf{m}_{t, l_t} \\) The output of the system is determined by \\( \\textbf{y_t} = \\textbf{V} \\textbf{h}^t + \\textbf{R} \\textbf{r}^t \\) where \\( \\textbf{V}, \\textbf{R} \\) are learnable parameters. Setting \\( \\textbf{R} = 0 \\) we can see that the result is the standard ESN.\nFor a more detailed explanation of the procedure and of the training process please refer to the original paper.\nImplementation in ReservoirComputing.jl Following both the paper and the code provided (original in Python, click here) we were able to implement a RMM mutable struct and a RMMdirect_predict function able to train and do predictions with the RMM model. The default constructor for RMM takes as input\nW the reservoir matrix in_data the training data out_data the desired output W_in the input layer matrix memory_size the size \\( K \\) of the memory activation optional activation function for the reservoir states, with default tanh alpha optional leaking rate, with default 1.0 nla_type optional non linear algorithm, eith default NLADefault() extended_states optional boolean for the extended states option, with default false The constructor trains the RMM, so ance it is initialized there is only need for a predict function. The RMMdirect_predict takes as input\nrmmm an initialized RMM input the input data and gives as output the prediction based on the input data given. The prediction process is relatively different from the implementation used in ReservoirComputing.jl, so we will not be able to do a proper comparison with the other models we implemented. In the future we do want to uniform the RMM with the other architectures present in the library, but it seems like a moth worth of work, so for the moment we are happy with the basic implementations obtained. Examples For example we will use the next step prediction for the Henon map, used also in last week test. The map is defined as\n$$x_{x+1} = 1 - ax_n^2 + y_n$$ $$ y_{n+1} = bx_n $$\nLet us start by installing and importing all the needed packages:\nusing Pkg Pkg.add(\"ReservoirComputing\") Pkg.add(\"DynamicalSystems\") Pkg.add(\"LinearAlgebra\") using ReservoirComputing using DynamicalSystems using LinearAlgebra Now we can generate the Henon map, and we will shift the data points by -0.5 and scale them by 2 to reproduce the data we had last week. The initial transient will be washed out and we will create four datasets called train_x, train_y, test_x and test_y:`\nds = Systems.henon() traj = trajectory(ds, 7000) data = Matrix(traj) data = (data .-0.5) .* 2 shift = 200 train_len = 2000 predict_len = 3000 train_x = data[shift:shift+train_len-1, :] train_y = data[shift+1:shift+train_len, :] test_x = data[shift+train_len:shift+train_len+predict_len-1, :] test_y = data[shift+train_len+1:shift+train_len+predict_len, :] Having the needed data we can proceed to the prediction task. In the RMM paper the model is tested using Cycle Reservoirs with Regular Jumps [3] so we will do the same for our test. In addition to that we will also use the other minimum complexity reservoirs [4] that we implemented in week 6. The input layer used is obtained with the function irrational_sign_input(), that builds a fully connected layer with the same values which signs are determined by the values of an irrational number, in our case pi. Setting the parameters for the construction of the RMM\napprox_res_size = 128 sigma = 1.0 beta = 1*10^(-5) extended_states = false input_weight = 0.1 cyrcle_weight = 0.99 jump_weight = 0.1 jumps = 12 memory_size = 16 We can now build the reservoir and the RMMs needed for the comparison of the results:\nWcrj = CRJ(approx_res_size, cyrcle_weight, jump_weight, jumps) Wscr = SCR(approx_res_size, cyrcle_weight) Wdlrb = DLRB(approx_res_size, cyrcle_weight, jump_weight) Wdlr = DLR(approx_res_size, cyrcle_weight) W_in = irrational_sign_input(approx_res_size, size(train_x, 2), input_weight) rmmcrj = RMM(Wcrj, train_x, train_y, W_in, memory_size, beta) rmmscr = RMM(Wscr, train_x, train_y, W_in, memory_size, beta) rmmdlrb = RMM(Wdlrb, train_x, train_y, W_in, memory_size, beta) rmmdlr = RMM(Wdlr, train_x, train_y, W_in, memory_size, beta) Now that we have our trained RMM we want to predict the one step ahead henon map and compare the results obtained with different reservoirs. In oreder to do so we are first going to implement a quick nmse function :\nfunction NMSE(target, output) num = 0.0 den = 0.0 sums = [] for i=1:size(target, 2) append!(sums, sum(target[:, i])) end for i=1:size(target, 1) num += norm(output[i, :]-target[i, :])^2.0 den += norm(target[i, :]-sums./size(target, 1))^2.0 end nmse = (num/size(target, 1))/(den/size(target, 1)) return nmse end after that we are going to predict the system and compare the results:\nrmms = [rmmcrj, rmmscr, rmmdlrb, rmmdlr] for rmm in rmms output2 = RMMdirect_predict(rmm, test_x) println(NMSE(train_y, output2)) end 1.4856355716974217 1.4868276240921912 1.5624183281454223 1.5237046076873637 As we can see the best performing architecture is the one with the CRJ reservoir. The SCR closely follows.\nThis tests are not the one used in the paper, but given that I was a little behind with the implementation I thought to do a couple of quick and easy ones instead. The model is really interesting and I want to continue to explore the possibilities that it offers. The implementation, while working, is not yet finished: there are a couple of finishing touches to give and a couple of more checks to do. I really want to be able to reproduce the results of the paper but the base implementations of ESN in ReservoirComputing and the paper code are really different, and it will take at least another week of full work to unravel all the small details. Huge thanks to the author Benjamin Paaßen that answered quickly and kindly to my emails.\nAs always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!\nCiting Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:\n@article{JMLR:v23:22-0611, author = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}, title = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}, journal = {Journal of Machine Learning Research}, year = {2022}, volume = {23}, number = {288}, pages = {1--8}, url = {http://jmlr.org/papers/v23/22-0611.html} } Documentation [1] Paaßen, Benjamin, and Alexander Schulz. “Reservoir memory machines.” arXiv preprint arXiv:2003.04793 (2020).\n[2] Graves, Alex, Greg Wayne, and Ivo Danihelka. “Neural turing machines.” arXiv preprint arXiv:1410.5401 (2014).\n[3] Rodan, Ali, and Peter Tiňo. “Simple deterministically constructed cycle reservoirs with regular jumps.” Neural computation 24.7 (2012): 1822-1852.\n[4] Rodan, Ali, and Peter Tino. “Minimum complexity echo state network.” IEEE transactions on neural networks 22.1 (2010): 131-144.\n","wordCount":"1400","inLanguage":"en","datePublished":"2020-08-13T14:41:28+02:00","dateModified":"2020-08-13T14:41:28+02:00","author":{"@type":"Person","name":"Francesco Martinuzzi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://martinuzzifrancesco.github.io/posts/10_gsoc_week/"},"publisher":{"@type":"Organization","name":"Francesco Martinuzzi","logo":{"@type":"ImageObject","url":"https://martinuzzifrancesco.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://martinuzzifrancesco.github.io/ accesskey=h title="Francesco Martinuzzi (Alt + H)">Francesco Martinuzzi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://martinuzzifrancesco.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://martinuzzifrancesco.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://martinuzzifrancesco.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>GSoC week 10: Reservoir Memory Machines</h1><div class=post-meta><span title='2020-08-13 14:41:28 +0200 CEST'>August 13, 2020</span>&nbsp;·&nbsp;Francesco Martinuzzi</div></header><div class=post-content><p>For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year <a href=#1>[1]</a>. This was one of the hardest, and longest, implementations to date and I believe there is still some work to be done. In this post we will briefly touch on the theory behind the model, and after an example of their usage will be presented.</p><h1 id=theoretical-background>Theoretical Background<a hidden class=anchor aria-hidden=true href=#theoretical-background>#</a></h1><p>Born as an alternative to the Neural Turing Machine <a href=#2>[2]</a> the RMM is an extension of the Echo State Network model, with the addition of an actual memory \( \textbf{M}_t \in \mathbb{R}^{K \times n} \), a write head and a read head. The dynamics of the RMM are the following:</p><ul><li>In the first step the previous memory state is copied \( \textbf{M}_t = \textbf{M}_{t-1} \), with the initial memory step being initialized to zero.</li><li>The write head is then controlled by the value \( c_t^w = \textbf{u}^w \textbf{x}^t + \textbf{v}^r \textbf{h}^t \) where \( \textbf{u}^w, \textbf{v}^r \) are learnable parameters and \( \textbf{x}^t, \textbf{h}^t \) are the input vector and state vector at time t respectively. If \( c_t^w > 0 \) then the input is written to memory, \( \textbf{m}_{t, k} = \textbf{x}^t \) and \( k_t = k_{t-1}+1 \). \( k \) is resetted to 1 if it exceeds the memory size \( K \). In the other case the memory and \( k \) are left as they are.</li><li>Each time step the read head is controlled in a similar way using the vector \( \textbf{c}_t^r = \textbf{U}^r \textbf{x}^t + \textbf{V}^r \textbf{h}^t \) where \( \textbf{U}^r, \textbf{V}^r \) are learnable parameters. If \( c^r_{t, 2} = max{c^r_{t, 1}, c^r_{t, 2}, c^r_{t, 3}} \) \( l_t = l_{t-1}+1 \), otherwise \( l_t = 1 \). After that the memory read at time \( t \) is set as the \( l\)th row of \( \textbf{M}_t \), \( \textbf{r}^t = \textbf{m}_{t, l_t} \)</li></ul><p>The output of the system is determined by \( \textbf{y_t} = \textbf{V} \textbf{h}^t + \textbf{R} \textbf{r}^t \) where \( \textbf{V}, \textbf{R} \) are learnable parameters. Setting \( \textbf{R} = 0 \) we can see that the result is the standard ESN.</p><p>For a more detailed explanation of the procedure and of the training process please refer to the original paper.</p><h1 id=implementation-in-reservoircomputingjl>Implementation in ReservoirComputing.jl<a hidden class=anchor aria-hidden=true href=#implementation-in-reservoircomputingjl>#</a></h1><p>Following both the paper and the code provided (original in Python, click <a href=https://gitlab.ub.uni-bielefeld.de/bpaassen/reservoir-memory-machines>here</a>) we were able to implement a <code>RMM</code> mutable struct and a <code>RMMdirect_predict</code> function able to train and do predictions with the RMM model. The default constructor for <code>RMM</code> takes as input</p><ul><li><code>W</code> the reservoir matrix</li><li><code>in_data</code> the training data</li><li><code>out_data</code> the desired output</li><li><code>W_in</code> the input layer matrix</li><li><code>memory_size</code> the size \( K \) of the memory</li><li><code>activation</code> optional activation function for the reservoir states, with default <code>tanh</code></li><li><code>alpha</code> optional leaking rate, with default 1.0</li><li><code>nla_type</code> optional non linear algorithm, eith default <code>NLADefault()</code></li><li><code>extended_states</code> optional boolean for the extended states option, with default <code>false</code></li></ul><p>The constructor trains the RMM, so ance it is initialized there is only need for a predict function. The <code>RMMdirect_predict</code> takes as input</p><ul><li><code>rmmm</code> an initialized RMM</li><li><code>input</code> the input data
and gives as output the prediction based on the input data given. The prediction process is relatively different from the implementation used in ReservoirComputing.jl, so we will not be able to do a proper comparison with the other models we implemented. In the future we do want to uniform the RMM with the other architectures present in the library, but it seems like a moth worth of work, so for the moment we are happy with the basic implementations obtained.</li></ul><h1 id=examples>Examples<a hidden class=anchor aria-hidden=true href=#examples>#</a></h1><p>For example we will use the next step prediction for the <a href=https://en.wikipedia.org/wiki/H%C3%A9non_map>Henon map</a>, used also in last week test. The map is defined as</p><p>$$x_{x+1} = 1 - ax_n^2 + y_n$$
$$ y_{n+1} = bx_n $$</p><p>Let us start by installing and importing all the needed packages:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> Pkg
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;ReservoirComputing&#34;</span>)
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;DynamicalSystems&#34;</span>)
</span></span><span style=display:flex><span>Pkg<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;LinearAlgebra&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> ReservoirComputing
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> DynamicalSystems
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> LinearAlgebra
</span></span></code></pre></div><p>Now we can generate the Henon map, and we will shift the data points by -0.5 and scale them by 2 to reproduce the data we had last week. The initial transient will be washed out and we will create four datasets called <code>train_x</code>, <code>train_y</code>, <code>test_x</code> and <code>test_y</code>:`</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>ds <span style=color:#f92672>=</span> Systems<span style=color:#f92672>.</span>henon()
</span></span><span style=display:flex><span>traj <span style=color:#f92672>=</span> trajectory(ds, <span style=color:#ae81ff>7000</span>)
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> <span style=color:#66d9ef>Matrix</span>(traj)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> (data <span style=color:#f92672>.-</span><span style=color:#ae81ff>0.5</span>) <span style=color:#f92672>.*</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>shift <span style=color:#f92672>=</span> <span style=color:#ae81ff>200</span>
</span></span><span style=display:flex><span>train_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>2000</span>
</span></span><span style=display:flex><span>predict_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>3000</span>
</span></span><span style=display:flex><span>train_x <span style=color:#f92672>=</span> data[shift<span style=color:#f92672>:</span>shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>:</span>]
</span></span><span style=display:flex><span>train_y <span style=color:#f92672>=</span> data[shift<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>shift<span style=color:#f92672>+</span>train_len, <span style=color:#f92672>:</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>test_x <span style=color:#f92672>=</span> data[shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>:</span>shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>+</span>predict_len<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>:</span>]
</span></span><span style=display:flex><span>test_y <span style=color:#f92672>=</span> data[shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>+</span>predict_len, <span style=color:#f92672>:</span>]
</span></span></code></pre></div><p>Having the needed data we can proceed to the prediction task. In the RMM paper the model is tested using Cycle Reservoirs with Regular Jumps <a href=#3>[3]</a> so we will do the same for our test. In addition to that we will also use the other minimum complexity reservoirs <a href=#4>[4]</a> that we implemented in <a href=https://martinuzzifrancesco.github.io/posts/06_gsoc_week/>week 6</a>. The input layer used is obtained with the function <code>irrational_sign_input()</code>, that builds a fully connected layer with the same values which signs are determined by the values of an irrational number, in our case pi. Setting the parameters for the construction of the RMM</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>*</span><span style=color:#ae81ff>10</span><span style=color:#f92672>^</span>(<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>input_weight <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>cyrcle_weight <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.99</span>
</span></span><span style=display:flex><span>jump_weight <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>jumps <span style=color:#f92672>=</span> <span style=color:#ae81ff>12</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>memory_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>
</span></span></code></pre></div><p>We can now build the reservoir and the RMMs needed for the comparison of the results:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>Wcrj <span style=color:#f92672>=</span> CRJ(approx_res_size, cyrcle_weight, jump_weight, jumps)
</span></span><span style=display:flex><span>Wscr <span style=color:#f92672>=</span> SCR(approx_res_size, cyrcle_weight)
</span></span><span style=display:flex><span>Wdlrb <span style=color:#f92672>=</span> DLRB(approx_res_size, cyrcle_weight, jump_weight)
</span></span><span style=display:flex><span>Wdlr <span style=color:#f92672>=</span> DLR(approx_res_size, cyrcle_weight)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>W_in <span style=color:#f92672>=</span> irrational_sign_input(approx_res_size, size(train_x, <span style=color:#ae81ff>2</span>), input_weight)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rmmcrj <span style=color:#f92672>=</span> RMM(Wcrj, train_x, train_y, W_in, memory_size, beta)
</span></span><span style=display:flex><span>rmmscr <span style=color:#f92672>=</span> RMM(Wscr, train_x, train_y, W_in, memory_size, beta)
</span></span><span style=display:flex><span>rmmdlrb <span style=color:#f92672>=</span> RMM(Wdlrb, train_x, train_y, W_in, memory_size, beta)
</span></span><span style=display:flex><span>rmmdlr <span style=color:#f92672>=</span> RMM(Wdlr, train_x, train_y, W_in, memory_size, beta)
</span></span></code></pre></div><p>Now that we have our trained RMM we want to predict the one step ahead henon map and compare the results obtained with different reservoirs. In oreder to do so we are first going to implement a quick nmse function :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>function</span> NMSE(target, output)
</span></span><span style=display:flex><span>    num <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>    den <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>    sums <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(target, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        append!(sums, sum(target[<span style=color:#f92672>:</span>, i]))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(target, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        num <span style=color:#f92672>+=</span> norm(output[i, <span style=color:#f92672>:</span>]<span style=color:#f92672>-</span>target[i, <span style=color:#f92672>:</span>])<span style=color:#f92672>^</span><span style=color:#ae81ff>2.0</span>
</span></span><span style=display:flex><span>        den <span style=color:#f92672>+=</span> norm(target[i, <span style=color:#f92672>:</span>]<span style=color:#f92672>-</span>sums<span style=color:#f92672>./</span>size(target, <span style=color:#ae81ff>1</span>))<span style=color:#f92672>^</span><span style=color:#ae81ff>2.0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    nmse <span style=color:#f92672>=</span> (num<span style=color:#f92672>/</span>size(target, <span style=color:#ae81ff>1</span>))<span style=color:#f92672>/</span>(den<span style=color:#f92672>/</span>size(target, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> nmse
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><p>after that we are going to predict the system and compare the results:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>rmms <span style=color:#f92672>=</span> [rmmcrj, rmmscr, rmmdlrb, rmmdlr]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> rmm <span style=color:#66d9ef>in</span> rmms
</span></span><span style=display:flex><span>    output2 <span style=color:#f92672>=</span> RMMdirect_predict(rmm, test_x)
</span></span><span style=display:flex><span>    println(NMSE(train_y, output2))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><pre tabindex=0><code>1.4856355716974217
1.4868276240921912
1.5624183281454223
1.5237046076873637
</code></pre><p>As we can see the best performing architecture is the one with the CRJ reservoir. The SCR closely follows.</p><p>This tests are not the one used in the paper, but given that I was a little behind with the implementation I thought to do a couple of quick and easy ones instead. The model is really interesting and I want to continue to explore the possibilities that it offers. The implementation, while working, is not yet finished: there are a couple of finishing touches to give and a couple of more checks to do. I really want to be able to reproduce the results of the paper but the base implementations of ESN in ReservoirComputing and the paper code are really different, and it will take at least another week of full work to unravel all the small details. Huge thanks to the author <a href=https://bpaassen.gitlab.io/>Benjamin Paaßen</a> that answered quickly and kindly to my emails.</p><p>As always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!</p><h2 id=citing>Citing<a hidden class=anchor aria-hidden=true href=#citing>#</a></h2><p>Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bibtex data-lang=bibtex><span style=display:flex><span><span style=color:#a6e22e>@article</span>{JMLR:v23:22-0611,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>author</span>  = <span style=color:#e6db74>{Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>title</span>   = <span style=color:#e6db74>{ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>journal</span> = <span style=color:#e6db74>{Journal of Machine Learning Research}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>year</span>    = <span style=color:#e6db74>{2022}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>volume</span>  = <span style=color:#e6db74>{23}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>number</span>  = <span style=color:#e6db74>{288}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>pages</span>   = <span style=color:#e6db74>{1--8}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>url</span>     = <span style=color:#e6db74>{http://jmlr.org/papers/v23/22-0611.html}</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=documentation>Documentation<a hidden class=anchor aria-hidden=true href=#documentation>#</a></h2><p>[1]
Paaßen, Benjamin, and Alexander Schulz. &ldquo;Reservoir memory machines.&rdquo; arXiv preprint arXiv:2003.04793 (2020).</p><p>[2]
Graves, Alex, Greg Wayne, and Ivo Danihelka. &ldquo;Neural turing machines.&rdquo; arXiv preprint arXiv:1410.5401 (2014).</p><p>[3]
Rodan, Ali, and Peter Tiňo. &ldquo;Simple deterministically constructed cycle reservoirs with regular jumps.&rdquo; Neural computation 24.7 (2012): 1822-1852.</p><p>[4]
Rodan, Ali, and Peter Tino. &ldquo;Minimum complexity echo state network.&rdquo; IEEE transactions on neural networks 22.1 (2010): 131-144.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://martinuzzifrancesco.github.io/>Francesco Martinuzzi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>