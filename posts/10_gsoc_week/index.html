<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.74.3" />

  <title>GSoC week 10: Reservoir Memory Machines &middot; Francesco Martinuzzi</title>

  <meta name="description" content="" />

  

<meta itemprop="name" content="GSoC week 10: Reservoir Memory Machines">
<meta itemprop="description" content="For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year [1].">
<meta itemprop="datePublished" content="2020-08-13T14:41:28+02:00" />
<meta itemprop="dateModified" content="2020-08-13T14:41:28+02:00" />
<meta itemprop="wordCount" content="1315">
<meta itemprop="image" content="https://martinuzzifrancesco.github.io/images/"/>



<meta itemprop="keywords" content="" />


<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://martinuzzifrancesco.github.io/images/"/>

<meta name="twitter:title" content="GSoC week 10: Reservoir Memory Machines"/>
<meta name="twitter:description" content="For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year [1]."/>


<meta property="og:title" content="GSoC week 10: Reservoir Memory Machines" />
<meta property="og:description" content="For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year [1]." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/10_gsoc_week/" />
<meta property="og:image" content="https://martinuzzifrancesco.github.io/images/"/>
<meta property="article:published_time" content="2020-08-13T14:41:28+02:00" />
<meta property="article:modified_time" content="2020-08-13T14:41:28+02:00" /><meta property="og:site_name" content="Francesco Martinuzzi" />



  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/print.css"
        media="print">

  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/poole.css">

  <link type="text/css"
        rel="stylesheet"
        href="https://martinuzzifrancesco.github.io/css/hyde.css">

  
<style type="text/css">
  .sidebar {
    background-color: #536060;
  }

  .read-more-link a {
    border-color: #536060;
  }

  .pagination li a {
    color: #536060;
    border: 1px solid #536060;
  }

  .pagination li.active a {
    background-color: #536060;
  }

  .pagination li a:hover {
    background-color: #536060;
    opacity: 0.75;
  }

  footer a,
  .content a,
  .related-posts li a:hover {
    color: #536060;
  }
</style>



  

  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700&display=swap">

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css"
        integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk="
        crossorigin="anonymous" />

  <link rel="apple-touch-icon-precomposed"
        sizes="144x144"
        href="/apple-touch-icon-144-precomposed.png">

  <link rel="shortcut icon" href="/favicon.png">

  
  </head>
<body>
  <aside class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      
        
        <div class="author-image">
          <img src="https://martinuzzifrancesco.github.io/images/prova.jpg" class="img-circle img-headshot center" alt="Profile Picture">
        </div>
        
      

      <h1>Francesco Martinuzzi</h1>

      
      <p class="lead">Physicist learning how to make machines learn</p>
      
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li>
          <a href="https://martinuzzifrancesco.github.io/">Home</a>
        </li>
        <li>
          <a href="/posts/"> Posts </a>
        </li><li>
          <a href="/about/"> About </a>
        </li><li>
          <a href="/contact/"> Contact </a>
        </li>
      </ul>
    </nav>

    <section class="social-icons">
      
      <a href="https://github.com/MartinuzziFrancesco" rel="me" title="GitHub">
        <i class="fab fa-github" aria-hidden="true"></i>
      </a>
      
      <a href="https://twitter.com/MartinuzziFra" rel="me" title="Twitter">
        <i class="fab fa-twitter" aria-hidden="true"></i>
      </a>
      
    </section>
  </div>
</aside>


  <main class="content container">
  <div class="post">
  <h1>GSoC week 10: Reservoir Memory Machines</h1>

  <div class="post-date">
    <time datetime="2020-08-13T14:41:28&#43;0200">Aug 13, 2020</time> Â· 7 min read
  </div>

  <p>For the 10th week of the GSoC program I wanterd to implement a fairly new model, namely the Reservoir Memory Machines (RMM), proposed earlier this year <a href="#1">[1]</a>. This was one of the hardest, and longest, implementations to date and I believe there is still some work to be done. In this post we will briefly touch on the theory behind the model, and after an example of their usage will be presented.</p>
<h1 id="theoretical-background">Theoretical Background</h1>
<p>Born as an alternative to the Neural Turing Machine <a href="#2">[2]</a> the RMM is an extension of the Echo State Network model, with the addition of an actual memory \( \textbf{M}_t \in \mathbb{R}^{K \times n} \), a write head and a read head. The dynamics of the RMM are the following:</p>
<ul>
<li>In the first step the previous memory state is copied \( \textbf{M}_t = \textbf{M}_{t-1} \), with the initial memory step being initialized to zero.</li>
<li>The write head is then controlled by the value \( c_t^w = \textbf{u}^w \textbf{x}^t + \textbf{v}^r \textbf{h}^t \) where \( \textbf{u}^w, \textbf{v}^r \) are learnable parameters and \( \textbf{x}^t, \textbf{h}^t \) are the input vector and state vector at time t respectively. If \( c_t^w &gt; 0 \) then the input is written to memory, \( \textbf{m}_{t, k} = \textbf{x}^t \) and \( k_t = k_{t-1}+1 \). \( k \) is resetted to 1 if it exceeds the memory size \( K \). In the other case the memory and \( k \) are left as they are.</li>
<li>Each time step the read head is controlled in a similar way using the vector \( \textbf{c}_t^r = \textbf{U}^r \textbf{x}^t + \textbf{V}^r \textbf{h}^t \) where \( \textbf{U}^r, \textbf{V}^r \) are learnable parameters. If \( c^r_{t, 2} = max{c^r_{t, 1}, c^r_{t, 2}, c^r_{t, 3}} \) \( l_t = l_{t-1}+1 \), otherwise \( l_t = 1 \). After that the memory read at time \( t \) is set as the \(  l\)th row of \( \textbf{M}_t \), \( \textbf{r}^t = \textbf{m}_{t, l_t} \)</li>
</ul>
<p>The output of the system is determined by \( \textbf{y_t} = \textbf{V} \textbf{h}^t + \textbf{R} \textbf{r}^t \) where \( \textbf{V}, \textbf{R} \) are learnable parameters. Setting \( \textbf{R} = 0 \) we can see that the result is the standard ESN.</p>
<p>For a more detailed explanation of the procedure and of the training process please refer to the original paper.</p>
<h1 id="implementation-in-reservoircomputingjl">Implementation in ReservoirComputing.jl</h1>
<p>Following both the paper and the code provided (original in Python, click <a href="https://gitlab.ub.uni-bielefeld.de/bpaassen/reservoir-memory-machines">here</a>) we were able to implement a <code>RMM</code> mutable struct and a <code>RMMdirect_predict</code> function able to train and do predictions with the RMM model. The default constructor for <code>RMM</code> takes as input</p>
<ul>
<li><code>W</code> the reservoir matrix</li>
<li><code>in_data</code> the training data</li>
<li><code>out_data</code> the desired output</li>
<li><code>W_in</code> the input layer matrix</li>
<li><code>memory_size</code> the size \( K \) of the memory</li>
<li><code>activation</code> optional activation function for the reservoir states, with default <code>tanh</code></li>
<li><code>alpha</code> optional leaking rate, with default 1.0</li>
<li><code>nla_type</code> optional non linear algorithm, eith default <code>NLADefault()</code></li>
<li><code>extended_states</code> optional boolean for the extended states option, with default <code>false</code></li>
</ul>
<p>The constructor trains the RMM, so ance it is initialized there is only need for a predict function. The <code>RMMdirect_predict</code> takes as input</p>
<ul>
<li><code>rmmm</code> an initialized RMM</li>
<li><code>input</code> the input data
and gives as output the prediction based on the input data given. The prediction process is relatively different from the implementation used in ReservoirComputing.jl, so we will not be able to do a proper comparison with the other models we implemented. In the future we do want to uniform the RMM with the other architectures present in the library, but it seems like a moth worth of work, so for the moment we are happy with the basic implementations obtained.</li>
</ul>
<h1 id="examples">Examples</h1>
<p>For example we will use the next step prediction for the <a href="https://en.wikipedia.org/wiki/H%C3%A9non_map">Henon map</a>, used also in last week test. The map is defined as</p>
<p>$$x_{x+1} = 1 - ax_n^2 + y_n$$
$$ y_{n+1} = bx_n $$</p>
<p>Let us start by installing and importing all the needed packages:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia"><span style="color:#66d9ef">using</span> Pkg
Pkg<span style="color:#f92672">.</span>add(<span style="color:#e6db74">&#34;ReservoirComputing&#34;</span>)
Pkg<span style="color:#f92672">.</span>add(<span style="color:#e6db74">&#34;DynamicalSystems&#34;</span>)
Pkg<span style="color:#f92672">.</span>add(<span style="color:#e6db74">&#34;LinearAlgebra&#34;</span>)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia"><span style="color:#66d9ef">using</span> ReservoirComputing
<span style="color:#66d9ef">using</span> DynamicalSystems
<span style="color:#66d9ef">using</span> LinearAlgebra
</code></pre></div><p>Now we can generate the Henon map, and we will shift the data points by -0.5 and scale them by 2 to reproduce the data we had last week. The initial transient will be washed out and we will create four datasets called <code>train_x</code>, <code>train_y</code>, <code>test_x</code> and <code>test_y</code>:`</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">ds <span style="color:#f92672">=</span> Systems<span style="color:#f92672">.</span>henon()
traj <span style="color:#f92672">=</span> trajectory(ds, <span style="color:#ae81ff">7000</span>)
data <span style="color:#f92672">=</span> <span style="color:#66d9ef">Matrix</span>(traj)

data <span style="color:#f92672">=</span> (data <span style="color:#f92672">.-</span><span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">.*</span> <span style="color:#ae81ff">2</span>
shift <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>
train_len <span style="color:#f92672">=</span> <span style="color:#ae81ff">2000</span>
predict_len <span style="color:#f92672">=</span> <span style="color:#ae81ff">3000</span>
train_x <span style="color:#f92672">=</span> data[shift<span style="color:#f92672">:</span>shift<span style="color:#f92672">+</span>train_len<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#f92672">:</span>]
train_y <span style="color:#f92672">=</span> data[shift<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>shift<span style="color:#f92672">+</span>train_len, <span style="color:#f92672">:</span>]

test_x <span style="color:#f92672">=</span> data[shift<span style="color:#f92672">+</span>train_len<span style="color:#f92672">:</span>shift<span style="color:#f92672">+</span>train_len<span style="color:#f92672">+</span>predict_len<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#f92672">:</span>]
test_y <span style="color:#f92672">=</span> data[shift<span style="color:#f92672">+</span>train_len<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>shift<span style="color:#f92672">+</span>train_len<span style="color:#f92672">+</span>predict_len, <span style="color:#f92672">:</span>]
</code></pre></div><p>Having the needed data we can proceed to the prediction task. In the RMM paper the model is tested using Cycle Reservoirs with Regular Jumps <a href="#3">[3]</a> so we will do the same for our test. In addition to that we will also use the other minimum complexity reservoirs <a href="#4">[4]</a> that we implemented in <a href="https://martinuzzifrancesco.github.io/posts/06_gsoc_week/">week 6</a>. The input layer used is obtained with the function <code>irrational_sign_input()</code>, that builds a fully connected layer with the same values which signs are determined by the values of an irrational number, in our case pi. Setting the parameters for the construction of the RMM</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">approx_res_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
sigma <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">*</span><span style="color:#ae81ff">10</span><span style="color:#f92672">^</span>(<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>)
extended_states <span style="color:#f92672">=</span> <span style="color:#66d9ef">false</span>

input_weight <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
cyrcle_weight <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.99</span>
jump_weight <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
jumps <span style="color:#f92672">=</span> <span style="color:#ae81ff">12</span>

memory_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>
</code></pre></div><p>We can now build the reservoir and the RMMs needed for the comparison of the results:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">Wcrj <span style="color:#f92672">=</span> CRJ(approx_res_size, cyrcle_weight, jump_weight, jumps)
Wscr <span style="color:#f92672">=</span> SCR(approx_res_size, cyrcle_weight)
Wdlrb <span style="color:#f92672">=</span> DLRB(approx_res_size, cyrcle_weight, jump_weight)
Wdlr <span style="color:#f92672">=</span> DLR(approx_res_size, cyrcle_weight)

W_in <span style="color:#f92672">=</span> irrational_sign_input(approx_res_size, size(train_x, <span style="color:#ae81ff">2</span>), input_weight)

rmmcrj <span style="color:#f92672">=</span> RMM(Wcrj, train_x, train_y, W_in, memory_size, beta)
rmmscr <span style="color:#f92672">=</span> RMM(Wscr, train_x, train_y, W_in, memory_size, beta)
rmmdlrb <span style="color:#f92672">=</span> RMM(Wdlrb, train_x, train_y, W_in, memory_size, beta)
rmmdlr <span style="color:#f92672">=</span> RMM(Wdlr, train_x, train_y, W_in, memory_size, beta)
</code></pre></div><p>Now that we have our trained RMM we want to predict the one step ahead henon map and compare the results obtained with different reservoirs. In oreder to do so we are first going to implement a quick nmse function :</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia"><span style="color:#66d9ef">function</span> NMSE(target, output)
    num <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
    den <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
    sums <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> i<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>size(target, <span style="color:#ae81ff">2</span>)
        append!(sums, sum(target[<span style="color:#f92672">:</span>, i]))
    <span style="color:#66d9ef">end</span>
    <span style="color:#66d9ef">for</span> i<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>size(target, <span style="color:#ae81ff">1</span>)
        num <span style="color:#f92672">+=</span> norm(output[i, <span style="color:#f92672">:</span>]<span style="color:#f92672">-</span>target[i, <span style="color:#f92672">:</span>])<span style="color:#f92672">^</span><span style="color:#ae81ff">2.0</span>
        den <span style="color:#f92672">+=</span> norm(target[i, <span style="color:#f92672">:</span>]<span style="color:#f92672">-</span>sums<span style="color:#f92672">./</span>size(target, <span style="color:#ae81ff">1</span>))<span style="color:#f92672">^</span><span style="color:#ae81ff">2.0</span>
    <span style="color:#66d9ef">end</span>
    nmse <span style="color:#f92672">=</span> (num<span style="color:#f92672">/</span>size(target, <span style="color:#ae81ff">1</span>))<span style="color:#f92672">/</span>(den<span style="color:#f92672">/</span>size(target, <span style="color:#ae81ff">1</span>))
    <span style="color:#66d9ef">return</span> nmse
<span style="color:#66d9ef">end</span>
</code></pre></div><p>after that we are going to predict the system and compare the results:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">rmms <span style="color:#f92672">=</span> [rmmcrj, rmmscr, rmmdlrb, rmmdlr]

<span style="color:#66d9ef">for</span> rmm <span style="color:#66d9ef">in</span> rmms
    output2 <span style="color:#f92672">=</span> RMMdirect_predict(rmm, test_x)
    println(NMSE(train_y, output2))
<span style="color:#66d9ef">end</span>
</code></pre></div><pre><code>1.4856355716974217
1.4868276240921912
1.5624183281454223
1.5237046076873637
</code></pre><p>As we can see the best performing architecture is the one with the CRJ reservoir. The SCR closely follows.</p>
<p>This tests are not the one used in the paper, but given that I was a little behind with the implementation I thought to do a couple of quick and easy ones instead. The model is really interesting and I want to continue to explore the possibilities that it offers. The implementation, while working, is not yet finished: there are a couple of finishing touches to give and a couple of more checks to do. I really want to be able to reproduce the results of the paper but the base implementations of ESN in ReservoirComputing and the paper code are really different, and it will take at least another week of full work to unravel all the small details. Huge thanks to the author <a href="https://bpaassen.gitlab.io/">Benjamin PaaÃen</a> that answered quickly and kindly to my emails.</p>
<p>As always, if you have any questions regarding the model, the package or you have found errors in my post, please donât hesitate to contact me!</p>
<h2 id="documentation">Documentation</h2>
<p><!-- raw HTML omitted -->[1]<!-- raw HTML omitted -->
PaaÃen, Benjamin, and Alexander Schulz. &ldquo;Reservoir memory machines.&rdquo; arXiv preprint arXiv:2003.04793 (2020).</p>
<p><!-- raw HTML omitted -->[2]<!-- raw HTML omitted -->
Graves, Alex, Greg Wayne, and Ivo Danihelka. &ldquo;Neural turing machines.&rdquo; arXiv preprint arXiv:1410.5401 (2014).</p>
<p><!-- raw HTML omitted -->[3]<!-- raw HTML omitted -->
Rodan, Ali, and Peter TiÅo. &ldquo;Simple deterministically constructed cycle reservoirs with regular jumps.&rdquo; Neural computation 24.7 (2012): 1822-1852.</p>
<p><!-- raw HTML omitted -->[4]<!-- raw HTML omitted -->
Rodan, Ali, and Peter Tino. &ldquo;Minimum complexity echo state network.&rdquo; IEEE transactions on neural networks 22.1 (2010): 131-144.</p>

</div>


  </main>

  <footer>
  <div>
    &copy; Martinuzzi Francesco 2020
    Â·
    
    <a href="https://creativecommons.org/licenses/by-sa/4.0"
       target="_blank">CC BY-SA 4.0</a>
    
    
  </div>
</footer>

</body>

    <script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
            fontCache: 'global'
        }
    };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script> 


</html>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/js/all.min.js"
          integrity="sha256-MAgcygDRahs+F/Nk5Vz387whB4kSK9NXlDN3w58LLq0="
          crossorigin="anonymous"></script>

  

  
</body>
</html>
