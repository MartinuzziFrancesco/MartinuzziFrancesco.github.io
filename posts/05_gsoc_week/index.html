<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Data-driven prediction of chaotic systems: comparison of Echo State Network variations | Francesco Martinuzzi</title><meta name=keywords content><meta name=description content="This post is meant to be a high level comparison of different variations of the Echo State Network (ESN) model, implemented in the first month of Google Summer of Code. The theoretical background for all the proposed models has already been covered in past posts, so we will not touch on it in this one to keep things as thight as possible; if one is interested all my previous posts can be found here."><meta name=author content="Francesco Martinuzzi"><link rel=canonical href=https://martinuzzifrancesco.github.io/posts/05_gsoc_week/><link crossorigin=anonymous href=/assets/css/stylesheet.849fd8bd636f9bdcc8fd3087509c431a61faae78bd735e7ba75e6fd13ec64f83.css integrity="sha256-hJ/YvWNvm9zI/TCHUJxDGmH6rni9c157p15v0T7GT4M=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://martinuzzifrancesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://martinuzzifrancesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://martinuzzifrancesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://martinuzzifrancesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://martinuzzifrancesco.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Data-driven prediction of chaotic systems: comparison of Echo State Network variations "><meta property="og:description" content="This post is meant to be a high level comparison of different variations of the Echo State Network (ESN) model, implemented in the first month of Google Summer of Code. The theoretical background for all the proposed models has already been covered in past posts, so we will not touch on it in this one to keep things as thight as possible; if one is interested all my previous posts can be found here."><meta property="og:type" content="article"><meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/05_gsoc_week/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-07-05T21:54:29+02:00"><meta property="article:modified_time" content="2020-07-05T21:54:29+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Data-driven prediction of chaotic systems: comparison of Echo State Network variations "><meta name=twitter:description content="This post is meant to be a high level comparison of different variations of the Echo State Network (ESN) model, implemented in the first month of Google Summer of Code. The theoretical background for all the proposed models has already been covered in past posts, so we will not touch on it in this one to keep things as thight as possible; if one is interested all my previous posts can be found here."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://martinuzzifrancesco.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","item":"https://martinuzzifrancesco.github.io/posts/05_gsoc_week/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","name":"Data-driven prediction of chaotic systems: comparison of Echo State Network variations ","description":"This post is meant to be a high level comparison of different variations of the Echo State Network (ESN) model, implemented in the first month of Google Summer of Code. The theoretical background for all the proposed models has already been covered in past posts, so we will not touch on it in this one to keep things as thight as possible; if one is interested all my previous posts can be found here.","keywords":[],"articleBody":"This post is meant to be a high level comparison of different variations of the Echo State Network (ESN) model, implemented in the first month of Google Summer of Code. The theoretical background for all the proposed models has already been covered in past posts, so we will not touch on it in this one to keep things as thight as possible; if one is interested all my previous posts can be found here. The ESNs are known for their capability of yielding good short term predictions and long term reconstructions of chaotic systems: in order to prove this we are going to test all the proposed models using the Lorenz system.\nIn order to determine the accuracy of the results we will use two different methods:\nFor the short term accuracy we chose an arbitrary time horizon and the difference between the actual timeseries, obtained solving the differential equations of the Lorenz system, and the predicted timeseries will be evaluated using the Root Mean Square Deviation (RMSE). The rmse implementation in Julia is done with the following function function rmse(y, yt) rmse = 0.0 for i=1:size(y, 1) rmse += (y[i]-yt[i])^2.0 end rmse = sqrt(rmse/(size(y, 1))) return rmse end For the long term climate we chose to follow the approach of Pathak [1] and we will show the return map of successive maxima of \\( z(t) \\). To get this data we leveraged the function findlocalmaxima of the package Images.jl. The Julia function used to get the vector of local maxima is defined as follows using Images function local_maxima(input_data) maxs_cart = findlocalmaxima(input_data) maxs = [idx[1] for idx in maxs_cart] max_values = [] for max in maxs push!(max_values, input_data[max]) end return max_values end The data used for all the training and prediction for the ESN in this work is obtained in the following way:\nu0 = [1.0,0.0,0.0] tspan = (0.0,2000.0) p = [10.0,28.0,8/3] #define lorenz system function lorenz(du,u,p,t) du[1] = p[1]*(u[2]-u[1]) du[2] = u[1]*(p[2]-u[3]) - u[2] du[3] = u[1]*u[2] - p[3]*u[3] end #solve and take data prob = ODEProblem(lorenz, u0, tspan, p) sol = solve(prob, RK4(), adaptive=false, dt=0.02) v = sol.u data = Matrix(hcat(v...)) shift = 300 train_len = 5000 predict_len = 1250 return_map_size = 20000 train = data[:, shift:shift+train_len-1] test = data[:, shift+train_len:shift+train_len+predict_len-1] return_map = data[:,shift+train_len:shift+train_len+return_map_size-1]; Where the test data will be used for display and the first 400 timesteps for the short term prediction. The return_map data will instead be used for the creation of the return maps.\nOrdinary ESN The Ordinary Least Squares (OLS) trained ESN is the model used in [1] to accurately predict the Lorenz system in the short term and replicate its climate in the long term. We will use the same construction given in their paper, and most of these parameters will be used also for the other variation presented in this post. The parameters and the training for the ESN are as follows\nusing ReservoirComputing using Random approx_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 0.0 alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(esn, beta) output = ESNpredict(esn, predict_len, W_out) 1.963047 seconds (5.79 M allocations: 310.671 MiB, 4.29% gc time) We can plot a comparison to have a visual feedback for the coordinates\nplot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") From just a quick eye test we can see that the short term prediction is rather good, and in the long term the behaviour seems to be in line with the numerical solution. For the short term prediction we are going to use the already defined rmse function an all three the variables to check the accuracy. The arbitrary length of the short time horizon is set to 400 and will stay the same all throughout this work.\nfor i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 0.9484793317137318 1.4450094312490096 1.769172087306121 Since the values are the lower the better we can be satisfied with what we obtained. This numbers will mostly be used for comparisons between models and their significance by themselves is very limited, also because this is the result of a single run. To show model consistency a more deep analisys has to be conducted, but aslo this aspect will be discussed in the ending section.\nTo have quantitative confirmation that our models is capable of predicting a reliable synthetic dataset we are going to predict the system using the return_map dataset, and then plot the consecutives maxima of the \\( z(t) \\) coordinate.\noutput_map = ESNpredict(esn, return_map_size, W_out) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) It is nice to see that our results are in line with what is displayed in the paper [2].\nRidge ESN One usual problem that can be encountered when dealing with OLS is the insurgence of numerical instabilities when inverting \\( (\\textbf{X} \\textbf{X}^T) \\) [2] where \\( \\textbf{X} \\) is the feature matrix (states matrix in the case of ESNs). A solution to this is to apply a regularization to the loss function, and one of the most common is the \\( L_2 \\) regularization. This way we obtain what is called ridge regression or Tikhonov regularization. The ridge ESN is trained in an equal manner as the OLS ESN we discussed above, only setting a parameter beta different than zero. The parameter that we chose is by no mean optimized and it is chosen by manual search, and this holds sadly for all the parameters in the models here presented. In the Conclusions section we will talk a little more about this aspect of this work.\napprox_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 0.001 alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(esn, beta) output = ESNpredict(esn, predict_len, W_out); 2.223003 seconds (5.79 M allocations: 310.671 MiB, 2.85% gc time) This methods is just a little slower, as it has to be expected. It is still an acceptable by any means. Plotting the results we obtain:\nplot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") The behaviour is similar to the standard ESN, but let’s take a look at the short and long term.\nfor i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 5.3658081215986 6.552586461430827 4.995926155420491 It was clearly visible before that the short term behaviour was not as good as the standard counterpart. The long term predictions are still acceptable, as we can see here:\noutput_map = ESNpredict(esn, return_map_size, W_out) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) Clearly in this case the original architecture proves superior in the short term, but in the long term the both are really viable. Depending on the situation and dataset the ridge ESN can be a valide choice for accuracy and speed of training, and it could also be the only viable choice between the two, if the problem is ill-posed.\nLasso ESN Another common regularization for the OLS regression is the \\( L_1 \\) regularization, resulting in a regression model called Lasso. This is a stronger regularization, and it shows from the results. The ESN is built in the same way as before, only this time the parameter beta indicates the Lasso regularizer. Since the expression doesn’t have a closed form solution we will need a different solver, in this case ProxGrad. For this we will need to import a different package, called MLJLinearModels.jl.\nusing MLJLinearModels approx_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 1*10^(-7) alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(Lasso(beta, ProxGrad(max_iter=10000)), esn) output = ESNpredict(esn, predict_len, W_out) 24.400234 seconds (9.82 M allocations: 1.534 GiB, 1.17% gc time) The training time is slower than the couple of models we showed before. This difference is mainly to the already mentioned lack of closed form solution for the Lasso regression. Let us plot the results to start the analysis the results.\nplot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") It is clear that this regularization is not capable of returning an accurate prediction, both short term and long term. Let’s print the rmse\nfor i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 11.769042467212172 13.599065187854675 10.651859641213985 and plot the return map\noutput_map = ESNpredict(esn, return_map_size, W_out) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) From this results is clear that the \\( L_1 \\) regularization is not capable of good short term prediction and in the long term yields a periodic timeseries, as we can see from the return map, only showing values in 5 contained regions.\nHuber loss function Not only the squared function can be used as a loss function: in literature it has also been proposed the use of the Huber loss function, supposedly more strong in the presence of outliers. The dataset we are using is free of them, but this function should still be able to give accurate results. Since we can apply regularization also in this case, we are going to explore the two cases already explored for the squared function: \\( L_2 \\) regularization and \\( L_1 \\) regularization.\n\\( L_2 \\) Normalization Again leveraging the MLJLinearModels package we can construct our ESN\napprox_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 0.001 alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(RobustHuber(0.5, beta, 0.0, Newton()), esn) output = ESNpredict(esn, predict_len, W_out) 9.397286 seconds (14.37 M allocations: 1.748 GiB, 2.88% gc time) The training time is less than the Lasso regularization, but more than the OLS and Ridge training. Plotting the data we obtain:\nplot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") Let’s go explore the rmse for the short term:\nfor i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 5.36580225918975 6.552573774882654 4.995989255679152 The results seem similar to the Ridge ESN. For the long term the return map shows the following:\noutput_map = ESNpredict(esn, return_map_size, W_out) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) Even though they are not as clear cut as the OLS ESN and Ridge ESN the long term behaviour is still acceptable.\n\\( L_1 \\) Normalization Since the Lasso ESN showed the worst results of all the model seen until now this could indicate that the \\( L_1 \\) norm is not suited for this task. To have confirmation of this intuition we can train the Huber ESN with the \\( L_1 \\) norm to see if it yields better results than the Lasso ESN.\napprox_res_size = 300 radius = 1.2 degree = 6 activation = tanh sigma = 0.1 beta = 1*10^(-7) alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(RobustHuber(0.5, 0.0, beta, ProxGrad(max_iter=10000)), esn) output = ESNpredict(esn, predict_len, W_out) 30.699361 seconds (12.60 M allocations: 3.398 GiB, 1.40% gc time) As expected the training time is in line with the Lasso ESN. Plotting the results we can see that sadly they are worst than the Lasso counterpart\nplot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") Calculating the rmse for the first 400 steps returns\nfor i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 8.247263444106135 9.896322405158461 12.569601968865513 And the return map clearly shows a periodic long term behaviour:\noutput_map = ESNpredict(esn, return_map_size, W_out) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) The periodicity is more pronounced than the Lasso ESN, making this model the worst performing so far on this task.\nDifferent reservoir construction For all the model proposed we used the standard construction of the reservoir, based on the rescaling of the spectral radius to be less than a given value. In literature there are other alternatives explored and in the ReservoirComputing.jl package is present the implementation of an algorithm for the construction of the reservoir matrix based on the Single Value Decomposition (SVD), proposed in [4]. We are going to give the results using this construction only for the OLS ESN, but a more wide study could be done comparing the performances of all the proposed models using the two different implementations of the reservoir.\napprox_res_size = 300 max_value = 1.2 sparsity = 0.1 activation = tanh sigma = 0.1 beta = 0.0 alpha = 1.0 nla_type = NLAT2() extended_states = false Random.seed!(4242) W_in = init_dense_input_layer(approx_res_size, size(train, 1), sigma) W_new = pseudoSVD(approx_res_size, max_value, sparsity, reverse_sort = true) esn = ESN(W_new, train, W_in, activation = activation, alpha = alpha, nla_type = nla_type, extended_states = extended_states) @time W_out = ESNtrain(esn, beta) output = ESNpredict(esn, predict_len, W_out) 0.994207 seconds (3.18 M allocations: 188.436 MiB, 3.64% gc time) The training time is as fast as the normal OLS ESN, as it is to be expected. Plotting the result we obtain\nplot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") From the plot it seems that the model is producing an acceptable Lorenz system surrogate dataset. The short term will not be the best but in the long term maybe the model could behave as the OLS ESN.\nfor i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 8.299351042330624 9.954728072605832 9.507147274321735 Not as good as the normal reservoir counterpart, but let’s look at the return map for the long term behaviour:\noutput_map = ESNpredict(esn, return_map_size, W_out) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) Event though it seemed that the predicted coordinates had a similar behaviour as the actual Lorenz system the return map clearly shows that in the long term the behaviour is not consistent. This approach to reservoir construction is of course pretty novel and has to pass several optimization steps for its parameters in order to be production ready, but this first test is somewhat disappointing.\nEcho State Gaussian Processes Firstly proposed in [3] the Echo State Gaussian Processes (ESGP) can be considered an extension of the ESN. In the original paper only the Radial Basis function is explored for the prediction, but in this post we wanted to give a couple of examples of others kernels, so we will use also the Matern kernel and the Polynomial kernel for the prediction of the Lorenz system. The construction of this model is based on GaussianProcesses, so in the first run we will need to import that package as well.\nRadial Basis kernel Starting from the kernel used in the original paper, we also set the non linear algorithm to the default one, equal to none. The behaviour with different algorithms for this family of models has not been investigated and could be subject of future works, but for now we will just limit the work to the standard one. Keeping the other parameters equal the ESGP can be built in the following way\nusing GaussianProcesses #model parameters degree = 6 approx_res_size = 300 radius = 1.2 activation = tanh sigma = 0.1 alpha = 1.0 nla_type = NLADefault() extended_states = false #create echo state network Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) mean = MeanZero() kernel = SE(1.0, 1.0) @time gp = ESGPtrain(esn, mean, kernel, lognoise = -2.0, optimize = false); output, sigmas = ESGPpredict(esn, predict_len, gp) 43.323255 seconds (6.81 M allocations: 2.053 GiB, 2.53% gc time) The slowest time so far, but maybe the results will be worth the extra seconds it took to train\nplot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") The results do not look bad, in the short term sadly emerges a discrepancy early on that will lower the rmse values\nfor i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 10.5883475480527 11.759694995689005 6.745353916847517 As expected the rmses are not the greatest, but the model can still recover with a nice display of long term behaviour\noutput_map, sigma_map = ESGPpredict(esn, return_map_size, gp) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) and indeed it does, outside of a single point in a strange location the model seems to capture the Lorenz climate quite well, not quite as good as the OLS ESN or even the Ridge ESN.\nMatern kernel Another common kernel is the Matern kernel, and training the ESGP using this kernel is a straightforward process, identical to the one we just followed for the Radial Basis kernel:\n#model parameters degree = 6 approx_res_size = 300 radius = 1.2 activation = tanh sigma = 0.1 alpha = 1.0 nla_type = NLADefault() extended_states = false #create echo state network Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) mean = MeanZero() kernel = Matern(1/2, 1.0, 1.0) @time gp = ESGPtrain(esn, mean, kernel, lognoise = -2.0, optimize = false); output, sigmas = ESGPpredict(esn, predict_len, gp) 40.309179 seconds (1.08 M allocations: 1.761 GiB, 7.58% gc time) Plotting the results\nplot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") The results seem similar to the ones obtained using the Radial Basis kernel. To be sure of this we need to calculate the rmse\nfor i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 7.150484170664864 8.520570924338477 7.037257939507858 The lower rmses shows a better short term prediction. Plotting the return map to analize the long term results\noutput_map, sigma_map = ESGPpredict(esn, return_map_size, gp) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) The return map is not a clear cut as we saw in other models, but for the majority of the times it seems that the model is still capable of retaining the climate of the Lorenz system.\nPolynomial kernel The last kernel for the ESGP is the Polynomial kernel. We are now familiar with the construction\n#model parameters degree = 6 approx_res_size = 300 radius = 1.2 activation = tanh sigma = 0.1 alpha = 1.0 nla_type = NLADefault() extended_states = false #create echo state network Random.seed!(4242) esn = ESN(approx_res_size, train, degree, radius, activation = activation, alpha = alpha, sigma = sigma, nla_type = nla_type, extended_states = extended_states) mean = MeanZero() kernel = Poly(1.0, 1.0, 2) @time gp = ESGPtrain(esn, mean, kernel, lognoise = -2.0, optimize = false); output, sigmas = ESGPpredict(esn, predict_len, gp) 16.979100 seconds (4.27 M allocations: 3.025 GiB, 10.13% gc time) Plotting the results\nplot(transpose(output),layout=(3,1), label=\"predicted\") plot!(transpose(test),layout=(3,1), label=\"actual\") we can see a nice prediction on the short term. Using once again the rmse\nfor i=1:size(data, 1) println(rmse(test[i,1:400], output[i, 1:400])) end 0.9128150765679983 1.3969462542792563 1.6437787377104938 The short term rmse are the best out of the ESGP kernel used until now. Taking a look also to the long term we can see the following\noutput_map, sigma_map = ESGPpredict(esn, return_map_size, gp) max_esn = local_maxima(output_map[3,:]) max_ode = local_maxima(return_map[3,:]) scatter(max_ode[1:end-1], max_ode[2:end], label=\"Actual\") scatter!(max_esn[1:end-1], max_esn[2:end], label=\"Predicted\") xlims!((30, 47)) ylims!(30, 47) The results are in line with the OLS ESN. Considering that the results of the OLS ESN were obtained following a published paper and underwent major optimization while the parameters we have chosen are the fruit of a quick manul search we can say that this model could outperform the ESN for chaotic time series prediction.\nConclusions This post is just scratching the surface on the studies needed for this family of models. As we mentioned numerous times throughout the post, the parameter optimization that was done for this wark was just acceptable, being a manual search starting from values optimized for a specific model (OLS ESN). This fact was most evident when testing the SVD based reservoir construction: even though it is a very similar resulting matrix the results were suboptimal. This could have been just an error in the selection of the parameters, and with a more suiting set this construction could perform as well as the standard one.\nBut even with more carefully chosen parameters it is necessary to do multiple runs and obtain a large pool of statistics in order to label a model variation more efficient than the other: in this post we have seen that the Polynomial ESGP has returned amazing results, but changing the seed and using a different reservoir the best model could very well be the radial basis ESGP, or the Huber ESN. This work was based on the single run of all the models because is more intended to be a showcase of the new implementations done for the GSoC project, and the reproducibility was the most important aspect of it.\nSadly in this post we weren’t able to showcase the Support Vector Echo State Machines (SVESMs) but the results they obtained wasn’t in line with any of the proposed models, and it performed way worst than even the \\( L_1 \\) trained ESNs. The paper proposing the models leveraged it to solve a different family of problems, so it could be that this task is not suited for this particular variation of the ESN.\nRegarding the ESGP the possible directions for future studies are really numerous: there are a vast number of other avaiable kernels that we didn’t explore, and even in the one we used the possibility to optimize the parameter is built in in the model, and in our case wasn’t used just for time limitations. Not only it is possible to obtain better results that the one we showed purely by parameter optimization but also by using different kernels at the same time: in the Gaussian Processes is usual to see different kernels combined togheter, through multiplication and addition. This possibility adds an incredible perspective to this model, and I hope future studies will takle it.\nAs usual thanks for reading, if you spot any mistake or are just curious about the model and implementation don’t hesitate to contact me.\nCiting Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:\n@article{JMLR:v23:22-0611, author = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}, title = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}, journal = {Journal of Machine Learning Research}, year = {2022}, volume = {23}, number = {288}, pages = {1--8}, url = {http://jmlr.org/papers/v23/22-0611.html} } Documentation [1] Pathak, Jaideep, et al. “Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data.” Chaos: An Interdisciplinary Journal of Nonlinear Science 27.12 (2017): 121102.\n[2] Lukoševičius, Mantas. “A practical guide to applying echo state networks.” Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 659-686.\n[3] Chatzis, Sotirios P., and Yiannis Demiris. “Echo state Gaussian process.” IEEE Transactions on Neural Networks 22.9 (2011): 1435-1445.\n[4] Yang, Cuili, et al. “Design of polynomial echo state networks for time series prediction.” Neurocomputing 290 (2018): 148-160.\n","wordCount":"3850","inLanguage":"en","datePublished":"2020-07-05T21:54:29+02:00","dateModified":"2020-07-05T21:54:29+02:00","author":{"@type":"Person","name":"Francesco Martinuzzi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://martinuzzifrancesco.github.io/posts/05_gsoc_week/"},"publisher":{"@type":"Organization","name":"Francesco Martinuzzi","logo":{"@type":"ImageObject","url":"https://martinuzzifrancesco.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://martinuzzifrancesco.github.io/ accesskey=h title="Francesco Martinuzzi (Alt + H)">Francesco Martinuzzi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://martinuzzifrancesco.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://martinuzzifrancesco.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://martinuzzifrancesco.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Data-driven prediction of chaotic systems: comparison of Echo State Network variations</h1><div class=post-meta><span title='2020-07-05 21:54:29 +0200 CEST'>July 5, 2020</span>&nbsp;·&nbsp;Francesco Martinuzzi</div></header><div class=post-content><p>This post is meant to be a high level comparison of different variations of the Echo State Network (ESN) model, implemented in the first month of <a href=https://summerofcode.withgoogle.com/>Google Summer of Code</a>. The theoretical background for all the proposed models has already been covered in past posts, so we will not touch on it in this one to keep things as thight as possible; if one is interested all my previous posts can be found <a href=https://martinuzzifrancesco.github.io/posts/>here</a>. The ESNs are known for their capability of yielding good short term predictions and long term reconstructions of chaotic systems: in order to prove this we are going to test all the proposed models using the <a href=https://en.wikipedia.org/wiki/Lorenz_system>Lorenz system</a>.</p><p>In order to determine the accuracy of the results we will use two different methods:</p><ul><li>For the short term accuracy we chose an arbitrary time horizon and the difference between the actual timeseries, obtained solving the differential equations of the Lorenz system, and the predicted timeseries will be evaluated using the Root Mean Square Deviation (RMSE). The rmse implementation in Julia is done with the following function</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>function</span> rmse(y, yt)
</span></span><span style=display:flex><span>    rmse <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(y, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        rmse <span style=color:#f92672>+=</span> (y[i]<span style=color:#f92672>-</span>yt[i])<span style=color:#f92672>^</span><span style=color:#ae81ff>2.0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    rmse <span style=color:#f92672>=</span> sqrt(rmse<span style=color:#f92672>/</span>(size(y, <span style=color:#ae81ff>1</span>)))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> rmse
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><ul><li>For the long term climate we chose to follow the approach of Pathak <a href=#1>[1]</a> and we will show the return map of successive maxima of \( z(t) \). To get this data we leveraged the function <code>findlocalmaxima</code> of the package Images.jl. The Julia function used to get the vector of local maxima is defined as follows</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> Images
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> local_maxima(input_data)
</span></span><span style=display:flex><span>    maxs_cart <span style=color:#f92672>=</span> findlocalmaxima(input_data)
</span></span><span style=display:flex><span>    maxs <span style=color:#f92672>=</span> [idx[<span style=color:#ae81ff>1</span>] <span style=color:#66d9ef>for</span> idx <span style=color:#66d9ef>in</span> maxs_cart]
</span></span><span style=display:flex><span>    max_values <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> max <span style=color:#66d9ef>in</span> maxs
</span></span><span style=display:flex><span>        push!(max_values, input_data[max])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> max_values
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><p>The data used for all the training and prediction for the ESN in this work is obtained in the following way:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>u0 <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1.0</span>,<span style=color:#ae81ff>0.0</span>,<span style=color:#ae81ff>0.0</span>]                       
</span></span><span style=display:flex><span>tspan <span style=color:#f92672>=</span> (<span style=color:#ae81ff>0.0</span>,<span style=color:#ae81ff>2000.0</span>)                      
</span></span><span style=display:flex><span>p <span style=color:#f92672>=</span> [<span style=color:#ae81ff>10.0</span>,<span style=color:#ae81ff>28.0</span>,<span style=color:#ae81ff>8</span><span style=color:#f92672>/</span><span style=color:#ae81ff>3</span>]
</span></span><span style=display:flex><span><span style=color:#75715e>#define lorenz system </span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> lorenz(du,u,p,t)
</span></span><span style=display:flex><span>    du[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> p[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>*</span>(u[<span style=color:#ae81ff>2</span>]<span style=color:#f92672>-</span>u[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>    du[<span style=color:#ae81ff>2</span>] <span style=color:#f92672>=</span> u[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>*</span>(p[<span style=color:#ae81ff>2</span>]<span style=color:#f92672>-</span>u[<span style=color:#ae81ff>3</span>]) <span style=color:#f92672>-</span> u[<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>    du[<span style=color:#ae81ff>3</span>] <span style=color:#f92672>=</span> u[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>*</span>u[<span style=color:#ae81ff>2</span>] <span style=color:#f92672>-</span> p[<span style=color:#ae81ff>3</span>]<span style=color:#f92672>*</span>u[<span style=color:#ae81ff>3</span>]
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#solve and take data</span>
</span></span><span style=display:flex><span>prob <span style=color:#f92672>=</span> ODEProblem(lorenz, u0, tspan, p)  
</span></span><span style=display:flex><span>sol <span style=color:#f92672>=</span> solve(prob, RK4(), adaptive<span style=color:#f92672>=</span>false, dt<span style=color:#f92672>=</span><span style=color:#ae81ff>0.02</span>)   
</span></span><span style=display:flex><span>v <span style=color:#f92672>=</span> sol<span style=color:#f92672>.</span>u
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> <span style=color:#66d9ef>Matrix</span>(hcat(v<span style=color:#f92672>...</span>))
</span></span><span style=display:flex><span>shift <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>train_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>5000</span>
</span></span><span style=display:flex><span>predict_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>1250</span>
</span></span><span style=display:flex><span>return_map_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>20000</span>
</span></span><span style=display:flex><span>train <span style=color:#f92672>=</span> data[<span style=color:#f92672>:</span>, shift<span style=color:#f92672>:</span>shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>test <span style=color:#f92672>=</span> data[<span style=color:#f92672>:</span>, shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>:</span>shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>+</span>predict_len<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>return_map <span style=color:#f92672>=</span> data[<span style=color:#f92672>:</span>,shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>:</span>shift<span style=color:#f92672>+</span>train_len<span style=color:#f92672>+</span>return_map_size<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>];
</span></span></code></pre></div><p>Where the <code>test</code> data will be used for display and the first 400 timesteps for the short term prediction. The <code>return_map</code> data will instead be used for the creation of the return maps.</p><h1 id=ordinary-esn>Ordinary ESN<a hidden class=anchor aria-hidden=true href=#ordinary-esn>#</a></h1><p>The Ordinary Least Squares (OLS) trained ESN is the model used in <a href=#1>[1]</a> to accurately predict the Lorenz system in the short term and replicate its climate in the long term. We will use the same construction given in their paper, and most of these parameters will be used also for the other variation presented in this post. The parameters and the training for the ESN are as follows</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> ReservoirComputing
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> Random 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>degree <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLAT2()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Random<span style=color:#f92672>.</span>seed!(<span style=color:#ae81ff>4242</span>)
</span></span><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(approx_res_size, train, degree, radius, 
</span></span><span style=display:flex><span>    activation <span style=color:#f92672>=</span> activation, alpha <span style=color:#f92672>=</span> alpha, sigma <span style=color:#f92672>=</span> sigma, nla_type <span style=color:#f92672>=</span> nla_type, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> W_out <span style=color:#f92672>=</span> ESNtrain(esn, beta)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> ESNpredict(esn, predict_len, W_out)
</span></span></code></pre></div><pre tabindex=0><code>1.963047 seconds (5.79 M allocations: 310.671 MiB, 4.29% gc time)
</code></pre><p>We can plot a comparison to have a visual feedback for the coordinates</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>plot(transpose(output),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;predicted&#34;</span>)
</span></span><span style=display:flex><span>plot!(transpose(test),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;actual&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86641679-9cf0ef00-bfdb-11ea-838e-42ee25f6e2d4.png alt=oesn_coords>
From just a quick eye test we can see that the short term prediction is rather good, and in the long term the behaviour seems to be in line with the numerical solution. For the short term prediction we are going to use the already defined <code>rmse</code> function an all three the variables to check the accuracy. The arbitrary length of the short time horizon is set to 400 and will stay the same all throughout this work.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(data, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    println(rmse(test[i,<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>], output[i, <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>]))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><pre tabindex=0><code>0.9484793317137318
1.4450094312490096
1.769172087306121
</code></pre><p>Since the values are the lower the better we can be satisfied with what we obtained. This numbers will mostly be used for comparisons between models and their significance by themselves is very limited, also because this is the result of a single run. To show model consistency a more deep analisys has to be conducted, but aslo this aspect will be discussed in the ending section.</p><p>To have quantitative confirmation that our models is capable of predicting a reliable synthetic dataset we are going to predict the system using the <code>return_map</code> dataset, and then plot the consecutives maxima of the \( z(t) \) coordinate.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>output_map <span style=color:#f92672>=</span> ESNpredict(esn, return_map_size, W_out)
</span></span><span style=display:flex><span>max_esn <span style=color:#f92672>=</span> local_maxima(output_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>max_ode <span style=color:#f92672>=</span> local_maxima(return_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>scatter(max_ode[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_ode[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Actual&#34;</span>)
</span></span><span style=display:flex><span>scatter!(max_esn[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_esn[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Predicted&#34;</span>)
</span></span><span style=display:flex><span>xlims!((<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>))
</span></span><span style=display:flex><span>ylims!(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86643189-c6f6e100-bfdc-11ea-9756-a8a7bb6e1ccf.png alt=oesn_map></p><p>It is nice to see that our results are in line with what is displayed in the paper <a href=#2>[2]</a>.</p><h2 id=ridge-esn>Ridge ESN<a hidden class=anchor aria-hidden=true href=#ridge-esn>#</a></h2><p>One usual problem that can be encountered when dealing with OLS is the insurgence of numerical instabilities when inverting \( (\textbf{X} \textbf{X}^T) \) <a href=#2>[2]</a> where \( \textbf{X} \) is the feature matrix (states matrix in the case of ESNs). A solution to this is to apply a regularization to the loss function, and one of the most common is the \( L_2 \) regularization. This way we obtain what is called ridge regression or Tikhonov regularization. The ridge ESN is trained in an equal manner as the OLS ESN we discussed above, only setting a parameter <code>beta</code> different than zero. The parameter that we chose is by no mean optimized and it is chosen by manual search, and this holds sadly for all the parameters in the models here presented. In the Conclusions section we will talk a little more about this aspect of this work.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>degree <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.001</span>
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLAT2()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Random<span style=color:#f92672>.</span>seed!(<span style=color:#ae81ff>4242</span>)
</span></span><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(approx_res_size, train, degree, radius, 
</span></span><span style=display:flex><span>    activation <span style=color:#f92672>=</span> activation, alpha <span style=color:#f92672>=</span> alpha, sigma <span style=color:#f92672>=</span> sigma, nla_type <span style=color:#f92672>=</span> nla_type, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> W_out <span style=color:#f92672>=</span> ESNtrain(esn, beta)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> ESNpredict(esn, predict_len, W_out);
</span></span></code></pre></div><pre tabindex=0><code>2.223003 seconds (5.79 M allocations: 310.671 MiB, 2.85% gc time)
</code></pre><p>This methods is just a little slower, as it has to be expected. It is still an acceptable by any means. Plotting the results we obtain:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>plot(transpose(output),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;predicted&#34;</span>)
</span></span><span style=display:flex><span>plot!(transpose(test),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;actual&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86644987-4afd9880-bfde-11ea-9994-efc19c408fdc.png alt=resn_coords>
The behaviour is similar to the standard ESN, but let&rsquo;s take a look at the short and long term.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(data, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    println(rmse(test[i,<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>], output[i, <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>]))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><pre tabindex=0><code>5.3658081215986
6.552586461430827
4.995926155420491
</code></pre><p>It was clearly visible before that the short term behaviour was not as good as the standard counterpart. The long term predictions are still acceptable, as we can see here:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>output_map <span style=color:#f92672>=</span> ESNpredict(esn, return_map_size, W_out)
</span></span><span style=display:flex><span>max_esn <span style=color:#f92672>=</span> local_maxima(output_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>max_ode <span style=color:#f92672>=</span> local_maxima(return_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>scatter(max_ode[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_ode[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Actual&#34;</span>)
</span></span><span style=display:flex><span>scatter!(max_esn[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_esn[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Predicted&#34;</span>)
</span></span><span style=display:flex><span>xlims!((<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>))
</span></span><span style=display:flex><span>ylims!(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86645536-c52e1d00-bfde-11ea-8ebc-4b9029d0ff96.png alt=resn_map></p><p>Clearly in this case the original architecture proves superior in the short term, but in the long term the both are really viable. Depending on the situation and dataset the ridge ESN can be a valide choice for accuracy and speed of training, and it could also be the only viable choice between the two, if the problem is ill-posed.</p><h2 id=lasso-esn>Lasso ESN<a hidden class=anchor aria-hidden=true href=#lasso-esn>#</a></h2><p>Another common regularization for the OLS regression is the \( L_1 \) regularization, resulting in a regression model called Lasso. This is a stronger regularization, and it shows from the results. The ESN is built in the same way as before, only this time the parameter <code>beta</code> indicates the Lasso regularizer. Since the expression doesn&rsquo;t have a closed form solution we will need a different solver, in this case <code>ProxGrad</code>. For this we will need to import a different package, called MLJLinearModels.jl.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> MLJLinearModels
</span></span><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>degree <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>*</span><span style=color:#ae81ff>10</span><span style=color:#f92672>^</span>(<span style=color:#f92672>-</span><span style=color:#ae81ff>7</span>)
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLAT2()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Random<span style=color:#f92672>.</span>seed!(<span style=color:#ae81ff>4242</span>)
</span></span><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(approx_res_size, train, degree, radius, 
</span></span><span style=display:flex><span>    activation <span style=color:#f92672>=</span> activation, alpha <span style=color:#f92672>=</span> alpha, sigma <span style=color:#f92672>=</span> sigma, nla_type <span style=color:#f92672>=</span> nla_type, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> W_out <span style=color:#f92672>=</span> ESNtrain(Lasso(beta, ProxGrad(max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>)), esn)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> ESNpredict(esn, predict_len, W_out)
</span></span></code></pre></div><pre tabindex=0><code>24.400234 seconds (9.82 M allocations: 1.534 GiB, 1.17% gc time)
</code></pre><p>The training time is slower than the couple of models we showed before. This difference is mainly to the already mentioned lack of closed form solution for the Lasso regression. Let us plot the results to start the analysis the results.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>plot(transpose(output),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;predicted&#34;</span>)
</span></span><span style=display:flex><span>plot!(transpose(test),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;actual&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86646913-ee02e200-bfdf-11ea-95de-5e8a52ee09a4.png alt=lasso_coords>
It is clear that this regularization is not capable of returning an accurate prediction, both short term and long term. Let&rsquo;s print the rmse</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(data, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    println(rmse(test[i,<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>], output[i, <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>]))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><pre tabindex=0><code>11.769042467212172
13.599065187854675
10.651859641213985
</code></pre><p>and plot the return map</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>output_map <span style=color:#f92672>=</span> ESNpredict(esn, return_map_size, W_out)
</span></span><span style=display:flex><span>max_esn <span style=color:#f92672>=</span> local_maxima(output_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>max_ode <span style=color:#f92672>=</span> local_maxima(return_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>scatter(max_ode[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_ode[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Actual&#34;</span>)
</span></span><span style=display:flex><span>scatter!(max_esn[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_esn[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Predicted&#34;</span>)
</span></span><span style=display:flex><span>xlims!((<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>))
</span></span><span style=display:flex><span>ylims!(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86647220-2c000600-bfe0-11ea-8479-46f931fff980.png alt=lasso_map></p><p>From this results is clear that the \( L_1 \) regularization is not capable of good short term prediction and in the long term yields a periodic timeseries, as we can see from the return map, only showing values in 5 contained regions.</p><h1 id=huber-loss-function>Huber loss function<a hidden class=anchor aria-hidden=true href=#huber-loss-function>#</a></h1><p>Not only the squared function can be used as a loss function: in literature it has also been proposed the use of the Huber loss function, supposedly more strong in the presence of outliers. The dataset we are using is free of them, but this function should still be able to give accurate results. Since we can apply regularization also in this case, we are going to explore the two cases already explored for the squared function: \( L_2 \) regularization and \( L_1 \) regularization.</p><h2 id=-l_2--normalization>\( L_2 \) Normalization<a hidden class=anchor aria-hidden=true href=#-l_2--normalization>#</a></h2><p>Again leveraging the MLJLinearModels package we can construct our ESN</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>degree <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.001</span>
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLAT2()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Random<span style=color:#f92672>.</span>seed!(<span style=color:#ae81ff>4242</span>)
</span></span><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(approx_res_size, train, degree, radius, 
</span></span><span style=display:flex><span>    activation <span style=color:#f92672>=</span> activation, alpha <span style=color:#f92672>=</span> alpha, sigma <span style=color:#f92672>=</span> sigma, nla_type <span style=color:#f92672>=</span> nla_type, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> W_out <span style=color:#f92672>=</span> ESNtrain(RobustHuber(<span style=color:#ae81ff>0.5</span>, beta, <span style=color:#ae81ff>0.0</span>, Newton()), esn)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> ESNpredict(esn, predict_len, W_out)
</span></span></code></pre></div><pre tabindex=0><code>9.397286 seconds (14.37 M allocations: 1.748 GiB, 2.88% gc time)
</code></pre><p>The training time is less than the Lasso regularization, but more than the OLS and Ridge training. Plotting the data we obtain:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>plot(transpose(output),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;predicted&#34;</span>)
</span></span><span style=display:flex><span>plot!(transpose(test),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;actual&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86648435-26ef8680-bfe1-11ea-963d-5da5b57f26e2.png alt=huber_coords></p><p>Let&rsquo;s go explore the rmse for the short term:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(data, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    println(rmse(test[i,<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>], output[i, <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>]))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><pre tabindex=0><code>5.36580225918975
6.552573774882654
4.995989255679152
</code></pre><p>The results seem similar to the Ridge ESN. For the long term the return map shows the following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>output_map <span style=color:#f92672>=</span> ESNpredict(esn, return_map_size, W_out)
</span></span><span style=display:flex><span>max_esn <span style=color:#f92672>=</span> local_maxima(output_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>max_ode <span style=color:#f92672>=</span> local_maxima(return_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>scatter(max_ode[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_ode[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Actual&#34;</span>)
</span></span><span style=display:flex><span>scatter!(max_esn[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_esn[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Predicted&#34;</span>)
</span></span><span style=display:flex><span>xlims!((<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>))
</span></span><span style=display:flex><span>ylims!(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86648579-471f4580-bfe1-11ea-84cf-af89c1443b3d.png alt=huber_map></p><p>Even though they are not as clear cut as the OLS ESN and Ridge ESN the long term behaviour is still acceptable.</p><h2 id=-l_1--normalization>\( L_1 \) Normalization<a hidden class=anchor aria-hidden=true href=#-l_1--normalization>#</a></h2><p>Since the Lasso ESN showed the worst results of all the model seen until now this could indicate that the \( L_1 \) norm is not suited for this task. To have confirmation of this intuition we can train the Huber ESN with the \( L_1 \) norm to see if it yields better results than the Lasso ESN.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>degree <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>*</span><span style=color:#ae81ff>10</span><span style=color:#f92672>^</span>(<span style=color:#f92672>-</span><span style=color:#ae81ff>7</span>)
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLAT2()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Random<span style=color:#f92672>.</span>seed!(<span style=color:#ae81ff>4242</span>)
</span></span><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(approx_res_size, train, degree, radius, 
</span></span><span style=display:flex><span>    activation <span style=color:#f92672>=</span> activation, alpha <span style=color:#f92672>=</span> alpha, sigma <span style=color:#f92672>=</span> sigma, nla_type <span style=color:#f92672>=</span> nla_type, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> W_out <span style=color:#f92672>=</span> ESNtrain(RobustHuber(<span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.0</span>, beta, ProxGrad(max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>)), esn)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> ESNpredict(esn, predict_len, W_out)
</span></span></code></pre></div><pre tabindex=0><code>30.699361 seconds (12.60 M allocations: 3.398 GiB, 1.40% gc time)
</code></pre><p>As expected the training time is in line with the Lasso ESN. Plotting the results we can see that sadly they are worst than the Lasso counterpart</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>plot(transpose(output),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;predicted&#34;</span>)
</span></span><span style=display:flex><span>plot!(transpose(test),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;actual&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86649348-f65c1c80-bfe1-11ea-93a6-dc252b21a2ea.png alt=huberl1_coords></p><p>Calculating the rmse for the first 400 steps returns</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(data, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    println(rmse(test[i,<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>], output[i, <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>]))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><pre tabindex=0><code>8.247263444106135
9.896322405158461
12.569601968865513
</code></pre><p>And the return map clearly shows a periodic long term behaviour:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>output_map <span style=color:#f92672>=</span> ESNpredict(esn, return_map_size, W_out)
</span></span><span style=display:flex><span>max_esn <span style=color:#f92672>=</span> local_maxima(output_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>max_ode <span style=color:#f92672>=</span> local_maxima(return_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>scatter(max_ode[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_ode[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Actual&#34;</span>)
</span></span><span style=display:flex><span>scatter!(max_esn[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_esn[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Predicted&#34;</span>)
</span></span><span style=display:flex><span>xlims!((<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>))
</span></span><span style=display:flex><span>ylims!(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86649575-2c999c00-bfe2-11ea-9a14-008e62d980f4.png alt=huberl1_map></p><p>The periodicity is more pronounced than the Lasso ESN, making this model the worst performing so far on this task.</p><h2 id=different-reservoir-construction>Different reservoir construction<a hidden class=anchor aria-hidden=true href=#different-reservoir-construction>#</a></h2><p>For all the model proposed we used the standard construction of the reservoir, based on the rescaling of the spectral radius to be less than a given value. In literature there are other alternatives explored and in the ReservoirComputing.jl package is present the implementation of an algorithm for the construction of the reservoir matrix based on the Single Value Decomposition (SVD), proposed in <a href=#4>[4]</a>. We are going to give the results using this construction only for the OLS ESN, but a more wide study could be done comparing the performances of all the proposed models using the two different implementations of the reservoir.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>max_value <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>sparsity <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLAT2()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Random<span style=color:#f92672>.</span>seed!(<span style=color:#ae81ff>4242</span>)
</span></span><span style=display:flex><span>W_in <span style=color:#f92672>=</span> init_dense_input_layer(approx_res_size, size(train, <span style=color:#ae81ff>1</span>), sigma)
</span></span><span style=display:flex><span>W_new <span style=color:#f92672>=</span> pseudoSVD(approx_res_size, max_value, sparsity, reverse_sort <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(W_new, train, W_in, 
</span></span><span style=display:flex><span>    activation <span style=color:#f92672>=</span> activation, alpha <span style=color:#f92672>=</span> alpha, nla_type <span style=color:#f92672>=</span> nla_type, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> W_out <span style=color:#f92672>=</span> ESNtrain(esn, beta)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> ESNpredict(esn, predict_len, W_out)
</span></span></code></pre></div><pre tabindex=0><code>0.994207 seconds (3.18 M allocations: 188.436 MiB, 3.64% gc time)
</code></pre><p>The training time is as fast as the normal OLS ESN, as it is to be expected. Plotting the result we obtain</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>plot(transpose(output),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;predicted&#34;</span>)
</span></span><span style=display:flex><span>plot!(transpose(test),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;actual&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86665455-f06d3800-bfef-11ea-8129-7e94186ad550.png alt=noesn_coords>
From the plot it seems that the model is producing an acceptable Lorenz system surrogate dataset. The short term will not be the best but in the long term maybe the model could behave as the OLS ESN.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(data, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    println(rmse(test[i,<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>], output[i, <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>]))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><pre tabindex=0><code>8.299351042330624
9.954728072605832
9.507147274321735
</code></pre><p>Not as good as the normal reservoir counterpart, but let&rsquo;s look at the return map for the long term behaviour:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>output_map <span style=color:#f92672>=</span> ESNpredict(esn, return_map_size, W_out)
</span></span><span style=display:flex><span>max_esn <span style=color:#f92672>=</span> local_maxima(output_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>max_ode <span style=color:#f92672>=</span> local_maxima(return_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>scatter(max_ode[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_ode[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Actual&#34;</span>)
</span></span><span style=display:flex><span>scatter!(max_esn[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_esn[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Predicted&#34;</span>)
</span></span><span style=display:flex><span>xlims!((<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>))
</span></span><span style=display:flex><span>ylims!(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86666199-a173d280-bff0-11ea-8ba7-c46906606627.png alt=noesn_map></p><p>Event though it seemed that the predicted coordinates had a similar behaviour as the actual Lorenz system the return map clearly shows that in the long term the behaviour is not consistent. This approach to reservoir construction is of course pretty novel and has to pass several optimization steps for its parameters in order to be production ready, but this first test is somewhat disappointing.</p><h1 id=echo-state-gaussian-processes>Echo State Gaussian Processes<a hidden class=anchor aria-hidden=true href=#echo-state-gaussian-processes>#</a></h1><p>Firstly proposed in <a href=#3>[3]</a> the Echo State Gaussian Processes (ESGP) can be considered an extension of the ESN. In the original paper only the Radial Basis function is explored for the prediction, but in this post we wanted to give a couple of examples of others kernels, so we will use also the Matern kernel and the Polynomial kernel for the prediction of the Lorenz system. The construction of this model is based on GaussianProcesses, so in the first run we will need to import that package as well.</p><h2 id=radial-basis-kernel>Radial Basis kernel<a hidden class=anchor aria-hidden=true href=#radial-basis-kernel>#</a></h2><p>Starting from the kernel used in the original paper, we also set the non linear algorithm to the default one, equal to none. The behaviour with different algorithms for this family of models has not been investigated and could be subject of future works, but for now we will just limit the work to the standard one. Keeping the other parameters equal the ESGP can be built in the following way</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> GaussianProcesses
</span></span><span style=display:flex><span><span style=color:#75715e>#model parameters</span>
</span></span><span style=display:flex><span>degree <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLADefault()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#create echo state network</span>
</span></span><span style=display:flex><span>Random<span style=color:#f92672>.</span>seed!(<span style=color:#ae81ff>4242</span>)
</span></span><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(approx_res_size, train, degree, radius, 
</span></span><span style=display:flex><span>    activation <span style=color:#f92672>=</span> activation, alpha <span style=color:#f92672>=</span> alpha, sigma <span style=color:#f92672>=</span> sigma, nla_type <span style=color:#f92672>=</span> nla_type, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mean <span style=color:#f92672>=</span> MeanZero()
</span></span><span style=display:flex><span>kernel <span style=color:#f92672>=</span> SE(<span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> gp <span style=color:#f92672>=</span> ESGPtrain(esn, mean, kernel, lognoise <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>2.0</span>, optimize <span style=color:#f92672>=</span> false);
</span></span><span style=display:flex><span>output, sigmas <span style=color:#f92672>=</span> ESGPpredict(esn, predict_len, gp)
</span></span></code></pre></div><pre tabindex=0><code>43.323255 seconds (6.81 M allocations: 2.053 GiB, 2.53% gc time)
</code></pre><p>The slowest time so far, but maybe the results will be worth the extra seconds it took to train</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>plot(transpose(output),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;predicted&#34;</span>)
</span></span><span style=display:flex><span>plot!(transpose(test),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;actual&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86651927-3cb27b00-bfe4-11ea-94aa-33299e4d5db5.png alt=radialbasis_coords>
The results do not look bad, in the short term sadly emerges a discrepancy early on that will lower the rmse values</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(data, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    println(rmse(test[i,<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>], output[i, <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>]))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><pre tabindex=0><code>10.5883475480527
11.759694995689005
6.745353916847517
</code></pre><p>As expected the rmses are not the greatest, but the model can still recover with a nice display of long term behaviour</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>output_map, sigma_map <span style=color:#f92672>=</span> ESGPpredict(esn, return_map_size, gp)
</span></span><span style=display:flex><span>max_esn <span style=color:#f92672>=</span> local_maxima(output_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>max_ode <span style=color:#f92672>=</span> local_maxima(return_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>scatter(max_ode[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_ode[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Actual&#34;</span>)
</span></span><span style=display:flex><span>scatter!(max_esn[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_esn[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Predicted&#34;</span>)
</span></span><span style=display:flex><span>xlims!((<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>))
</span></span><span style=display:flex><span>ylims!(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86664190-c36c5580-bfee-11ea-98dd-ff1e420430e2.png alt=radialbasis_map></p><p>and indeed it does, outside of a single point in a strange location the model seems to capture the Lorenz climate quite well, not quite as good as the OLS ESN or even the Ridge ESN.</p><h2 id=matern-kernel>Matern kernel<a hidden class=anchor aria-hidden=true href=#matern-kernel>#</a></h2><p>Another common kernel is the Matern kernel, and training the ESGP using this kernel is a straightforward process, identical to the one we just followed for the Radial Basis kernel:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#75715e>#model parameters</span>
</span></span><span style=display:flex><span>degree <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLADefault()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#create echo state network</span>
</span></span><span style=display:flex><span>Random<span style=color:#f92672>.</span>seed!(<span style=color:#ae81ff>4242</span>)
</span></span><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(approx_res_size, train, degree, radius, 
</span></span><span style=display:flex><span>    activation <span style=color:#f92672>=</span> activation, alpha <span style=color:#f92672>=</span> alpha, sigma <span style=color:#f92672>=</span> sigma, nla_type <span style=color:#f92672>=</span> nla_type, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mean <span style=color:#f92672>=</span> MeanZero()
</span></span><span style=display:flex><span>kernel <span style=color:#f92672>=</span> Matern(<span style=color:#ae81ff>1</span><span style=color:#f92672>/</span><span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> gp <span style=color:#f92672>=</span> ESGPtrain(esn, mean, kernel, lognoise <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>2.0</span>, optimize <span style=color:#f92672>=</span> false);
</span></span><span style=display:flex><span>output, sigmas <span style=color:#f92672>=</span> ESGPpredict(esn, predict_len, gp)
</span></span></code></pre></div><pre tabindex=0><code>40.309179 seconds (1.08 M allocations: 1.761 GiB, 7.58% gc time)
</code></pre><p>Plotting the results</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>plot(transpose(output),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;predicted&#34;</span>)
</span></span><span style=display:flex><span>plot!(transpose(test),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;actual&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86654732-78e6db00-bfe6-11ea-9fbd-e117b2abbcd5.png alt=matern_coords>
The results seem similar to the ones obtained using the Radial Basis kernel. To be sure of this we need to calculate the rmse</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(data, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    println(rmse(test[i,<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>], output[i, <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>]))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><pre tabindex=0><code>7.150484170664864
8.520570924338477
7.037257939507858
</code></pre><p>The lower rmses shows a better short term prediction. Plotting the return map to analize the long term results</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>output_map, sigma_map <span style=color:#f92672>=</span> ESGPpredict(esn, return_map_size, gp)
</span></span><span style=display:flex><span>max_esn <span style=color:#f92672>=</span> local_maxima(output_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>max_ode <span style=color:#f92672>=</span> local_maxima(return_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>scatter(max_ode[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_ode[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Actual&#34;</span>)
</span></span><span style=display:flex><span>scatter!(max_esn[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_esn[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Predicted&#34;</span>)
</span></span><span style=display:flex><span>xlims!((<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>))
</span></span><span style=display:flex><span>ylims!(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86655320-e8f56100-bfe6-11ea-8c82-83b92eec4943.png alt=matern_map>
The return map is not a clear cut as we saw in other models, but for the majority of the times it seems that the model is still capable of retaining the climate of the Lorenz system.</p><h2 id=polynomial-kernel>Polynomial kernel<a hidden class=anchor aria-hidden=true href=#polynomial-kernel>#</a></h2><p>The last kernel for the ESGP is the Polynomial kernel. We are now familiar with the construction</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#75715e>#model parameters</span>
</span></span><span style=display:flex><span>degree <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLADefault()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#create echo state network</span>
</span></span><span style=display:flex><span>Random<span style=color:#f92672>.</span>seed!(<span style=color:#ae81ff>4242</span>)
</span></span><span style=display:flex><span>esn <span style=color:#f92672>=</span> ESN(approx_res_size, train, degree, radius, 
</span></span><span style=display:flex><span>    activation <span style=color:#f92672>=</span> activation, alpha <span style=color:#f92672>=</span> alpha, sigma <span style=color:#f92672>=</span> sigma, nla_type <span style=color:#f92672>=</span> nla_type, extended_states <span style=color:#f92672>=</span> extended_states)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mean <span style=color:#f92672>=</span> MeanZero()
</span></span><span style=display:flex><span>kernel <span style=color:#f92672>=</span> Poly(<span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span><span style=color:#a6e22e>@time</span> gp <span style=color:#f92672>=</span> ESGPtrain(esn, mean, kernel, lognoise <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>2.0</span>, optimize <span style=color:#f92672>=</span> false);
</span></span><span style=display:flex><span>output, sigmas <span style=color:#f92672>=</span> ESGPpredict(esn, predict_len, gp)
</span></span></code></pre></div><pre tabindex=0><code>16.979100 seconds (4.27 M allocations: 3.025 GiB, 10.13% gc time)
</code></pre><p>Plotting the results</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>plot(transpose(output),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;predicted&#34;</span>)
</span></span><span style=display:flex><span>plot!(transpose(test),layout<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;actual&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86655904-5dc89b00-bfe7-11ea-97b9-1887bdd2c2bc.png alt=poly_coords>
we can see a nice prediction on the short term. Using once again the rmse</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>for</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>size(data, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    println(rmse(test[i,<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>], output[i, <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>400</span>]))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><pre tabindex=0><code>0.9128150765679983
1.3969462542792563
1.6437787377104938
</code></pre><p>The short term rmse are the best out of the ESGP kernel used until now. Taking a look also to the long term we can see the following</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>output_map, sigma_map <span style=color:#f92672>=</span> ESGPpredict(esn, return_map_size, gp)
</span></span><span style=display:flex><span>max_esn <span style=color:#f92672>=</span> local_maxima(output_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>max_ode <span style=color:#f92672>=</span> local_maxima(return_map[<span style=color:#ae81ff>3</span>,<span style=color:#f92672>:</span>])
</span></span><span style=display:flex><span>scatter(max_ode[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_ode[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Actual&#34;</span>)
</span></span><span style=display:flex><span>scatter!(max_esn[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], max_esn[<span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#66d9ef>end</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Predicted&#34;</span>)
</span></span><span style=display:flex><span>xlims!((<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>))
</span></span><span style=display:flex><span>ylims!(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>47</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/86656626-e5160e80-bfe7-11ea-9d77-8754ea0a60f2.png alt=poly_map></p><p>The results are in line with the OLS ESN. Considering that the results of the OLS ESN were obtained following a published paper and underwent major optimization while the parameters we have chosen are the fruit of a quick manul search we can say that this model could outperform the ESN for chaotic time series prediction.</p><h1 id=conclusions>Conclusions<a hidden class=anchor aria-hidden=true href=#conclusions>#</a></h1><p>This post is just scratching the surface on the studies needed for this family of models. As we mentioned numerous times throughout the post, the parameter optimization that was done for this wark was just acceptable, being a manual search starting from values optimized for a specific model (OLS ESN). This fact was most evident when testing the SVD based reservoir construction: even though it is a very similar resulting matrix the results were suboptimal. This could have been just an error in the selection of the parameters, and with a more suiting set this construction could perform as well as the standard one.</p><p>But even with more carefully chosen parameters it is necessary to do multiple runs and obtain a large pool of statistics in order to label a model variation more efficient than the other: in this post we have seen that the Polynomial ESGP has returned amazing results, but changing the seed and using a different reservoir the best model could very well be the radial basis ESGP, or the Huber ESN. This work was based on the single run of all the models because is more intended to be a showcase of the new implementations done for the GSoC project, and the reproducibility was the most important aspect of it.</p><p>Sadly in this post we weren&rsquo;t able to showcase the Support Vector Echo State Machines (SVESMs) but the results they obtained wasn&rsquo;t in line with any of the proposed models, and it performed way worst than even the \( L_1 \) trained ESNs. The paper proposing the models leveraged it to solve a different family of problems, so it could be that this task is not suited for this particular variation of the ESN.</p><p>Regarding the ESGP the possible directions for future studies are really numerous: there are a vast number of other avaiable kernels that we didn&rsquo;t explore, and even in the one we used the possibility to optimize the parameter is built in in the model, and in our case wasn&rsquo;t used just for time limitations. Not only it is possible to obtain better results that the one we showed purely by parameter optimization but also by using different kernels at the same time: in the Gaussian Processes is usual to see different kernels combined togheter, through multiplication and addition. This possibility adds an incredible perspective to this model, and I hope future studies will takle it.</p><p>As usual thanks for reading, if you spot any mistake or are just curious about the model and implementation don&rsquo;t hesitate to contact me.</p><h2 id=citing>Citing<a hidden class=anchor aria-hidden=true href=#citing>#</a></h2><p>Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bibtex data-lang=bibtex><span style=display:flex><span><span style=color:#a6e22e>@article</span>{JMLR:v23:22-0611,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>author</span>  = <span style=color:#e6db74>{Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>title</span>   = <span style=color:#e6db74>{ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>journal</span> = <span style=color:#e6db74>{Journal of Machine Learning Research}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>year</span>    = <span style=color:#e6db74>{2022}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>volume</span>  = <span style=color:#e6db74>{23}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>number</span>  = <span style=color:#e6db74>{288}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>pages</span>   = <span style=color:#e6db74>{1--8}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>url</span>     = <span style=color:#e6db74>{http://jmlr.org/papers/v23/22-0611.html}</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=documentation>Documentation<a hidden class=anchor aria-hidden=true href=#documentation>#</a></h2><p>[1]
Pathak, Jaideep, et al. &ldquo;Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data.&rdquo; Chaos: An Interdisciplinary Journal of Nonlinear Science 27.12 (2017): 121102.</p><p>[2]
Lukoševičius, Mantas. &ldquo;A practical guide to applying echo state networks.&rdquo; Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 659-686.</p><p>[3]
Chatzis, Sotirios P., and Yiannis Demiris. &ldquo;Echo state Gaussian process.&rdquo; IEEE Transactions on Neural Networks 22.9 (2011): 1435-1445.</p><p>[4]
Yang, Cuili, et al. &ldquo;Design of polynomial echo state networks for time series prediction.&rdquo; Neurocomputing 290 (2018): 148-160.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://martinuzzifrancesco.github.io/>Francesco Martinuzzi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>