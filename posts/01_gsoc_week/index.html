<!DOCTYPE html>
<html lang="">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>GSoC week 1: lasso, Elastic Net and Huber loss - Francesco Martinuzzi</title><meta name="Description" content="Francesco Martinuzzi"><meta property="og:title" content="GSoC week 1: lasso, Elastic Net and Huber loss" />
<meta property="og:description" content="The first week tackled the implementation of different kind of linear regression for the creation of the last layer in the Echo State Network. More specifically were added the possibility to add a \( l_1 \) regularization to the loss function (Lasso regression), both \( l_1 \) and \( l_2 \) regularizations (Elastic Net regression) and also added the possibility to choose the Huber loss function. As in the last post we will start from a brief theoretical background to explain the code and then we will showcase some examples taken from the literature." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/01_gsoc_week/" /><meta property="og:image" content="https://martinuzzifrancesco.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-07T19:06:37+02:00" />
<meta property="article:modified_time" content="2020-06-07T19:06:37+02:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://martinuzzifrancesco.github.io/logo.png"/>

<meta name="twitter:title" content="GSoC week 1: lasso, Elastic Net and Huber loss"/>
<meta name="twitter:description" content="The first week tackled the implementation of different kind of linear regression for the creation of the last layer in the Echo State Network. More specifically were added the possibility to add a \( l_1 \) regularization to the loss function (Lasso regression), both \( l_1 \) and \( l_2 \) regularizations (Elastic Net regression) and also added the possibility to choose the Huber loss function. As in the last post we will start from a brief theoretical background to explain the code and then we will showcase some examples taken from the literature."/>
<meta name="application-name" content="Francesco Martinuzzi">
<meta name="apple-mobile-web-app-title" content="Francesco Martinuzzi"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://martinuzzifrancesco.github.io/posts/01_gsoc_week/" /><link rel="prev" href="https://martinuzzifrancesco.github.io/posts/a-brief-introduction-to-reservoir-computing/" /><link rel="next" href="https://martinuzzifrancesco.github.io/posts/02_gsoc_week/" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "GSoC week 1: lasso, Elastic Net and Huber loss",
        "inLanguage": "",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/martinuzzifrancesco.github.io\/posts\/01_gsoc_week\/"
        },"image": ["https:\/\/martinuzzifrancesco.github.io\/images\/Apple-Devices-Preview.png"],"genre": "posts","wordcount":  1502 ,
        "url": "https:\/\/martinuzzifrancesco.github.io\/posts\/01_gsoc_week\/","datePublished": "2020-06-07T19:06:37+02:00","dateModified": "2020-06-07T19:06:37+02:00","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/martinuzzifrancesco.github.io\/images\/avatar.png",
                    "width":  892 ,
                    "height":  892 
                }},"author": {
                "@type": "Person",
                "name": "Francesco Martinuzzi"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('light' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'light' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Francesco Martinuzzi">Francesco Martinuzzi</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/" title="What have I done"> Posts </a><a class="menu-item" href="/about/" title="Who am I"> About </a><a class="menu-item" href="/research/" title="What do I do"> Research </a><a class="menu-item" href="/contact/" title="How to reach me"> Contact </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Francesco Martinuzzi">Francesco Martinuzzi</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="What have I done">Posts</a><a class="menu-item" href="/about/" title="Who am I">About</a><a class="menu-item" href="/research/" title="What do I do">Research</a><a class="menu-item" href="/contact/" title="How to reach me">Contact</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">GSoC week 1: lasso, Elastic Net and Huber loss</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>Francesco Martinuzzi</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="7076-67-06">7076-67-06</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;1502 words&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;8 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#ols-and-penalization">OLS and penalization</a></li>
    <li><a href="#huber-loss-function">Huber loss function</a></li>
    <li><a href="#implementation-in-reservoircomputingjl">Implementation in ReservoirComputing.jl</a></li>
  </ul>

  <ul>
    <li><a href="#references">References</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>The first week tackled the implementation of different kind of linear regression for the creation of the last layer in the Echo State Network. More specifically were added the possibility to add a \( l_1 \) regularization to the loss function (Lasso regression), both \( l_1 \) and \( l_2 \) regularizations (Elastic Net regression) and also added the possibility to choose the Huber loss function.
As in the last post we will start from a brief theoretical background to explain the code and then we will showcase some examples taken from the literature.</p>
<h1 id="theoretical-background">Theoretical Background</h1>
<p>In the <a href="https://martinuzzifrancesco.github.io/posts/a-brief-introduction-to-reservoir-computing/" target="_blank" rel="noopener noreffer">Brief Introduction to Reservoir Computing</a> we showed how it was possible to get the output layer from a linear regression over the states and the desired output using <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization" target="_blank" rel="noopener noreffer">Ridge regression</a>:</p>
<p>$$\textbf{W}_{\text{out}} = \textbf{Y}^{\text{target}} \textbf{X}^{\text{T}}(\textbf{X} \textbf{X}^{\text{T}} + \beta \textbf{I})^{-1}$$
but by doing so we actually jumped a few steps, and in the example it wasn&rsquo;t even used actually, it was just an <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares" target="_blank" rel="noopener noreffer">Ordinary Least Squares</a>(OLS). To know the difference we have to take a little step back. Inherently generalised linear regression models are an optimisation problem of the form</p>
<p>$$L(\textbf{y}, \textbf{X} \theta)+P(\theta)$$</p>
<p>where</p>
<ul>
<li>\( \textbf{y} \) is the target</li>
<li>\( \textbf{X} \) is the design matrix</li>
<li>\( \theta \) is a vector of coefficient to determine</li>
<li>\( L \) is a loss function</li>
<li>\( P \) is a penalty function</li>
</ul>
<h2 id="ols-and-penalization">OLS and penalization</h2>
<p>In the case of Ridge regression the loss function is the OLS, to wich is added a \( l_2 \) regularization. The function to minimize is of the form</p>
<p>$$||\textbf{y} - \textbf{X} \theta ||_2^2 + \lambda || \theta  ||_2^2$$</p>
<p>where \( ||.||_2 \) is the \( l_2 \) norm and \( \lambda \) is a penalization coefficient. In the Lorenz system example the lambda parameter was set to zero so in fact we were actually fitting using only the first part of the above expression, that corresponds to OLS, as said before. The formula we showed in the opening is actually quite different from this second definition, but this is because even though this is an optimisation problem the Ridge regression has a closed form solution. So if we imagine to have a matrix of targets \(\textbf{Y}\) and \(\theta = \textbf{W}_{\text{out}} \) then the first definition can be derived from the second.</p>
<p>Another form of regression based on the OLS loss function is <a href="https://en.wikipedia.org/wiki/Lasso_%28statistics%29" target="_blank" rel="noopener noreffer">Lasso</a> (least absolute shrinkage and selection operator) which uses the \( l_1 \) norm as regularization. The function to minimize will hence have the form</p>
<p>$$||\textbf{y} - \textbf{X} \theta ||_2^2 + \lambda || \theta  ||_1$$</p>
<p>This two methods can be linearly combined in order to obtain the <a href="https://en.wikipedia.org/wiki/Elastic_net_regularization" target="_blank" rel="noopener noreffer">Elastic Net regression</a> method, of the form</p>
<p>$$||\textbf{y} - \textbf{X} \theta ||_2^2 + \lambda || \theta  ||_2^2 + \gamma || \theta  ||_1$$</p>
<p>This last two methods, contrarily to Ridge regression, do not present a closed form and so one has to use other solutions to the optimization problem, such as <a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener noreffer">gradient descent</a> or <a href="https://en.wikipedia.org/wiki/Proximal_gradient_method" target="_blank" rel="noopener noreffer">proximal gradient method</a>.</p>
<h2 id="huber-loss-function">Huber loss function</h2>
<p>Of course one can choose other alternatives to the OLS loss function, and one of the most common is the <a href="https://en.wikipedia.org/wiki/Huber_loss" target="_blank" rel="noopener noreffer">Huber loss</a> function. Used in robust regression is known to respond well in the presence of outliers. The function is defined as follows</p>
<p>$$L_{\sigma}(a) = \frac{1}{2}a^2   \text{  for  } |a| \le \sigma$$
$$L_{\sigma}(a) = \sigma (|a| - \frac{1}{2} \sigma ) \text{   otherwise}$$</p>
<p>To this function we can apply the same regularization function priorly defined, the \( l_2 \) and \( l_1 \) norm if one so choses.</p>
<h2 id="implementation-in-reservoircomputingjl">Implementation in ReservoirComputing.jl</h2>
<p>The implementation in the library has been done leveraging the incredible job done by the MLJLinearModels.jl team. The <code>ESNtrain()</code> function can now take as argument the following structs:</p>
<ul>
<li><code>Ridge(lambda, solver)</code></li>
<li><code>Ridge(lambda, solver)</code></li>
<li><code>ElastNet(lambda, gamma, solver)</code></li>
<li><code>RobustHuber(delta, lambda, gamma, solver)</code></li>
</ul>
<p>where lambda gamma and delta are defined in the theoretical sections above and solver is a solver of the MLJLinearModels library. One must be careful to use the suitable solver for every loss and regularization combination. Further information can be found in the <a href="https://alan-turing-institute.github.io/MLJLinearModels.jl/stable/" target="_blank" rel="noopener noreffer">MLJLinearModels documentation</a>.</p>
<h1 id="examples">Examples</h1>
<p>The Lasso regression was first proposed in <a href="#1" rel="">[1]</a> and in <a href="#2" rel="">[2]</a> a variation is proposed on it and there are also comparison with Elastic Net regression. Other comparison are carried out in <a href="#3" rel="">[3]</a>, which is the paper we will follow as methodology. The Huber loss function is used for comparison in <a href="#4" rel="">[4]</a>, but to my knowledge has not been adopted in other papers.</p>
<p>Trying to follow the data preparation used in <a href="#3" rel="">[3]</a> we use the Rossler system this time to carry out our tests. The system is defined by the equations
$$\frac{dx}{dt} = -y -z$$
$$\frac{dy}{dt} = x + ay$$
$$\frac{dz}{dt} = b + z(x - c)$$
andit exhibits chaotic behavior for \( a = 0.2 \), \( b = 0.2 \) and \( c = 5.7 \). Using Range Kutta of order 4 from the initial positions \( (-1, 0, 3) \) the time series is generated with step size set to 0.005. In the paper the attractor is reconstructed in the phase space using embedding dimensions \( (3, 3, 3) \) and time delays \( (13, 13, 13) \) for the \(x, y, z\) series respectively. After, all the 9 resulting timeseries are rescaled in the range \( [-1, 1] \) and will all be used in the training of the ESN. Since the data preparation was unusual for me I spent a couple of hours wrapping my head around it. If one wants to know more about time delays and embeddings a good brief introduction is given in <a href="#5" rel="">[5]</a>. Thankfully <a href="https://juliadynamics.github.io/DynamicalSystems.jl/latest/" target="_blank" rel="noopener noreffer">DynamicalSystems.jl</a> makes lifes easier when dealing with this type of problems and using <code>embed()</code> I was quickly able to create the data as expressed in the paper. <a href="https://juliastats.org/StatsBase.jl/stable/" target="_blank" rel="noopener noreffer">StatsBase.jl</a> dealt with the rescaling part.</p>
<p>The parameter for the ESN are then set as follows</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="k">using</span> <span class="n">ReservoirComputing</span>
<span class="n">shift</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">sparsity</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">approx_res_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">radius</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">tanh</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">train_len</span> <span class="o">=</span> <span class="mi">3900</span>
<span class="n">predict_len</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">lambda</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">)</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">nla_type</span> <span class="o">=</span> <span class="n">NLADefault</span><span class="p">()</span>
<span class="n">extended_states</span> <span class="o">=</span> <span class="nb">true</span>

<span class="n">h</span> <span class="o">=</span> <span class="mi">1</span>
</code></pre></td></tr></table>
</div>
</div><p>The test was based on a h steps ahead prediction, which differs from the normal prediction because after every h steps of the ESN running autonomosly after training, the actual data is fed into the model, &ldquo;correcting&rdquo; the results. This way one has also to feed test data into to model, and the error is consequently quite low. As we can see the <code>h</code> parameter is set on 1, since that is the step used in the paper. The model only predicts one step in the future this way, for every step of the prediction.</p>
<p>To test the difference between values we used a user-defined <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation" target="_blank" rel="noopener noreffer">Root Mean Square Deviation</a> (RMSE) for the x coordinate, following paper guidelines. The results are as follows, given as a mean of 20 different initiations of random reservoirs:</p>
<p>$$  \text{rmse}_{\text{RESN}} = 9.033 \cdot 10^{-5} $$
$$  \text{rmse}_{\text{LESN}} = 0.006 $$
$$  \text{rmse}_{\text{EESN}} = 0.006 $$
$$  \text{rmse}_{\text{HESN}} = 9.040 \cdot 10^{-5} $$</p>
<p>where RESN is the Ridge regression trained ESN, LESN is the Lasso trained ESN, EESN is the Elastic Net trained ESN and HESN is the ESN trained with Huber function with \( \delta = 0.8\) and \( l_2 \) norm.</p>
<p>We can also take a look at a plot of the x coordinate bot actual and predicted, but as one can expect from a rmse so small there is almost no difference.</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://user-images.githubusercontent.com/10376688/83980445-41690e00-a916-11ea-8e0b-b65edd7a57d0.png"
        data-srcset="https://user-images.githubusercontent.com/10376688/83980445-41690e00-a916-11ea-8e0b-b65edd7a57d0.png, https://user-images.githubusercontent.com/10376688/83980445-41690e00-a916-11ea-8e0b-b65edd7a57d0.png 1.5x, https://user-images.githubusercontent.com/10376688/83980445-41690e00-a916-11ea-8e0b-b65edd7a57d0.png 2x"
        data-sizes="auto"
        alt="https://user-images.githubusercontent.com/10376688/83980445-41690e00-a916-11ea-8e0b-b65edd7a57d0.png"
        title="Rossler_coord" /></p>
<p>The results obtained, while not in line with what is showed in the literature, are actually far better, by several orders of magnitude in some instances. While not obtaining the comparison with the papers is somewhat not optimal, there are some problems with the paper examined: the presence of several blunders in the final published draft, sometimes really evident, do not give ground to the dismissal of results but can raise eyebrows to the transparency of the methods or data utilized.
In conclusion, the implementations to the ESN show promising results, but a more thorough exploration and examination needs to be done in order to really showcase their true utility. There will be time for that after GSoC ends, hopefully.</p>
<p>As always, if you have any questions regarding the model, the package or you have found errors in my post, please don&rsquo;t hesitate to contact me!</p>
<h2 id="references">References</h2>
<p><a id="1">[1]</a>
Han, Min, Wei-Jie Ren, and Mei-Ling Xu. &ldquo;An improved echo state network via L1-norm regularization.&rdquo; Acta Automatica Sinica 40.11 (2014): 2428-2435.</p>
<p><a id="2">[2]</a>
Xu, Meiling, Min Han, and Shunshoku Kanae. &ldquo;L 1/2 Norm Regularized Echo State Network for Chaotic Time Series Prediction.&rdquo; International Conference on Neural Information Processing. Springer, Cham, 2016.</p>
<p><a id="3">[3]</a>
Xu, Meiling, and Min Han. &ldquo;Adaptive elastic echo state network for multivariate time series prediction.&rdquo; IEEE transactions on cybernetics 46.10 (2016): 2173-2183.</p>
<p><a id="4">[4]</a>
Guo, Yu, et al. &ldquo;Robust echo state networks based on correntropy induced loss function.&rdquo; Neurocomputing 267 (2017): 295-303.</p>
<p><a id="5">[5]</a>
<a href="http://www.scholarpedia.org/article/Attractor_reconstruction" target="_blank" rel="noopener noreffer">http://www.scholarpedia.org/article/Attractor_reconstruction</a></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 7076-67-06</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/posts/01_gsoc_week/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/a-brief-introduction-to-reservoir-computing/" class="prev" rel="prev" title="A brief introduction to Reservoir Computing"><i class="fas fa-angle-left fa-fw"></i>A brief introduction to Reservoir Computing</a>
            <a href="/posts/02_gsoc_week/" class="next" rel="next" title="GSoC week 2: Support Vector Regression in Echo State Networks">GSoC week 2: Support Vector Regression in Echo State Networks<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2020 - 2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Francesco Martinuzzi</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
