<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GSoC week 1: lasso, Elastic Net and Huber loss | Francesco Martinuzzi</title><meta name=keywords content><meta name=description content="The first week tackled the implementation of different kind of linear regression for the creation of the last layer in the Echo State Network. More specifically were added the possibility to add a \( l_1 \) regularization to the loss function (Lasso regression), both \( l_1 \) and \( l_2 \) regularizations (Elastic Net regression) and also added the possibility to choose the Huber loss function. As in the last post we will start from a brief theoretical background to explain the code and then we will showcase some examples taken from the literature."><meta name=author content="Francesco Martinuzzi"><link rel=canonical href=https://martinuzzifrancesco.github.io/posts/01_gsoc_week/><link crossorigin=anonymous href=/assets/css/stylesheet.849fd8bd636f9bdcc8fd3087509c431a61faae78bd735e7ba75e6fd13ec64f83.css integrity="sha256-hJ/YvWNvm9zI/TCHUJxDGmH6rni9c157p15v0T7GT4M=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://martinuzzifrancesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://martinuzzifrancesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://martinuzzifrancesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://martinuzzifrancesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://martinuzzifrancesco.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="GSoC week 1: lasso, Elastic Net and Huber loss"><meta property="og:description" content="The first week tackled the implementation of different kind of linear regression for the creation of the last layer in the Echo State Network. More specifically were added the possibility to add a \( l_1 \) regularization to the loss function (Lasso regression), both \( l_1 \) and \( l_2 \) regularizations (Elastic Net regression) and also added the possibility to choose the Huber loss function. As in the last post we will start from a brief theoretical background to explain the code and then we will showcase some examples taken from the literature."><meta property="og:type" content="article"><meta property="og:url" content="https://martinuzzifrancesco.github.io/posts/01_gsoc_week/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-06-07T19:06:37+02:00"><meta property="article:modified_time" content="2020-06-07T19:06:37+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GSoC week 1: lasso, Elastic Net and Huber loss"><meta name=twitter:description content="The first week tackled the implementation of different kind of linear regression for the creation of the last layer in the Echo State Network. More specifically were added the possibility to add a \( l_1 \) regularization to the loss function (Lasso regression), both \( l_1 \) and \( l_2 \) regularizations (Elastic Net regression) and also added the possibility to choose the Huber loss function. As in the last post we will start from a brief theoretical background to explain the code and then we will showcase some examples taken from the literature."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://martinuzzifrancesco.github.io/posts/"},{"@type":"ListItem","position":2,"name":"GSoC week 1: lasso, Elastic Net and Huber loss","item":"https://martinuzzifrancesco.github.io/posts/01_gsoc_week/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GSoC week 1: lasso, Elastic Net and Huber loss","name":"GSoC week 1: lasso, Elastic Net and Huber loss","description":"The first week tackled the implementation of different kind of linear regression for the creation of the last layer in the Echo State Network. More specifically were added the possibility to add a \\( l_1 \\) regularization to the loss function (Lasso regression), both \\( l_1 \\) and \\( l_2 \\) regularizations (Elastic Net regression) and also added the possibility to choose the Huber loss function. As in the last post we will start from a brief theoretical background to explain the code and then we will showcase some examples taken from the literature.","keywords":[],"articleBody":"The first week tackled the implementation of different kind of linear regression for the creation of the last layer in the Echo State Network. More specifically were added the possibility to add a \\( l_1 \\) regularization to the loss function (Lasso regression), both \\( l_1 \\) and \\( l_2 \\) regularizations (Elastic Net regression) and also added the possibility to choose the Huber loss function. As in the last post we will start from a brief theoretical background to explain the code and then we will showcase some examples taken from the literature.\nTheoretical Background In the Brief Introduction to Reservoir Computing we showed how it was possible to get the output layer from a linear regression over the states and the desired output using Ridge regression:\n$$\\textbf{W}_{\\text{out}} = \\textbf{Y}^{\\text{target}} \\textbf{X}^{\\text{T}}(\\textbf{X} \\textbf{X}^{\\text{T}} + \\beta \\textbf{I})^{-1}$$ but by doing so we actually jumped a few steps, and in the example it wasn’t even used actually, it was just an Ordinary Least Squares(OLS). To know the difference we have to take a little step back. Inherently generalised linear regression models are an optimisation problem of the form\n$$L(\\textbf{y}, \\textbf{X} \\theta)+P(\\theta)$$\nwhere\n\\( \\textbf{y} \\) is the target \\( \\textbf{X} \\) is the design matrix \\( \\theta \\) is a vector of coefficient to determine \\( L \\) is a loss function \\( P \\) is a penalty function OLS and penalization In the case of Ridge regression the loss function is the OLS, to wich is added a \\( l_2 \\) regularization. The function to minimize is of the form\n$$||\\textbf{y} - \\textbf{X} \\theta ||_2^2 + \\lambda || \\theta ||_2^2$$\nwhere \\( ||.||_2 \\) is the \\( l_2 \\) norm and \\( \\lambda \\) is a penalization coefficient. In the Lorenz system example the lambda parameter was set to zero so in fact we were actually fitting using only the first part of the above expression, that corresponds to OLS, as said before. The formula we showed in the opening is actually quite different from this second definition, but this is because even though this is an optimisation problem the Ridge regression has a closed form solution. So if we imagine to have a matrix of targets \\(\\textbf{Y}\\) and \\(\\theta = \\textbf{W}_{\\text{out}} \\) then the first definition can be derived from the second.\nAnother form of regression based on the OLS loss function is Lasso (least absolute shrinkage and selection operator) which uses the \\( l_1 \\) norm as regularization. The function to minimize will hence have the form\n$$||\\textbf{y} - \\textbf{X} \\theta ||_2^2 + \\lambda || \\theta ||_1$$\nThis two methods can be linearly combined in order to obtain the Elastic Net regression method, of the form\n$$||\\textbf{y} - \\textbf{X} \\theta ||_2^2 + \\lambda || \\theta ||_2^2 + \\gamma || \\theta ||_1$$\nThis last two methods, contrarily to Ridge regression, do not present a closed form and so one has to use other solutions to the optimization problem, such as gradient descent or proximal gradient method.\nHuber loss function Of course one can choose other alternatives to the OLS loss function, and one of the most common is the Huber loss function. Used in robust regression is known to respond well in the presence of outliers. The function is defined as follows\n$$L_{\\sigma}(a) = \\frac{1}{2}a^2 \\text{ for } |a| \\le \\sigma$$ $$L_{\\sigma}(a) = \\sigma (|a| - \\frac{1}{2} \\sigma ) \\text{ otherwise}$$\nTo this function we can apply the same regularization function priorly defined, the \\( l_2 \\) and \\( l_1 \\) norm if one so choses.\nImplementation in ReservoirComputing.jl The implementation in the library has been done leveraging the incredible job done by the MLJLinearModels.jl team. The ESNtrain() function can now take as argument the following structs:\nRidge(lambda, solver) Ridge(lambda, solver) ElastNet(lambda, gamma, solver) RobustHuber(delta, lambda, gamma, solver) where lambda gamma and delta are defined in the theoretical sections above and solver is a solver of the MLJLinearModels library. One must be careful to use the suitable solver for every loss and regularization combination. Further information can be found in the MLJLinearModels documentation.\nExamples The Lasso regression was first proposed in [1] and in [2] a variation is proposed on it and there are also comparison with Elastic Net regression. Other comparison are carried out in [3], which is the paper we will follow as methodology. The Huber loss function is used for comparison in [4], but to my knowledge has not been adopted in other papers.\nTrying to follow the data preparation used in [3] we use the Rossler system this time to carry out our tests. The system is defined by the equations $$\\frac{dx}{dt} = -y -z$$ $$\\frac{dy}{dt} = x + ay$$ $$\\frac{dz}{dt} = b + z(x - c)$$ andit exhibits chaotic behavior for \\( a = 0.2 \\), \\( b = 0.2 \\) and \\( c = 5.7 \\). Using Range Kutta of order 4 from the initial positions \\( (-1, 0, 3) \\) the time series is generated with step size set to 0.005. In the paper the attractor is reconstructed in the phase space using embedding dimensions \\( (3, 3, 3) \\) and time delays \\( (13, 13, 13) \\) for the \\(x, y, z\\) series respectively. After, all the 9 resulting timeseries are rescaled in the range \\( [-1, 1] \\) and will all be used in the training of the ESN. Since the data preparation was unusual for me I spent a couple of hours wrapping my head around it. If one wants to know more about time delays and embeddings a good brief introduction is given in [5]. Thankfully DynamicalSystems.jl makes lifes easier when dealing with this type of problems and using embed() I was quickly able to create the data as expressed in the paper. StatsBase.jl dealt with the rescaling part.\nThe parameter for the ESN are then set as follows\nusing ReservoirComputing shift = 100 sparsity = 0.05 approx_res_size = 500 radius = 0.9 activation = tanh sigma = 0.01 train_len = 3900 predict_len = 1000 lambda = 5*10^(-5) gamma = 5*10^(-5) alpha = 1.0 nla_type = NLADefault() extended_states = true h = 1 The test was based on a h steps ahead prediction, which differs from the normal prediction because after every h steps of the ESN running autonomosly after training, the actual data is fed into the model, “correcting” the results. This way one has also to feed test data into to model, and the error is consequently quite low. As we can see the h parameter is set on 1, since that is the step used in the paper. The model only predicts one step in the future this way, for every step of the prediction.\nTo test the difference between values we used a user-defined Root Mean Square Deviation (RMSE) for the x coordinate, following paper guidelines. The results are as follows, given as a mean of 20 different initiations of random reservoirs:\n$$ \\text{rmse}_{\\text{RESN}} = 9.033 \\cdot 10^{-5} $$ $$ \\text{rmse}_{\\text{LESN}} = 0.006 $$ $$ \\text{rmse}_{\\text{EESN}} = 0.006 $$ $$ \\text{rmse}_{\\text{HESN}} = 9.040 \\cdot 10^{-5} $$\nwhere RESN is the Ridge regression trained ESN, LESN is the Lasso trained ESN, EESN is the Elastic Net trained ESN and HESN is the ESN trained with Huber function with \\( \\delta = 0.8\\) and \\( l_2 \\) norm.\nWe can also take a look at a plot of the x coordinate bot actual and predicted, but as one can expect from a rmse so small there is almost no difference.\nThe results obtained, while not in line with what is showed in the literature, are actually far better, by several orders of magnitude in some instances. While not obtaining the comparison with the papers is somewhat not optimal, there are some problems with the paper examined: the presence of several blunders in the final published draft, sometimes really evident, do not give ground to the dismissal of results but can raise eyebrows to the transparency of the methods or data utilized. In conclusion, the implementations to the ESN show promising results, but a more thorough exploration and examination needs to be done in order to really showcase their true utility. There will be time for that after GSoC ends, hopefully.\nAs always, if you have any questions regarding the model, the package or you have found errors in my post, please don’t hesitate to contact me!\nCiting Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:\n@article{JMLR:v23:22-0611, author = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}, title = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}, journal = {Journal of Machine Learning Research}, year = {2022}, volume = {23}, number = {288}, pages = {1--8}, url = {http://jmlr.org/papers/v23/22-0611.html} } References [1] Han, Min, Wei-Jie Ren, and Mei-Ling Xu. “An improved echo state network via L1-norm regularization.” Acta Automatica Sinica 40.11 (2014): 2428-2435.\n[2] Xu, Meiling, Min Han, and Shunshoku Kanae. “L 1/2 Norm Regularized Echo State Network for Chaotic Time Series Prediction.” International Conference on Neural Information Processing. Springer, Cham, 2016.\n[3] Xu, Meiling, and Min Han. “Adaptive elastic echo state network for multivariate time series prediction.” IEEE transactions on cybernetics 46.10 (2016): 2173-2183.\n[4] Guo, Yu, et al. “Robust echo state networks based on correntropy induced loss function.” Neurocomputing 267 (2017): 295-303.\n[5] http://www.scholarpedia.org/article/Attractor_reconstruction\n","wordCount":"1566","inLanguage":"en","datePublished":"2020-06-07T19:06:37+02:00","dateModified":"2020-06-07T19:06:37+02:00","author":{"@type":"Person","name":"Francesco Martinuzzi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://martinuzzifrancesco.github.io/posts/01_gsoc_week/"},"publisher":{"@type":"Organization","name":"Francesco Martinuzzi","logo":{"@type":"ImageObject","url":"https://martinuzzifrancesco.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://martinuzzifrancesco.github.io/ accesskey=h title="Francesco Martinuzzi (Alt + H)">Francesco Martinuzzi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://martinuzzifrancesco.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://martinuzzifrancesco.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://martinuzzifrancesco.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>GSoC week 1: lasso, Elastic Net and Huber loss</h1><div class=post-meta><span title='2020-06-07 19:06:37 +0200 CEST'>June 7, 2020</span>&nbsp;·&nbsp;Francesco Martinuzzi</div></header><div class=post-content><p>The first week tackled the implementation of different kind of linear regression for the creation of the last layer in the Echo State Network. More specifically were added the possibility to add a \( l_1 \) regularization to the loss function (Lasso regression), both \( l_1 \) and \( l_2 \) regularizations (Elastic Net regression) and also added the possibility to choose the Huber loss function.
As in the last post we will start from a brief theoretical background to explain the code and then we will showcase some examples taken from the literature.</p><h1 id=theoretical-background>Theoretical Background<a hidden class=anchor aria-hidden=true href=#theoretical-background>#</a></h1><p>In the <a href=https://martinuzzifrancesco.github.io/posts/a-brief-introduction-to-reservoir-computing/>Brief Introduction to Reservoir Computing</a> we showed how it was possible to get the output layer from a linear regression over the states and the desired output using <a href=https://en.wikipedia.org/wiki/Tikhonov_regularization>Ridge regression</a>:</p><p>$$\textbf{W}_{\text{out}} = \textbf{Y}^{\text{target}} \textbf{X}^{\text{T}}(\textbf{X} \textbf{X}^{\text{T}} + \beta \textbf{I})^{-1}$$
but by doing so we actually jumped a few steps, and in the example it wasn&rsquo;t even used actually, it was just an <a href=https://en.wikipedia.org/wiki/Ordinary_least_squares>Ordinary Least Squares</a>(OLS). To know the difference we have to take a little step back. Inherently generalised linear regression models are an optimisation problem of the form</p><p>$$L(\textbf{y}, \textbf{X} \theta)+P(\theta)$$</p><p>where</p><ul><li>\( \textbf{y} \) is the target</li><li>\( \textbf{X} \) is the design matrix</li><li>\( \theta \) is a vector of coefficient to determine</li><li>\( L \) is a loss function</li><li>\( P \) is a penalty function</li></ul><h2 id=ols-and-penalization>OLS and penalization<a hidden class=anchor aria-hidden=true href=#ols-and-penalization>#</a></h2><p>In the case of Ridge regression the loss function is the OLS, to wich is added a \( l_2 \) regularization. The function to minimize is of the form</p><p>$$||\textbf{y} - \textbf{X} \theta ||_2^2 + \lambda || \theta ||_2^2$$</p><p>where \( ||.||_2 \) is the \( l_2 \) norm and \( \lambda \) is a penalization coefficient. In the Lorenz system example the lambda parameter was set to zero so in fact we were actually fitting using only the first part of the above expression, that corresponds to OLS, as said before. The formula we showed in the opening is actually quite different from this second definition, but this is because even though this is an optimisation problem the Ridge regression has a closed form solution. So if we imagine to have a matrix of targets \(\textbf{Y}\) and \(\theta = \textbf{W}_{\text{out}} \) then the first definition can be derived from the second.</p><p>Another form of regression based on the OLS loss function is <a href=https://en.wikipedia.org/wiki/Lasso_(statistics)>Lasso</a> (least absolute shrinkage and selection operator) which uses the \( l_1 \) norm as regularization. The function to minimize will hence have the form</p><p>$$||\textbf{y} - \textbf{X} \theta ||_2^2 + \lambda || \theta ||_1$$</p><p>This two methods can be linearly combined in order to obtain the <a href=https://en.wikipedia.org/wiki/Elastic_net_regularization>Elastic Net regression</a> method, of the form</p><p>$$||\textbf{y} - \textbf{X} \theta ||_2^2 + \lambda || \theta ||_2^2 + \gamma || \theta ||_1$$</p><p>This last two methods, contrarily to Ridge regression, do not present a closed form and so one has to use other solutions to the optimization problem, such as <a href=https://en.wikipedia.org/wiki/Gradient_descent>gradient descent</a> or <a href=https://en.wikipedia.org/wiki/Proximal_gradient_method>proximal gradient method</a>.</p><h2 id=huber-loss-function>Huber loss function<a hidden class=anchor aria-hidden=true href=#huber-loss-function>#</a></h2><p>Of course one can choose other alternatives to the OLS loss function, and one of the most common is the <a href=https://en.wikipedia.org/wiki/Huber_loss>Huber loss</a> function. Used in robust regression is known to respond well in the presence of outliers. The function is defined as follows</p><p>$$L_{\sigma}(a) = \frac{1}{2}a^2 \text{ for } |a| \le \sigma$$
$$L_{\sigma}(a) = \sigma (|a| - \frac{1}{2} \sigma ) \text{ otherwise}$$</p><p>To this function we can apply the same regularization function priorly defined, the \( l_2 \) and \( l_1 \) norm if one so choses.</p><h2 id=implementation-in-reservoircomputingjl>Implementation in ReservoirComputing.jl<a hidden class=anchor aria-hidden=true href=#implementation-in-reservoircomputingjl>#</a></h2><p>The implementation in the library has been done leveraging the incredible job done by the MLJLinearModels.jl team. The <code>ESNtrain()</code> function can now take as argument the following structs:</p><ul><li><code>Ridge(lambda, solver)</code></li><li><code>Ridge(lambda, solver)</code></li><li><code>ElastNet(lambda, gamma, solver)</code></li><li><code>RobustHuber(delta, lambda, gamma, solver)</code></li></ul><p>where lambda gamma and delta are defined in the theoretical sections above and solver is a solver of the MLJLinearModels library. One must be careful to use the suitable solver for every loss and regularization combination. Further information can be found in the <a href=https://alan-turing-institute.github.io/MLJLinearModels.jl/stable/>MLJLinearModels documentation</a>.</p><h1 id=examples>Examples<a hidden class=anchor aria-hidden=true href=#examples>#</a></h1><p>The Lasso regression was first proposed in <a href=#1>[1]</a> and in <a href=#2>[2]</a> a variation is proposed on it and there are also comparison with Elastic Net regression. Other comparison are carried out in <a href=#3>[3]</a>, which is the paper we will follow as methodology. The Huber loss function is used for comparison in <a href=#4>[4]</a>, but to my knowledge has not been adopted in other papers.</p><p>Trying to follow the data preparation used in <a href=#3>[3]</a> we use the Rossler system this time to carry out our tests. The system is defined by the equations
$$\frac{dx}{dt} = -y -z$$
$$\frac{dy}{dt} = x + ay$$
$$\frac{dz}{dt} = b + z(x - c)$$
andit exhibits chaotic behavior for \( a = 0.2 \), \( b = 0.2 \) and \( c = 5.7 \). Using Range Kutta of order 4 from the initial positions \( (-1, 0, 3) \) the time series is generated with step size set to 0.005. In the paper the attractor is reconstructed in the phase space using embedding dimensions \( (3, 3, 3) \) and time delays \( (13, 13, 13) \) for the \(x, y, z\) series respectively. After, all the 9 resulting timeseries are rescaled in the range \( [-1, 1] \) and will all be used in the training of the ESN. Since the data preparation was unusual for me I spent a couple of hours wrapping my head around it. If one wants to know more about time delays and embeddings a good brief introduction is given in <a href=#5>[5]</a>. Thankfully <a href=https://juliadynamics.github.io/DynamicalSystems.jl/latest/>DynamicalSystems.jl</a> makes lifes easier when dealing with this type of problems and using <code>embed()</code> I was quickly able to create the data as expressed in the paper. <a href=https://juliastats.org/StatsBase.jl/stable/>StatsBase.jl</a> dealt with the rescaling part.</p><p>The parameter for the ESN are then set as follows</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>using</span> ReservoirComputing
</span></span><span style=display:flex><span>shift <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sparsity <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.05</span>
</span></span><span style=display:flex><span>approx_res_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>500</span>
</span></span><span style=display:flex><span>radius <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.9</span>
</span></span><span style=display:flex><span>activation <span style=color:#f92672>=</span> tanh
</span></span><span style=display:flex><span>sigma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>train_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>3900</span>
</span></span><span style=display:flex><span>predict_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>lambda <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span><span style=color:#f92672>*</span><span style=color:#ae81ff>10</span><span style=color:#f92672>^</span>(<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>gamma <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span><span style=color:#f92672>*</span><span style=color:#ae81ff>10</span><span style=color:#f92672>^</span>(<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>nla_type <span style=color:#f92672>=</span> NLADefault()
</span></span><span style=display:flex><span>extended_states <span style=color:#f92672>=</span> true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>h <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>The test was based on a h steps ahead prediction, which differs from the normal prediction because after every h steps of the ESN running autonomosly after training, the actual data is fed into the model, &ldquo;correcting&rdquo; the results. This way one has also to feed test data into to model, and the error is consequently quite low. As we can see the <code>h</code> parameter is set on 1, since that is the step used in the paper. The model only predicts one step in the future this way, for every step of the prediction.</p><p>To test the difference between values we used a user-defined <a href=https://en.wikipedia.org/wiki/Root-mean-square_deviation>Root Mean Square Deviation</a> (RMSE) for the x coordinate, following paper guidelines. The results are as follows, given as a mean of 20 different initiations of random reservoirs:</p><p>$$ \text{rmse}_{\text{RESN}} = 9.033 \cdot 10^{-5} $$
$$ \text{rmse}_{\text{LESN}} = 0.006 $$
$$ \text{rmse}_{\text{EESN}} = 0.006 $$
$$ \text{rmse}_{\text{HESN}} = 9.040 \cdot 10^{-5} $$</p><p>where RESN is the Ridge regression trained ESN, LESN is the Lasso trained ESN, EESN is the Elastic Net trained ESN and HESN is the ESN trained with Huber function with \( \delta = 0.8\) and \( l_2 \) norm.</p><p>We can also take a look at a plot of the x coordinate bot actual and predicted, but as one can expect from a rmse so small there is almost no difference.</p><p><img loading=lazy src=https://user-images.githubusercontent.com/10376688/83980445-41690e00-a916-11ea-8e0b-b65edd7a57d0.png alt=Rossler_coord></p><p>The results obtained, while not in line with what is showed in the literature, are actually far better, by several orders of magnitude in some instances. While not obtaining the comparison with the papers is somewhat not optimal, there are some problems with the paper examined: the presence of several blunders in the final published draft, sometimes really evident, do not give ground to the dismissal of results but can raise eyebrows to the transparency of the methods or data utilized.
In conclusion, the implementations to the ESN show promising results, but a more thorough exploration and examination needs to be done in order to really showcase their true utility. There will be time for that after GSoC ends, hopefully.</p><p>As always, if you have any questions regarding the model, the package or you have found errors in my post, please don&rsquo;t hesitate to contact me!</p><h2 id=citing>Citing<a hidden class=anchor aria-hidden=true href=#citing>#</a></h2><p>Part of the work done for this project has been published. If this post or the ReservoirComputing.jl software has been helpful, please consider citing the accompanying paper:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bibtex data-lang=bibtex><span style=display:flex><span><span style=color:#a6e22e>@article</span>{JMLR:v23:22-0611,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>author</span>  = <span style=color:#e6db74>{Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>title</span>   = <span style=color:#e6db74>{ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>journal</span> = <span style=color:#e6db74>{Journal of Machine Learning Research}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>year</span>    = <span style=color:#e6db74>{2022}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>volume</span>  = <span style=color:#e6db74>{23}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>number</span>  = <span style=color:#e6db74>{288}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>pages</span>   = <span style=color:#e6db74>{1--8}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>url</span>     = <span style=color:#e6db74>{http://jmlr.org/papers/v23/22-0611.html}</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1]
Han, Min, Wei-Jie Ren, and Mei-Ling Xu. &ldquo;An improved echo state network via L1-norm regularization.&rdquo; Acta Automatica Sinica 40.11 (2014): 2428-2435.</p><p>[2]
Xu, Meiling, Min Han, and Shunshoku Kanae. &ldquo;L 1/2 Norm Regularized Echo State Network for Chaotic Time Series Prediction.&rdquo; International Conference on Neural Information Processing. Springer, Cham, 2016.</p><p>[3]
Xu, Meiling, and Min Han. &ldquo;Adaptive elastic echo state network for multivariate time series prediction.&rdquo; IEEE transactions on cybernetics 46.10 (2016): 2173-2183.</p><p>[4]
Guo, Yu, et al. &ldquo;Robust echo state networks based on correntropy induced loss function.&rdquo; Neurocomputing 267 (2017): 295-303.</p><p>[5]
<a href=http://www.scholarpedia.org/article/Attractor_reconstruction>http://www.scholarpedia.org/article/Attractor_reconstruction</a></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://martinuzzifrancesco.github.io/>Francesco Martinuzzi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>